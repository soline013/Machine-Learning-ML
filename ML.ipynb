{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AGXi52kkuNzg",
        "RBbhoW7wuZSl",
        "Pk0wNAwMj5Ij",
        "zxR9_fEAu0Hz",
        "ou1yGz3Gq3u_",
        "oW9U3Ihg8Ctv",
        "_z5nmudhnL8h",
        "JdlIL9m0fVRA",
        "oLRbcjMQrNQo",
        "W3Rglb066XuI",
        "KUptTr0JdqAC",
        "Uff-dbE7rW-u",
        "J0toKFA9ng3z",
        "trrFQt_nJ_lZ",
        "WrSMbBLMwOfN",
        "HvfBUpZaVb8W",
        "E6A0CDbIVdOR",
        "-pGGSSlo4N0u",
        "itv3Kng4r2Sc",
        "0UnfMbiBtw_u",
        "OrHLM2lY1eFr",
        "hPFaXOtj8ABg",
        "1B3h7C3KsKr9",
        "NjIFTBEpI3BX",
        "3xDpd3YtGKpA",
        "xx438ZNhGNeb"
      ],
      "authorship_tag": "ABX9TyOmVz9X5w86ZlaFcH0tYrAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soline013/Machine_Learning-ML/blob/master/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja0e1SSoZeJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrP6lZaogyOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "theKfbglg8pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwYeFmjwO3Q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGXi52kkuNzg",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2k6B4UepD7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRa3lRoLVm9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hello = tf.constant(\"Hello, Tensorflow!\") #.constant()\n",
        "sess = tf.Session() #.Session()\n",
        "print(sess.run(hello)) #.run() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UOddTw1Tk1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "node1 = tf.constant(3.0, tf.float32)\n",
        "node2 = tf.constant(4.0)\n",
        "node3 = tf.add(node1, node2)  #.add()\n",
        "\n",
        "sess = tf.Session()\n",
        "print(\"sess.run(node1, node2): \", sess.run([node1, node2])) \n",
        "print(\"sess.run(node3): \", sess.run(node3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDvANm1kVgSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_node = tf.placeholder(tf.float32) #.placeholeder()와 feed_dict={a:a_data}\n",
        "b_node = tf.placeholder(tf.float32)\n",
        "add_node = a_node + b_node\n",
        "\n",
        "print(sess.run(add_node, feed_dict={a_node:2, b_node:13}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBbhoW7wuZSl",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sWVWbSaucq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGd_ht7sv0qr",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#H(x) = Wx + b\n",
        "x_train = [1, 2, 3]\n",
        "y_train = [1, 2, 3]\n",
        "w = tf.Variable(tf.random_normal([1]), name='weight') #.Variable(): 변수\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias') #.random_normal(Shapes)\n",
        "hypothesis = x_train * w + b\n",
        "\n",
        "#Cost(W,b)=∑...\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y_train)) #.reduce_mean(): 평균 #.square()\n",
        "\n",
        "#Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) #.GradientDescentOptimizer()\n",
        "train = optimizer.minimize(cost) #.minimize()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer()) #.global_variables_initializer()\n",
        "\n",
        "for step in range(2001):\n",
        "  sess.run(train)\n",
        "  if step % 20 == 0:\n",
        "    print(\"step:\", step, \"cost:\", sess.run(cost), \"w:\", sess.run(w), \"b:\", sess.run(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "zwDZ9AC6c-Jj",
        "colab": {}
      },
      "source": [
        "#H(x) = Wx + b\n",
        "x = tf.placeholder(tf.float32, shape=[None])\n",
        "y = tf.placeholder(tf.float32, shape=[None])\n",
        "w = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "hypothesis = x * w + b\n",
        "\n",
        "#Cost(W,b)=∑...\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
        "\n",
        "#Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "  cost_val, w_val, b_val, _ = sess.run([cost, w, b, train],\n",
        "      feed_dict={x: [2, 2.1, 2.2], y: [11, 12, 13]})\n",
        "  if step % 20 == 0:\n",
        "    print(\"step:\", step, \"cost:\", cost_val, \"w:\", w_val, \"b:\", b_val)\n",
        "\n",
        "#11.97 = 2.1 * 4.7 + 2.1\n",
        "#y = x * w + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk0wNAwMj5Ij",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_OUD--UkB8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import matplotlib.pyplot as plt\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm7M9B0u9Rvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [1, 2, 3]\n",
        "y = [1, 2, 3]\n",
        "W = tf.placeholder(tf.float32)\n",
        "hypothesis = x * W\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "W_val = []\n",
        "cost_val = []\n",
        "for i in range(-30, 50):\n",
        "  feed_W = i * 0.1\n",
        "  curr_cost, curr_W = sess.run([cost, W], feed_dict={W : feed_W})\n",
        "  W_val.append(curr_W) #.append()\n",
        "  cost_val.append(curr_cost)\n",
        "\n",
        "plt.plot(W_val, cost_val) #.plot()\n",
        "plt.show() #.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eta9cwISElXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [1, 2, 3]\n",
        "y_data = [1, 2, 3]\n",
        "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "hypothesis = x * W\n",
        "\n",
        "cost = tf.reduce_sum(tf.square(hypothesis -y)) #.reduce_sum()\n",
        "\n",
        "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
        "learning_rate = 0.1\n",
        "gradient = tf.reduce_mean((W * x - y) * x)\n",
        "descent = W - learning_rate * gradient\n",
        "update = W.assign(descent) #.assign()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(21):\n",
        "  sess.run(update, feed_dict={x: x_data, y: y_data})\n",
        "  print(\"step:\", step, \"cost:\", sess.run(cost, feed_dict={x: x_data, y: y_data}), \"W:\", sess.run(W))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWjbwVe_L4NI",
        "colab": {}
      },
      "source": [
        "x = [1, 2, 3]\n",
        "y = [1, 2, 3]\n",
        "W = tf.Variable(5.0)\n",
        "hypothesis = x * W\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis -y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(100):\n",
        "  print(\"step:\", step, \"W:\", sess.run(W))\n",
        "  sess.run(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nkdDJZ-fOwxf",
        "colab": {}
      },
      "source": [
        "x = [1, 2, 3]\n",
        "y = [1, 2, 3]\n",
        "W = tf.Variable(5.0)\n",
        "hypothesis = x * W\n",
        "gradient = tf.reduce_mean((W * x - y) * x) * 2\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis -y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "# Get gradients\n",
        "# Optional: modify gradient if necessary\n",
        "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
        "gvs = optimizer.compute_gradients(cost, [W]) #.compute_gradients()\n",
        "apply_gradients = optimizer.apply_gradients(gvs) #.apply_gradients()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(100):\n",
        "  print(step, sess.run([gradient, W, gvs]))\n",
        "  sess.run(apply_gradients)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxR9_fEAu0Hz",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 04."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1yGz3Gq3u_",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 04-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiqLjOb1u5WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJwJqrPMu9xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1_data = [73, 93, 89, 96, 73]\n",
        "x2_data = [80, 88, 91, 98, 66]\n",
        "x3_data = [75, 93, 90, 100, 70]\n",
        "y_data = [152, 185, 180, 196, 142]\n",
        "\n",
        "x1 = tf.placeholder(tf.float32)\n",
        "x2 = tf.placeholder(tf.float32)\n",
        "x3 = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "\n",
        "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
        "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
        "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = x1*w1 + x2*w2 + x3*w3 + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis -y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for step in range(2001):\n",
        "  cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
        "                                 feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, y: y_data,})\n",
        "  if step % 10 == 0:\n",
        "    print(\"Step: \", step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gZY6WQPyqvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]\n",
        "y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "w = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(x, w)+b #.matmul()\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for step in range(2001):\n",
        "  cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={x: x_data, y: y_data})\n",
        "  if step % 10 == 0:\n",
        "    print(\"Step: \", step, \"Cost: \", cost_val, \"\\nPrediction\\n\", hy_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9U3Ihg8Ctv",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 04-2. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n21wZLDQnJNS",
        "colab_type": "text"
      },
      "source": [
        "파일 불러오기 #자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKzd6xIt8Gy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIlRUcMS8Kt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('sample_data/mnist_test.csv', delimiter=',', dtype=np.float32) #.loadtext()\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "print(x_data.shape, x_data, len(x_data))\n",
        "print(y_data.shape, y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z5nmudhnL8h",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 05."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxCupQGYnUMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZjERhv9nWgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]] #0,1 Encoding\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "w = tf.Variable(tf.random_normal([2, 1]), name='weight') #.random_normal([n: 들어오는 개수, m:나가는 개수])\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias') #.random_normal([m: 나가는 개수])\n",
        "hypothesis = tf.sigmoid((tf.matmul(x, w) + b)) #.sigmoid()\n",
        "\n",
        "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1-y) * tf.log(1-hypothesis)) #.log()\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #.cast()\n",
        "accurary = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32)) #.equal()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for step in range(10001):\n",
        "    cost_val, _ = sess.run([cost, train], feed_dict={x: x_data, y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(step, cost_val)\n",
        "\n",
        "  h, c, a = sess.run([hypothesis, predicted, accurary],\n",
        "                   feed_dict={x: x_data, y: y_data})\n",
        "  print(\"\\nHypothesis: \", h, \"\\nCorrect(Y): \", c, \"\\nAccuracy: \", a)\n",
        "\n",
        "#그냥 실행을 돌릴 경우 nan이 출력되어 0.1을 hypothesis에 곱해주었다.\n",
        "#이후 실행은 되지만, 제대로 된 결과가 나오지 않음.\n",
        "\n",
        "#cost에 -를 붙여주면 제대로 된 결과가 나온다. 계산 결과 음수가 나온 것을 다시 양수로 바꿔주는 과정. 오류를 해결하였음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdlIL9m0fVRA",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 06."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLRbcjMQrNQo",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 06-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCXYocUEf0ch",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0jkaqk3NgNaO",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5,], [1,2,5,6,], [1,6,6,6], [1,7,7,7]]\n",
        "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
        "#One Hot Encoding [0,0,1]=2, [0,1,0]=1, [1,0,0]=0\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "nb_classes = 3\n",
        "w = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias') \n",
        "hypothesis = tf.nn.softmax(tf.matmul(x, w) + b) #.softmax()\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypothesis), axis=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "    sess.run(optimizer, feed_dict={x: x_data, y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(\"Step:\", step, \"Cost:\", sess.run(cost, feed_dict={x: x_data, y: y_data}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn8Ur2dVlQRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = sess.run(hypothesis, feed_dict={x: [[1,11,7,9]]}) \n",
        "print(a, sess.run(tf.arg_max(a, 1))) #.arg_max()\n",
        "\n",
        "print('----------')\n",
        "\n",
        "b = sess.run(hypothesis, feed_dict={x: [[1,3,4,3]]}) \n",
        "print(b, sess.run(tf.arg_max(b, 1)))\n",
        "\n",
        "print('----------')\n",
        "\n",
        "c = sess.run(hypothesis, feed_dict={x: [[1,1,0,1]]}) \n",
        "print(c, sess.run(tf.arg_max(c, 1)))\n",
        "\n",
        "print('----------')\n",
        "\n",
        "all = sess.run(hypothesis, feed_dict={x: [[1,11,7,9], [1,3,4,3], [1,1,0,1]]}) \n",
        "print(all, sess.run(tf.arg_max(all, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Rglb066XuI",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 06-2.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjUAUjdS6em3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LmBDRNGNAou8",
        "colab": {}
      },
      "source": [
        "#원래 강의와 다르게 직접 데이터를 입력함.\n",
        "\n",
        "x_data = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5,], [1,2,5,6,], [1,6,6,6], [1,7,7,7]]\n",
        "y_data = [[2], [2], [2], [1], [1], [1], [0], [0]]\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "y = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "y_one_hot = tf.one_hot(y, nb_classes) #.one_hot()\n",
        "y_one_hot = tf.reshape(y_one_hot, [-1, nb_classes]) #.reshape()\n",
        "#rank N이 rank N+1이 되기 때문에 .one_hot() 이후 .reshape()을 사용한다.\n",
        "nb_classes = 3\n",
        "w = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "logits = tf.matmul(x, w) + b\n",
        "hypothesis = tf.nn.softmax(logits)\n",
        "\n",
        "\n",
        "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot)\n",
        "cost = tf.reduce_mean(cost_i) #.softmax_cross_entropy_with_logits()\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "    sess.run(optimizer, feed_dict={x: x_data, y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(\"Step:\", step, \"Cost:\", sess.run(cost, feed_dict={x: x_data, y: y_data}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUptTr0JdqAC",
        "colab_type": "text"
      },
      "source": [
        "# ML_Lab 07."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uff-dbE7rW-u",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 07-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rMpCB5ddtPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZiB2tW7dvEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2,1], [1,3,2], [1,3,4], [1,5,5,], [1,7,5], [1,2,5], [1,6,6,], [1,7,7]]\n",
        "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
        "x_test = [[2,1,1,], [3,1,2,], [3,3,4]]\n",
        "y_test = [[0,0,1], [0,0,1], [0,0,1]]\n",
        "\n",
        "x = tf.placeholder(\"float\", shape=[None, 3])\n",
        "y = tf.placeholder(\"float\", shape=[None, 3])\n",
        "w = tf.Variable(tf.random_normal([3, 3]))\n",
        "b = tf.Variable(tf.random_normal([3])) \n",
        "hypothesis = tf.nn.softmax(tf.matmul(x, w) + b)\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypothesis), axis=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "prediction = tf.arg_max(hypothesis, 1)\n",
        "is_correct = tf.equal(prediction, tf.arg_max(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for step in range(201):\n",
        "    cost_val, W_val, _ = sess.run([cost, w, optimizer], feed_dict={x: x_data, y: y_data})\n",
        "    print(\"Step:\", step, \"Cost:\", cost_val, \"\\n\", W_val)\n",
        "  print(\"Prediction:\", sess.run(prediction, feed_dict={x: x_test}))\n",
        "  print(\"Accuracy:\", sess.run(accuracy, feed_dict={x: x_test, y: y_test}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWfQxY850Alu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#From GitHub\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "\n",
        "def min_max_scaler(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    # noise term prevents the zero division\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "\n",
        "xy = np.array(\n",
        "    [\n",
        "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "        [819, 823, 1198100, 816, 820.450012],\n",
        "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# very important. It does not work without it.\n",
        "xy = min_max_scaler(xy)\n",
        "print(xy)\n",
        "'''\n",
        "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
        " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
        " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
        " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
        " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
        " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
        " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
        " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
        "'''\n",
        "\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# Hypothesis\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "# Simplified cost/loss function\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "# Minimize\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
        "\n",
        "# Launch the graph in a session.\n",
        "with tf.Session() as sess:\n",
        "    # Initializes global variables in the graph.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(101):\n",
        "        _, cost_val, hy_val = sess.run(\n",
        "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
        "        )\n",
        "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0toKFA9ng3z",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 07-2. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn02xLqAwybs",
        "colab_type": "text"
      },
      "source": [
        "MNIST Dataset .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6J4-tnZnjkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "mnist = pd.read_csv(io.StringIO('mnist_test.csv'))\n",
        "print(mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR6RLgULNZLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "D = pd.read_csv(io.StringIO('Data01.csv'))\n",
        "print(D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrFQt_nJ_lZ",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 08."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc6IC3AhY402",
        "colab_type": "text"
      },
      "source": [
        "Simple ID Array and Slicing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5dzuJgmKDDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import pprint\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4) #.PrettyPrinter()\n",
        "#sess = tf.InteractiveSession() 이용하지 않음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMldRFjlKdyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([0, 1, 2, 3, 4, 5, 6]) #.array()\n",
        "pp.pprint(t)\n",
        "print(t.ndim) #rank\n",
        "print(t.shape) #shape\n",
        "print(t[0], t[1], t[-1])\n",
        "print(t[2:5], t[4:-1])\n",
        "print(t[:2], t[3:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVdmIymgM9tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
        "pp.pprint(t) #.pprint()\n",
        "print(t.ndim) # rank\n",
        "print(t.shape) # shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qGNLGHPY9xN",
        "colab_type": "text"
      },
      "source": [
        "Shape, Rank, Axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gatEqNd_Ocbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.constant([1,2,3,4])\n",
        "tf.shape(t).eval() #.shape(), .eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7NVmIkCOoL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.constant([[1,2],\n",
        "                 [3,4]])\n",
        "tf.shape(t).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTotWXDAOqki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.constant([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
        "tf.shape(t).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oheE0K52Qky5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[ #Axis=0\n",
        "    [ #Axis=1\n",
        "        [ #Axis=2\n",
        "            [1,2,3,4], \n",
        "            [5,6,7,8],\n",
        "            [9,10,11,12] #Axis=3 or -1\n",
        "        ],\n",
        "        [\n",
        "            [13,14,15,16],\n",
        "            [17,18,19,20], \n",
        "            [21,22,23,24]\n",
        "        ]\n",
        "    ]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZk1K2dFZF5M",
        "colab_type": "text"
      },
      "source": [
        "Matmul & Multiply."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLCIzA53TD9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix1 = tf.constant([[1.,2], [3.,4.]])\n",
        "matrix2 = tf.constant([[1.],[2.]])\n",
        "print(\"Metrix 1 shape\", matrix1.shape)\n",
        "print(\"Metrix 2 shape\", matrix2.shape)\n",
        "tf.matmul(matrix1, matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQyCC2rTGgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(matrix1*matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEnfR9dNZJTK",
        "colab_type": "text"
      },
      "source": [
        "Broadcasting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__RIkserU38r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix1 = tf.constant([[3., 3.]])\n",
        "matrix2 = tf.constant([[2.],[2.]])\n",
        "(matrix1+matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSEYX2MeU5pO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix1 = tf.constant([[3., 3.]])\n",
        "matrix2 = tf.constant([[2., 2.]])\n",
        "(matrix1+matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JZCr1GVVngn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean([1, 2], axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_h9OU-dZMeU",
        "colab_type": "text"
      },
      "source": [
        "Reduce_mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34fMivbTVqnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [[1., 2.],\n",
        "     [3., 4.]]\n",
        "\n",
        "\n",
        "tf.reduce_mean(x).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukW4rW95VvYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(x, axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_NislJRVxDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(x, axis=1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm4eJPhWVzUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(x, axis=-1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2we9VWwRZPgV",
        "colab_type": "text"
      },
      "source": [
        "Reduce_sum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YJbNdYZXU5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_sum(x).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_2ZyE5TXWuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_sum(x, axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F-uXC6SXX0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_sum(x, axis=-1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LczmPXlXY10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(tf.reduce_sum(x, axis=-1)).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2PkeAKCZirw",
        "colab_type": "text"
      },
      "source": [
        "Argmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4lfM4ygYAcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [[0, 1, 2],\n",
        "     [2, 1, 0]]\n",
        "tf.argmax(x, axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYPoWCocYC1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.argmax(x, axis=1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBNCFeu-YESJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.argmax(x, axis=-1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14C2tEw2Zk-t",
        "colab_type": "text"
      },
      "source": [
        "Reshape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQFHYoxoYpTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([[[0, 1, 2], \n",
        "               [3, 4, 5]],\n",
        "              \n",
        "              [[6, 7, 8], \n",
        "               [9, 10, 11]]])\n",
        "t.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGVswnxoYtDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reshape(t, shape=[-1, 3]).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6EefYn7Yugm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reshape(t, shape=[-1, 1, 3]).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-tP22LYwzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.squeeze([[0], [1], [2]]).eval() #.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUuFsIMoYxhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.expand_dims([0, 1, 2], 1).eval() #.expand_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oxXg1QqaSkz",
        "colab_type": "text"
      },
      "source": [
        "One_hot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4P5m4hkaYGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.one_hot([[0], [1], [2], [0]], depth=3).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVpvMGmWaZvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.one_hot([[0], [1], [2], [0]], depth=3)\n",
        "tf.reshape(t, shape=[-1, 3]).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ilpwKfcX1J",
        "colab_type": "text"
      },
      "source": [
        "Casting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vov-q2wHcfZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.cast([1.8, 2.2, 3.3, 4.9], tf.int32).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh9UVVBFco43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.cast([True, False, 1 == 1, 0 == 1], tf.int32).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VAYZ-fRdFsK",
        "colab_type": "text"
      },
      "source": [
        "Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzdcadsPdHuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [1, 4]\n",
        "y = [2, 5]\n",
        "z = [3, 6]\n",
        "\n",
        "tf.stack([x, y, z]).eval() #.stack()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQcrGYWndPNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.stack([x, y, z], axis=1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K2XTmx8h0R6",
        "colab_type": "text"
      },
      "source": [
        "Ones_like & Zeros_like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Kg0d15iLOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [[0, 1, 2],\n",
        "     [2, 1, 0]]\n",
        "\n",
        "tf.ones_like(x).eval() #.ones_like()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydUOqucKiPYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.zeros_like(x).eval() #.zeros_like()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gipesm-Ai1Gv",
        "colab_type": "text"
      },
      "source": [
        "Zip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2-9ovYIi2lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x, y in zip([1, 2, 3], [4, 5, 6]):\n",
        "    print(x, y)\n",
        "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
        "    print(x, y, z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrSMbBLMwOfN",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 09."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26HDEFj_rj4i",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 09-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya3r0I8ewSYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvb66mv-wWms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
        "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
        "\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        _, cost_val, w_val = sess.run(\n",
        "                  [train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(\"Step:\", step, \"Cost:\", cost_val, \"\\n\", w_val)\n",
        "\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4e8r1sFynP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
        "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
        "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for step in range(10001):\n",
        "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(\"Step:\", step, \"Cost:\", cost_val)\n",
        "\n",
        "    h, p, a = sess.run(\n",
        "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNTIEvjT0gNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
        "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
        "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
        "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
        "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
        "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
        "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
        "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for step in range(10001):\n",
        "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(\"Step:\", step, \"Cost:\", cost_val)\n",
        "\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaGUlzMP02eS",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 09-2. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uisTHgfH_f_P",
        "colab_type": "text"
      },
      "source": [
        "Tensorboard .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6RPRenV07I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXdnObvc1Epb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32)\n",
        "y_data = np.array([[0.], [1.], [1.], [0.]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, shape=[None, 2], name=\"x\")\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"Layer1\"):\n",
        "    W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight_1\")\n",
        "    b1 = tf.Variable(tf.random_normal([2]), name=\"bias_1\")\n",
        "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "\n",
        "    tf.summary.histogram(\"W1\", W1)\n",
        "    tf.summary.histogram(\"b1\", b1)\n",
        "    tf.summary.histogram(\"Layer1\", layer1)\n",
        "\n",
        "\n",
        "with tf.name_scope(\"Layer2\"):\n",
        "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight_2\")\n",
        "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias_2\")\n",
        "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "    tf.summary.histogram(\"W2\", W2)\n",
        "    tf.summary.histogram(\"b2\", b2)\n",
        "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
        "\n",
        "# cost/loss function\n",
        "with tf.name_scope(\"Cost\"):\n",
        "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "    tf.summary.scalar(\"Cost\", cost)\n",
        "\n",
        "with tf.name_scope(\"Train\"):\n",
        "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    # tensorboard --logdir=./logs/xor_logs\n",
        "    merged_summary = tf.summary.merge_all()\n",
        "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
        "    writer.add_graph(sess.graph)  # Show the graph\n",
        "\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        _, summary, cost_val = sess.run(\n",
        "            [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}\n",
        "        )\n",
        "        writer.add_summary(summary, global_step=step)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(step, cost_val)\n",
        "\n",
        "    # Accuracy report\n",
        "    h, p, a = sess.run(\n",
        "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvfBUpZaVb8W",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 10. +"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMZ-FNVJVhDm",
        "colab_type": "text"
      },
      "source": [
        "다양한 방법으로 MNIST 돌리기 .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY6RGXP6jvra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W = tf.Variable(tf.random_normal([784, 10]))\n",
        "b = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "num_epochs = 50\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
        "    )\n",
        ")\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# train my model\n",
        "with tf.Session() as sess:\n",
        "    # initialize\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for iteration in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
        "\n",
        "    print(\"Learning Finished!\")\n",
        "\n",
        "    # Test model and check accuracy\n",
        "    print(\n",
        "        \"Accuracy:\",\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
        "    )\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(\n",
        "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6A0CDbIVdOR",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 11."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj3Srlf8rt9-",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 11-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZLYyQyuVrVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN6PAeVWWD55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "image = np.array([[[[1],[2],[3]],\n",
        "                   [[4],[5],[6]], \n",
        "                   [[7],[8],[9]]]], dtype=np.float32)\n",
        "print(image.shape)\n",
        "plt.imshow(image.reshape(3,3), cmap='Greys') #.imshow()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWSD_7y1X12b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"image.shape\", image.shape)\n",
        "weight = tf.constant([[[[1.]],[[1.]]],\n",
        "                      [[[1.]],[[1.]]]])\n",
        "print(\"weight.shape\", weight.shape)\n",
        "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID') #.nn.conv2d()\n",
        "conv2d_img = conv2d.eval()\n",
        "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3) #.swapaxes()\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "    print(one_img.reshape(2,2))\n",
        "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray') #.subplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgU2In9jXPvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"image.shape\", image.shape)\n",
        "\n",
        "weight = tf.constant([[[[1.]],[[1.]]],\n",
        "                      [[[1.]],[[1.]]]])\n",
        "print(\"weight.shape\", weight.shape)\n",
        "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
        "conv2d_img = conv2d.eval()\n",
        "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "    print(one_img.reshape(3,3))\n",
        "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbIVmaztZI9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"image.shape\", image.shape)\n",
        "weight = tf.constant([[[[1.,10.,-1.]],[[1.,10.,-1.]]],[[[1.,10.,-1.]],[[1.,10.,-1.]]]])\n",
        "print(\"weight.shape\", weight.shape)\n",
        "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
        "conv2d_img = conv2d.eval()\n",
        "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "    print(one_img.reshape(3,3))\n",
        "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0BIfUWRZk2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = np.array([[[[4],[3]],\n",
        "                    [[2],[1]]]], dtype=np.float32)\n",
        "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1], #.nn.max_pool()\n",
        "                    strides=[1, 1, 1, 1], padding='VALID')\n",
        "print(pool.shape)\n",
        "print(pool.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Ai8uCTZ4Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = np.array([[[[4],[3]],\n",
        "                    [[2],[1]]]], dtype=np.float32)\n",
        "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 1, 1, 1], padding='SAME')\n",
        "print(pool.shape)\n",
        "print(pool.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KT36qFedpN",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 11-2. 3. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mp_Fts_hjnop"
      },
      "source": [
        "MNIST_CNN .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo3ElfdqhNGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "#    Conv     -> (?, 28, 28, 32)\n",
        "#    Pool     -> (?, 14, 14, 32)\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "'''\n",
        "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
        "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
        "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
        "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "#    Conv      ->(?, 14, 14, 64)\n",
        "#    Pool      ->(?, 7, 7, 64)\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "'''\n",
        "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
        "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
        "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
        "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "#    Conv      ->(?, 7, 7, 128)\n",
        "#    Pool      ->(?, 4, 4, 128)\n",
        "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "'''\n",
        "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
        "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
        "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
        "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
        "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "'''\n",
        "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
        "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "'''\n",
        "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
        "'''\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "print('Learning started. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "\n",
        "# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pGGSSlo4N0u",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 12."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itv3Kng4r2Sc",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1gEF3L-4See",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.contrib import rnn\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBRFXctnn0vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXDamUM24f7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2)\n",
        "hidden_size = 2\n",
        "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size) #.contrib.rnn.BasicRNNCell()\n",
        "\n",
        "x_data = np.array([[h]], dtype=np.float32)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32) #.nn.dynamic_rnn()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "pp.pprint(outputs.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw057jOpn5pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
        "hidden_size = 2\n",
        "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
        "x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
        "print(x_data.shape)\n",
        "pp.pprint(x_data)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "pp.pprint(outputs.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjUBe6eUnx_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
        "x_data = np.array([[h, e, l, l, o],\n",
        "                   [e, o, l, l, l],\n",
        "                   [l, l, e, e, l]], dtype=np.float32)\n",
        "pp.pprint(x_data)\n",
        "    \n",
        "hidden_size = 2\n",
        "cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True) #.BasicLSTMCell()\n",
        "outputs, _states = tf.nn.dynamic_rnn(\n",
        "    cell, x_data, dtype=tf.float32)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "pp.pprint(outputs.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UnfMbiBtw_u",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-2.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0qGL507tzy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksXP39uTw5il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "x_data = [[0, 1, 0, 2, 3, 3]]\n",
        "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
        "              [0, 1, 0, 0, 0],   # i 1\n",
        "              [1, 0, 0, 0, 0],   # h 0\n",
        "              [0, 0, 1, 0, 0],   # e 2\n",
        "              [0, 0, 0, 1, 0],   # l 3\n",
        "              [0, 0, 0, 1, 0]]]  # l 3\n",
        "\n",
        "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n",
        "\n",
        "num_classes = 5\n",
        "input_dim = 5\n",
        "hidden_size = 5\n",
        "batch_size = 1\n",
        "sequence_length = 6\n",
        "learning_rate = 0.1\n",
        "\n",
        "X = tf.placeholder(\n",
        "    tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
        "\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
        "initial_state = cell.zero_state(batch_size, tf.float32) #.zero_state()\n",
        "outputs, _states = tf.nn.dynamic_rnn(\n",
        "    cell, X, initial_state=initial_state, dtype=tf.float32)\n",
        "\n",
        "# FC layer\n",
        "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
        "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
        "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
        "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
        "outputs = tf.contrib.layers.fully_connected( #.contrib.layers.fully_connected()\n",
        "    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
        "\n",
        "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
        "\n",
        "weights = tf.ones([batch_size, sequence_length]) #.ones()\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss( #.contrib.seq2seq.sequence_loss()\n",
        "    logits=outputs, targets=Y, weights=weights)\n",
        "loss = tf.reduce_mean(sequence_loss)\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "#.AdamOptimizer()\n",
        "\n",
        "prediction = tf.argmax(outputs, axis=2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(50):\n",
        "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
        "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
        "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
        "\n",
        "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
        "        print(\"\\tPrediction str: \", ''.join(result_str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHLM2lY1eFr",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-3.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJwyhMk01kDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcGTZu6W1l8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = \" if you want you\"\n",
        "idx2char = list(set(sample))  # index -> char\n",
        "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
        "\n",
        "# hyper parameters\n",
        "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
        "rnn_hidden_size = len(char2idx)  # RNN output size\n",
        "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
        "batch_size = 1  # one sample data, one batch\n",
        "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
        "learning_rate = 0.2\n",
        "\n",
        "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
        "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
        "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
        "\n",
        "# flatten the data (ignore batches for now). No effect if the batch size is 1\n",
        "X_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
        "X_for_softmax = tf.reshape(X_one_hot, [-1, rnn_hidden_size])\n",
        "\n",
        "# softmax layer (rnn_hidden_size -> num_classes)\n",
        "softmax_w = tf.get_variable(\"softmax_w\", [rnn_hidden_size, num_classes])\n",
        "softmax_b = tf.get_variable(\"softmax_b\", [num_classes]) #.get_variable()\n",
        "outputs = tf.matmul(X_for_softmax, softmax_w) + softmax_b\n",
        "\n",
        "# expend the data (revive the batches)\n",
        "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
        "weights = tf.ones([batch_size, sequence_length])\n",
        "\n",
        "# Compute sequence cost/loss\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
        "    logits=outputs, targets=Y, weights=weights)\n",
        "loss = tf.reduce_mean(sequence_loss)  # mean all sequence loss\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "prediction = tf.argmax(outputs, axis=2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(3000):\n",
        "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
        "        result = sess.run(prediction, feed_dict={X: x_data})\n",
        "\n",
        "        # print char using dic\n",
        "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
        "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPFaXOtj8ABg",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12.4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl6RSrpx-2j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5EITgxh-4iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {w: i for i, w in enumerate(char_set)}\n",
        "\n",
        "data_dim = len(char_set)\n",
        "hidden_size = len(char_set)\n",
        "num_classes = len(char_set)\n",
        "sequence_length = 10  # Any arbitrary number\n",
        "learning_rate = 0.1\n",
        "\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x = [char_dic[c] for c in x_str]  # x str to index\n",
        "    y = [char_dic[c] for c in y_str]  # y str to index\n",
        "\n",
        "    dataX.append(x)\n",
        "    dataY.append(y)\n",
        "\n",
        "batch_size = len(dataX)\n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "\n",
        "# One-hot encoding\n",
        "X_one_hot = tf.one_hot(X, num_classes)\n",
        "print(X_one_hot)  # check out the shape\n",
        "\n",
        "# Make a lstm cell with hidden_size (each unit output vector size)\n",
        "def lstm_cell():\n",
        "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
        "    return cell\n",
        "\n",
        "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n",
        "#.MultiRNNCell()\n",
        "\n",
        "# outputs: unfolding size x hidden size, state = hidden size\n",
        "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n",
        "\n",
        "# FC layer\n",
        "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
        "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
        "\n",
        "# reshape out for sequence_loss\n",
        "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
        "\n",
        "# All weights are 1 (equal weights)\n",
        "weights = tf.ones([batch_size, sequence_length])\n",
        "\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
        "    logits=outputs, targets=Y, weights=weights)\n",
        "mean_loss = tf.reduce_mean(sequence_loss)\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(500):\n",
        "    _, l, results = sess.run(\n",
        "        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n",
        "    for j, result in enumerate(results):\n",
        "        index = np.argmax(result, axis=1)\n",
        "        print(i, j, ''.join([char_set[t] for t in index]), l)\n",
        "\n",
        "# Let's print the last char of each result to check it works\n",
        "results = sess.run(outputs, feed_dict={X: dataX})\n",
        "for j, result in enumerate(results):\n",
        "    index = np.argmax(result, axis=1)\n",
        "    if j is 0:  # print all for the first result to make a sentence\n",
        "        print(''.join([char_set[t] for t in index]), end='')\n",
        "    else:\n",
        "        print(char_set[index[-1]], end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B3h7C3KsKr9",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-5. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0OeGtD2Iugz",
        "colab_type": "text"
      },
      "source": [
        "Dynamic RNN .#자세한 내용은 요약에서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjIFTBEpI3BX",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12.6.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRQaEIrqJQOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import os\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "#if \"DISPLAY\" not in os.environ:\n",
        "    # remove Travis CI Error\n",
        "    #matplotlib.use('Agg') 사용하지 않음.\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxCUw0cJSbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MinMaxScaler(data):\n",
        "    ''' Min Max Normalization\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : numpy.ndarray\n",
        "        input data to be normalized\n",
        "        shape: [Batch size, dimension]\n",
        "    Returns\n",
        "    ----------\n",
        "    data : numpy.ndarry\n",
        "        normalized data\n",
        "        shape: [Batch size, dimension]\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
        "    '''\n",
        "    numerator = data - np.min(data, 0) #.min()\n",
        "    denominator = np.max(data, 0) - np.min(data, 0) #.max()\n",
        "    # noise term prevents the zero division\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "\n",
        "# train Parameters\n",
        "seq_length = 7\n",
        "data_dim = 5\n",
        "hidden_dim = 10\n",
        "output_dim = 1\n",
        "learning_rate = 0.01\n",
        "iterations = 500\n",
        "\n",
        "# Open, High, Low, Volume, Close\n",
        "xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
        "xy = xy[::-1]  # reverse order (chronically ordered)\n",
        "\n",
        "# train/test split\n",
        "train_size = int(len(xy) * 0.7)\n",
        "train_set = xy[0:train_size]\n",
        "test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
        "\n",
        "# Scale each\n",
        "train_set = MinMaxScaler(train_set) #MinMaxScaler()\n",
        "test_set = MinMaxScaler(test_set)\n",
        "\n",
        "# build datasets\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, len(time_series) - seq_length):\n",
        "        _x = time_series[i:i + seq_length, :]\n",
        "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
        "        print(_x, \"->\", _y)\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "trainX, trainY = build_dataset(train_set, seq_length)\n",
        "testX, testY = build_dataset(test_set, seq_length)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "# build a LSTM network\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(\n",
        "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "Y_pred = tf.contrib.layers.fully_connected(\n",
        "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
        "\n",
        "# cost/loss\n",
        "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "# RMSE\n",
        "targets = tf.placeholder(tf.float32, [None, 1])\n",
        "predictions = tf.placeholder(tf.float32, [None, 1])\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training step\n",
        "    for i in range(iterations):\n",
        "        _, step_loss = sess.run([train, loss], feed_dict={\n",
        "                                X: trainX, Y: trainY})\n",
        "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
        "\n",
        "    # Test step\n",
        "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
        "    rmse_val = sess.run(rmse, feed_dict={\n",
        "                    targets: testY, predictions: test_predict})\n",
        "    print(\"RMSE: {}\".format(rmse_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R1kY5LRLvew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot predictions\n",
        "    plt.plot(testY)\n",
        "    plt.plot(test_predict)\n",
        "    plt.xlabel(\"Time Period\") #.xlabel()\n",
        "    plt.ylabel(\"Stock Price\") #.ylabel()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xDpd3YtGKpA",
        "colab_type": "text"
      },
      "source": [
        "#Memo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9wAvZCKGQIi",
        "colab_type": "text"
      },
      "source": [
        ".constant()\n",
        "\n",
        ".Session()\n",
        "\n",
        ".run()\n",
        "\n",
        ".add()\n",
        "\n",
        ".placeholeder()와 feed_dict={a:a_data}\n",
        "\n",
        ".Variable(): 변수\n",
        "\n",
        ".random_normal(Shapes)\n",
        "\n",
        ".reduce_mean(): 평균\n",
        "\n",
        ".square()\n",
        "\n",
        ".GradientDescentOptimizer()\n",
        "\n",
        ".minimize()\n",
        "\n",
        ".global_variables_initializer()\n",
        "\n",
        ".append()\n",
        "\n",
        ".plot()\n",
        "\n",
        ".show()\n",
        "\n",
        ".reduce_sum()\n",
        "\n",
        ".assign()\n",
        "\n",
        ".compute_gradients()\n",
        "\n",
        ".apply_gradients()\n",
        "\n",
        ".matmul()\n",
        "\n",
        ".loadtext()\n",
        "\n",
        ".set_random_seed()\n",
        "\n",
        ".string_input_producer()\n",
        "\n",
        ".TextLineReader()\n",
        "\n",
        ".read()\n",
        "\n",
        ".decode_csv()\n",
        "\n",
        ".batch()\n",
        "\n",
        ".Coordinator()\n",
        "\n",
        ".start_queue_runners()\n",
        "\n",
        ".request_stop()\n",
        "\n",
        ".join()\n",
        "\n",
        ".sigmoid()\n",
        "\n",
        ".log()\n",
        "\n",
        ".cast()\n",
        "\n",
        ".equal()\n",
        "\n",
        ".softmax()\n",
        "\n",
        ".arg_max()\n",
        "\n",
        ".one_hot()\n",
        "\n",
        ".reshape()\n",
        "\n",
        ".softmax_cross_entropy_with_logits()\n",
        "\n",
        ".format()\n",
        "\n",
        ".flatten()\n",
        "\n",
        ".PrettyPrinter()\n",
        "\n",
        ".InteractiveSession()\n",
        "\n",
        ".array()\n",
        "\n",
        ".pprint()\n",
        "\n",
        ".shape()\n",
        "\n",
        ".eval()\n",
        "\n",
        ".squeeze()\n",
        "\n",
        ".expand_dims()\n",
        "\n",
        ".stack()\n",
        "\n",
        ".ones_like()\n",
        "\n",
        ".zeros_like()\n",
        "\n",
        "zip()\n",
        "\n",
        ".nn.relu()\n",
        "\n",
        ".random.randn()\n",
        "\n",
        ".nn.dropout()\n",
        "\n",
        ".imshow()\n",
        "\n",
        ".nn.conv2d()\n",
        "\n",
        ".swapaxes()\n",
        "\n",
        ".subplot()\n",
        "\n",
        ".nn.max_pool()\n",
        "\n",
        ".contrib.rnn.BasicRNNCell()\n",
        "\n",
        ".nn.dynamic_rnn()\n",
        "\n",
        ".BasicLSTMCell()\n",
        "\n",
        ".zero_state()\n",
        "\n",
        ".contrib.layers.fully_connected()\n",
        "\n",
        ".ones()\n",
        "\n",
        ".contrib.seq2seq.sequence_loss()\n",
        "\n",
        ".AdamOptimizer()\n",
        "\n",
        ".get_variable()\n",
        "\n",
        ".MultiRNNCell()\n",
        "\n",
        ".min()\n",
        "\n",
        ".max()\n",
        "\n",
        "MinMaxScaler\n",
        "\n",
        ".xlabel()\n",
        "\n",
        ".ylabel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx438ZNhGNeb",
        "colab_type": "text"
      },
      "source": [
        "#이면지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtxsPLmzddsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "tf.set_random_seed(777)\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iLyobhdjqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = pd.read_csv('sample_data/mnist_test.csv')\n",
        "print(mnist)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}