"Attention is all you need"(Vaswani et al. 2017).

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

NLPì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë¡œ, 2017ë…„ NIPSì—ì„œ Googleì´ ì†Œê°œí•˜ì˜€ë‹¤.

RNNì—ì„œ ë²—ì–´ë‚˜ Attentionë§Œì„ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ì„ ê³ ì•ˆí•˜ì—¬ Multi-head Self-attentionì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

RNNì˜ ìˆœì°¨ì  ê³„ì‚°ì„ í–‰ë ¬ê³±ì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ëª¨ë“  ì¤‘ìš” ì •ë³´ë¥¼ Embedding í•œë‹¤.

Self-attentionì„ í†µí•´ ê°™ì€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ ìŒ ì‚¬ì´ì˜ ì˜ë¯¸, ë¬¸ë²• ê´€ê³„ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.

ê·¸ëŸ¬ë‚˜ Positional Encodingë§Œìœ¼ë¡œëŠ” ìœ„ì¹˜, ìˆœì„œ ì •ë³´ ì œê³µì— ì–´ë ¤ì›€ì´ ìˆì–´, BERTê°€ ë“±ì¥í•˜ê²Œ ë˜ì—ˆë‹¤. BERTì—ì„œëŠ” Positional Embeddingì´ë‹¤.

## Model

Inputs: Encoder / Outputs: Decoderë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/66259854/104466229-56b6c100-55f8-11eb-9662-1acfd8adf05c.png)

- Input(Output) Embedding
- Positional Encoding
- (Encoder-Decoder) (Masked) Multi Head (Self) Attention
- Scaled Dot Product Attention
- Dropout
- Layer Normalization
- Sub-layer & Residual Connection
- Feed Forward
- Linear & Softmax

---

![image](https://user-images.githubusercontent.com/66259854/104466270-61715600-55f8-11eb-8a2a-ad56d09a66a0.png)

ë…¼ë¬¸ì—ì„œëŠ” Encoderì™€ Decoderë¥¼ 6ê°œì”© ìŒ“ì•„ Encoding ë¶€ë¶„ê³¼ Decoding ë¶€ë¶„ì„ ë§Œë“¤ì—ˆë‹¤.

## Layer

Dropoutì— ëŒ€í•œ ì„¤ëª…ì€ ìƒëµí•œë‹¤.

Self Attentionì´ ì•„ë‹ˆê±°ë‚˜, Output Embeddingì¸ ê²½ìš°ê°€ ìˆìœ¼ë‚˜ ìœ„ì—ì„œ í•œ ë²ˆ í‘œê¸°í•œ ì´í›„ë¡œ ìƒëµí•œë‹¤.

ë˜í•œ Tensorflowì™€ Pytorchê°€ í˜¼ì¬ë˜ì–´ ìˆë‹¤.

### Input Embedding

Word Embedding

[tf.keras.layers.Embedding | TensorFlow Core v2.3.0](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)

[Glossary of Deep Learning: Word Embedding](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)

Convert 2D sequence (batch_size, input_length)

â†’ 3D (batch_size, input_length, $d_{model}$)

Word Embeddingì€ 6ê°œ ì¤‘ ê°€ì¥ ë°‘ì— ìˆëŠ” Encoderì—ì„œë§Œ ì¼ì–´ë‚œë‹¤.

### Positional Encoding

![image](https://user-images.githubusercontent.com/66259854/104466284-67673700-55f8-11eb-8399-a97d4b1da32f.png)

RNNì€ ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ê¸° ë•Œë¬¸ì—, ë‹¨ì–´ì˜ ìœ„ì¹˜ì— ë”°ë¼ ìœ„ì¹˜ ì •ë³´ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

ì´ íŠ¹ì§•ìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ RNNì´ ìì£¼ ì‚¬ìš©ë˜ì—ˆë‹¤.

ê·¸ë¦¬ê³  Transformerì—ì„œëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ ìœ„í•´ Positional Encodingì„ ì‚¬ìš©í•œë‹¤.

---

$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$$

$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$

$2i$ = ì§ìˆ˜ / $2i+1$ = í™€ìˆ˜

pos = Embedding Vectorì˜ ìœ„ì¹˜. ~~Sequence Batch ìˆ˜.~~

index = Embedding Vector ë‚´ Dimension Index. ~~Embedding Dimension.~~ 

### Multi Head Self Attention

![image](https://user-images.githubusercontent.com/66259854/104466694-db094400-55f8-11eb-98a6-55e0a2eb4ca3.png)

Multi Head Self Attentionì€ Query, Key, Value headë¡œ ë‚˜ë‰˜ê³ , ê°ê° ë‹¤ë¥¸ Linear Projection, Scaled Dot-Productë¥¼ ì§„í–‰í•œë‹¤. (Splitì„ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— ê°ê° ë‹¤ë¥¸ Scaled Dot-Productë¥¼ ì§„í–‰í•œë‹¤.)

ì´í›„ Concat, Linear Projectionì„ í•˜ëŠ”ë°, ì´ëŠ” tf.reshape, tf.transformìœ¼ë¡œ í•œ ë²ˆì— ì—°ì‚°í•  ìˆ˜ ìˆë‹¤.

### Scaled Dot Product Attention

![image](https://user-images.githubusercontent.com/66259854/104466713-df356180-55f8-11eb-9e5c-5cd6faac972c.png)

Learnable Parameterê°€ ì—†ë‹¤.

ê¸°ì¡´ Additive Attentionì€ Attention scoreë¥¼ êµ¬í•˜ëŠ” êµ¬ê°„ì— Feed Forward Layerê°€ ìˆì§€ë§Œ, Dot Product ì—°ì‚°ìœ¼ë¡œ ëŒ€ì²´í•˜ì˜€ë‹¤.

Encoderì—ì„œ Paddingì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ Padding Maskë¥¼ ì¶”ê°€í•´ì•¼ í•œë‹¤.

### Sub-layer & Residual Connection, Layer Normalization

![image](https://user-images.githubusercontent.com/66259854/104466733-e492ac00-55f8-11eb-873e-e56a6bc92906.png)

Encoderì—ëŠ” 2ê°œì˜ Sub-layerê°€ ìˆê³ , Decoderì—ëŠ” 3ê°œì˜ Sub-layerê°€ ìˆë‹¤.

ì´ Sub-layerëŠ” Residual Connectionì„ ê±°ì¹œë‹¤.

---

[Layer Normalization](https://arxiv.org/abs/1607.06450)

$$\bar{x} = \frac{a}{\sigma}(x - \mu) + b$$

$$LayerNorm(x_i)=\gamma \ \frac{x_{i, k} - \mu_i}{\sqrt{\sigma_i^2} + \epsilon} + \beta&&
&&(\gamma=1,\ \beta=0)$$

ì´í›„ $LayerNorm(xÂ +Â Sublayer(x))$ ì´ ì‹ì²˜ëŸ¼ Layer Normalizationì„ ì ìš©í•œë‹¤.

LNì€ Tensorì˜ ë§ˆì§€ë§‰ ì°¨ì›ì— ëŒ€í•´ì„œ í‰êµ°ê³¼ ë¶„ì‚°ì„ êµ¬í•˜ê³  ìœ„ì˜ ìˆ˜ì‹ì„ í†µí•´ ê°’ì„ ì •ê·œí™”í•œë‹¤.

### Feed Forward

$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$

Multi Head Attentionì—ì„œ ë‚˜ì˜¨ Attention ì •ë³´ë¥¼ ì •ë¦¬í•˜ëŠ” ì—­í• .

`FF - Relu - Dropout - FF` ìˆœì„œì˜ Sequentialí•œ êµ¬ì¡°ì´ë‹¤.

### Linear & Softmax

![image](https://user-images.githubusercontent.com/66259854/104466753-eb212380-55f8-11eb-9f12-2e00a64433b1.png)

1. Linear Layer.

    Fully-connectedë¡œ ë§ˆì§€ë§‰ Decoder Outputì„ Logits Vectorì— íˆ¬ì˜ì‹œí‚¨ë‹¤.

    Logits VectorëŠ” Output Vocabularyë¡œ ë§ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ ìˆê³ , Vectorì˜ ê° ì…€ì€ ëŒ€ì‘í•˜ëŠ” ë‹¨ì–´ì˜ ì ìˆ˜ì´ë¯€ë¡œ ì¶œë ¥ì„ í•´ì„í•  ìˆ˜ ìˆë‹¤.

2. Softmax Layer.

    ì´ ì ìˆ˜ë“¤ì„ Softmaxë¥¼ í†µí•´ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

    Argmaxë¥¼ ì´ìš©í•˜ì—¬ ë‚˜ì˜¨ ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì´ ìµœì¢… Outputì´ ëœë‹¤.

## Encoder

`- Multi Head Attention(Mask)`

`- Dropout1`

`- LayerNorm with Residual Connection1 â†’ Output`

`-`

`- Position wise Feed Forward`

`- Dropout2`

`- LayerNorm with Residual Connection2 â†’ Output`

tf.add()ë¡œ ResNetì—ì„œ ì‚¬ìš©í•˜ëŠ” Residual Connectionì„ í•œë‹¤.

Paddingì„ ì ìš©í•˜ì§€ ëª»í•˜ê²Œ Padding Maskë¥¼ ë„£ëŠ”ë‹¤.

## Decoder

`- Masked Multi Head Attention`

`- Dropout1`

`- LayerNorm with Residual Connection1 â†’ Query`

`-`

`- Encoder-Decoder Multi Head Attention
{Input: Query, Key(Encoder Output), Value(Encoder Output)}`

`- Dropout2`

`- LayerNorm with Residual Connection2 â†’ En/Decoder Output`

`-`

`- Position wise Feed Forward`

`- Dropout3`

`- LayerNorm with Residual Connection3 â†’ Output`

Encoderì˜ Outputì´ Key, Valueê°€ ëœë‹¤.

Masked Multi Head Attentionì—ëŠ” Look Ahead Maskì„ ë„£ê³ ,

Encoder-Decoder Multi Head Attentionì—ëŠ” ë˜ Padding Maskë¥¼ ë„£ëŠ”ë‹¤.

## Multi Head Self Attention &  Scaled Dot Product Attention

Layerì˜ Multi Head Self Attentionê³¼ Scaled Dot Product Attentionì„ ìƒì„¸íˆ ì„œìˆ í•œë‹¤.

### Vector Calculation.

ë¬¸ì¥ ë‚´ íŠ¹ì • ë‹¨ì–´ì— ëŒ€í•œ Self-attentionì„ ê³„ì‚°í•˜ë ¤ë©´, ë¬¸ì¥ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ê°ê° ì ìˆ˜ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.

ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë¹ ë¥¸ ì†ë„ë¥¼ ìœ„í•´ Vectorê°€ ì•„ë‹Œ Matrixë¥¼ ì‚¬ìš©í•œë‹¤.

1. Create Head Vector

    ![image](https://user-images.githubusercontent.com/66259854/104466771-f07e6e00-55f8-11eb-9e0c-871e9fdeb7aa.png)

    3ê°€ì§€ Head VectorëŠ” Input Vectorì™€ 3ê°œì˜ í•™ìŠµ ê°€ëŠ¥í•œ í–‰ë ¬ì„ ê°ê° ê³±í•´ì„œ ë§Œë“¤ì–´ì§„ë‹¤.

    Input Vectorì˜ í¬ê¸°ê°€ $d_{model}=512$ì¼ ë•Œ, Head Vectorì˜ í¬ê¸°ëŠ” 64ì¸ë°, Attentionì˜ ê³„ì‚° ë³µì¡ë„ë¥¼ ì¼ì •í•˜ê²Œ ë§Œë“¤ê³ ì í•˜ëŠ” êµ¬ì¡° ë•Œë¬¸ì´ë‹¤.

    (ë˜í•œ 64ë¼ëŠ” ê°’ì€ $num-heads=8$ë¡œ ê²°ì •ë˜ëŠ”ë°, $d_{model}$ì„ $num-heads$ë¡œ ë‚˜ëˆˆ ê°’ì´ë‹¤.)

2. Matmul

    ![image](https://user-images.githubusercontent.com/66259854/104466792-f411f500-55f8-11eb-98a7-c705415e851c.png)

    í˜„ì¬ ë‹¨ì–´ì˜ Q Vectorì™€ ëª¨ë“  ë‹¨ì–´ì˜ K Vertorë¥¼ Matmul í•œë‹¤.

3. Scale & Softmax (Mask ê³¼ì •ì„ ê±°ì¹  ìˆ˜ ìˆë‹¤.)

    ![image](https://user-images.githubusercontent.com/66259854/104466808-f83e1280-55f8-11eb-81b8-96cc4ab4b8b5.png)

    1. ì ìˆ˜ë“¤ì„ 8ë¡œ ë‚˜ëˆ„ëŠ”ë°, Key Vectorì˜ í¬ê¸° 64ì˜ ì œê³±ê·¼ì´ë‹¤.

        ì´ë¥¼ Attention Scoreë¼ê³  ë¶€ë¥¸ë‹¤.

    2. Softmaxë¥¼ ì·¨í•˜ê³ , ì´ ì ìˆ˜ëŠ” í˜„ì¬ ìœ„ì¹˜ì˜ ë‹¨ì–´ì—ì„œ ê° ë‹¨ì–´ë“¤ì˜ í‘œí˜„ì´ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€ ê²°ì •í•œë‹¤.

        ë‹¹ì—°íˆ í˜„ì¬ ìœ„ì¹˜ì˜ ë‹¨ì–´ê°€ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë³´ì´ë‚˜, ë‹¤ë¥¸ ë‹¨ì–´ì— ëŒ€í•œ ì •ë³´ë„ í¬í•¨ë˜ì–´ ìˆë‹¤.

4. Matmul

    ![image](https://user-images.githubusercontent.com/66259854/104466823-fbd19980-55f8-11eb-8eff-73760822a595.png)

    ì ìˆ˜ì— V Vectorë¥¼ ê³±í•œë‹¤.

    ì´ë¥¼ Attention Value í˜¹ì€ Context Vectorë¼ê³  ë¶€ë¥¸ë‹¤.

    Attentionì„ ìœ„í•´ ê´€ë ¨ì´ ìˆëŠ” ë‹¨ì–´ëŠ” ë‚¨ê²¨ë‘ê³ , ê´€ë ¨ì´ ì—†ëŠ” ë‹¨ì–´ëŠ” 0.001ê³¼ ê°™ì€ ì•„ì£¼ ì‘ì€ ìˆ«ìë¥¼ ê³±í•´ ì—†ì•¤ë‹¤.

5. Concat

    ì ìˆ˜ì™€ V Vectorê°€ ê³±í•´ì§„ Attention Valueë¥¼ Concat í•œë‹¤.

    í˜„ì¬ ìœ„ì¹˜ì— ëŒ€í•œ Self Attentionì˜ ì¶œë ¥ì´ë‹¤.

### Matrix Calcualtion.
1. Query, Key, Value Matrix

    ![image](https://user-images.githubusercontent.com/66259854/104466832-ff652080-55f8-11eb-9c85-7ed4d95ed7a1.png)

    Input Vector or Embedding Vectorë¥¼ í•˜ë‚˜ì˜ í–‰ë ¬ Xë¡œ ìŒ“ì•„ ì˜¬ë¦¬ê³ , í•™ìŠµí•  Weight í–‰ë ¬ì„ ê³±í•´ Q, K, Vë¥¼ ê³„ì‚°í•œë‹¤.

    í–‰ë ¬ Xì˜ ê° í–‰ì€ ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì— í•´ë‹¹í•œë‹¤.

2. One Equation

    ![image](https://user-images.githubusercontent.com/66259854/104466853-0429d480-55f9-11eb-83c7-73efc5047835.png)

    í–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´ Vector Calculation 2~5ë¥¼ í•˜ë‚˜ì˜ ì‹ìœ¼ë¡œ ì••ì¶•í•  ìˆ˜ ìˆë‹¤.

### Multi Head.

![image](https://user-images.githubusercontent.com/66259854/104466868-07bd5b80-55f9-11eb-9245-14ad6a9c555c.png)

Hê°œì˜ Query, Key, Value Weight í–‰ë ¬ì„ ê°–ê³  ìˆë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” $num-heads=8$ê°œì˜ Attention Headë¥¼ ê°–ëŠ”ë‹¤.

ê° Attention Headì—ì„œ Query, Key, ValueëŠ” ëœë¤ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ í•™ìŠµëœë‹¤.

![image](https://user-images.githubusercontent.com/66259854/104466881-0b50e280-55f9-11eb-93f3-48fd582581e0.png)

Self Attention ê³¼ì •ì„ ê±°ì¹˜ë©´ 8ê°œ(ë…¼ë¬¸ ê¸°ì¤€)ì˜ Z í–‰ë ¬ì´ ë‚˜ì˜¨ë‹¤.

ê·¸ëŸ¬ë‚˜ Feed FowardëŠ” í•œ ìœ„ì¹˜ì— ëŒ€í•´ í•œ ê°œì˜ í–‰ë ¬ë§Œ ë°›ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

![image](https://user-images.githubusercontent.com/66259854/104466916-1277f080-55f9-11eb-91f6-2f8ec0b1ab45.png)

ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 8ê°œì˜ Z í–‰ë ¬ì„ ì´ì–´ ë¶™ì—¬ì„œ í•˜ë‚˜ì˜ í–‰ë ¬ì„ ë§Œë“¤ê³ , ë˜ ë‹¤ë¥¸ Weight í–‰ë ¬ì¸ $W_O$ì„ ê³±í•œë‹¤.

![image](https://user-images.githubusercontent.com/66259854/104466933-173ca480-55f9-11eb-9b9b-c811931eefb9.png)

ëª¨ë‘ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë¦¼ì´ ëœë‹¤.

Multi HeadëŠ” Self Attentionì„ ë³‘ë ¬ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

### Difference of Attention.

![image](https://user-images.githubusercontent.com/66259854/104466947-1b68c200-55f9-11eb-9659-cf072d28d071.png)

1. Encoder Self Attentionì€ Encoderì—ì„œ ì´ë£¨ì–´ì§„ë‹¤.

2. Masked Decoder Self Attentionì€ Decoderì—ì„œ ì´ë£¨ì–´ì§„ë‹¤.

3. Encoder-Decoder Attentionë„ Decoderì—ì„œ ì´ë£¨ì–´ì§€ë‚˜, Self-attentionì´ ì•„ë‹ˆë‹¤.

    Query: Decoder Vector / Key, Value: Encoder Vectorë¡œ, Headì˜ ì¶œì²˜ê°€ ê°™ì§€ ì•Šìœ¼ë¯€ë¡œ ë™ì¼í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.

## Mask

### Padding Mask.

```python
def attention(query, key, value, mask=None, dropout=0.0):
    "Compute 'Scaled Dot Product Attention'"
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim = -1)

    # (Dropout described below)
    p_attn = F.dropout(p_attn, p=dropout)
    return torch.matmul(p_attn, value), p_attn
    ```

    ```python
    def scaled_dot_product_attention(query, key, value, mask):
    '''ì¤‘ëµ'''
        logits += (mask * -1e9) # ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬ì¸ logitsì— mask*-1e9 ê°’ì„ ë”í•´ì£¼ê³  ìˆë‹¤.
    '''ì¤‘ëµ'''
```

ì…ë ¥ ë¬¸ì¥ì— <PAD> Tokenì´ ìˆì„ ë•Œ, ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ì§€ ì•Šë„ë¡  Maskingì„ í•˜ì—¬ Attentionì—ì„œ ì œì™¸í•œë‹¤.

1. -1e9ì™€ ê°™ì€ ì‘ì€ ìŒìˆ˜ê°’ì„ ê³±í•œë‹¤.
2. Softmaxë¥¼ ì§€ë‚˜ê¸° ì´ì „ì— ì‘ì€ ìŒìˆ˜ê°’ì´ ìˆìœ¼ë¯€ë¡œ, Softmaxë¥¼ ì§€ë‚˜ë©´ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.

### Look-ahead Mask.

![image](https://user-images.githubusercontent.com/66259854/104466965-202d7600-55f9-11eb-9ab9-195fbc75c5a5.png)

seq2seq Decoderì™€ ë‹¬ë¦¬, Transformer DecoderëŠ” ë¬¸ì¥ í–‰ë ¬ë¡œ ì…ë ¥ì„ í•œ ë²ˆì— ë°›ëŠ”ë‹¤.

ë”°ë¼ì„œ í˜„ì¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ, ë¯¸ë˜ ì‹œì ì˜ ë‹¨ì–´ë„ ì°¸ê³ í•˜ëŠ” ì¼ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ Decoderì˜ ì²« ë²ˆì§¸ ì„œë¸Œì¸µì—ì„œ Look-ahead Maskë¥¼ ì¶”ê°€í•œë‹¤.

---

![image](https://user-images.githubusercontent.com/66259854/104466981-24f22a00-55f9-11eb-97e4-4bef39ae6da9.png)

```python
    def subsequent_mask(size):
        "Mask out subsequent positions."
        attn_shape = (1, size, size)
        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
        return torch.from_numpy(subsequent_mask) == 0
```

```python
    @staticmethod
        def make_std_mask(tgt, pad):
            "Create a mask to hide padding and future words."
            tgt_mask = (tgt != pad).unsqueeze(-2)
            tgt_mask = tgt_mask & Variable(
                subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
            return tgt_mask
```

1. Self-attentionì„ í†µí•´ Attention Score Matrixë¥¼ ì–»ëŠ”ë‹¤.
2. ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì´ Maskingí•˜ì—¬ ë¯¸ë˜ ì‹œì ì˜ ë‹¨ì–´ë¥¼ ì°¸ê³ í•˜ì§€ ëª»í•˜ë„ë¡ ë°”ê¾¼ë‹¤.

## ğŸ¸

### Auto Regressive & Teacher Forcing.

[Transformer model for language understanding | TensorFlow Core](https://www.tensorflow.org/tutorials/text/transformer#training_and_checkpointing)

[ê³ ì‘ ì¸ê°„ : ë„¤ì´ë²„ ë¸”ë¡œê·¸](https://blog.naver.com/just_nlp/222136930059)

```python
class Batch:
    "Object for holding a batch of data with mask during training."
    def __init__(self, src, trg=None, pad=1):
        self.src = src
        self.src_mask = (src != pad).unsqueeze(-2)
        if trg is not None:
            self.trg = trg[:, :-1]
            self.trg_y = trg[:, 1:]
            self.trg_mask = \
                self.make_std_mask(self.trg, pad)
            self.ntokens = (self.trg_ != pad).data.sum()
```

1. Transformerì˜ DecoderëŠ” Generate í•  ë•Œ ë¬´ì¡°ê±´ 1ê°œì˜ Tokenì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµë˜ì–´ ìˆë‹¤. Sequenceë¥¼ Generate í•˜ë ¤ë©´ Sequence ê¸¸ì´ë§Œí¼ Decoderë¥¼ ë°˜ë³µí•´ì„œ ì‹¤í–‰í•œë‹¤.

2. Inference ìƒí™©ì—ì„œ í•™ìŠµì„ ìƒê°í•´ ë³´ì. í•™ìŠµ ë‹¨ê³„ì—ì„œë„ 1ê°œì”© ìƒì„±í•˜ë„ë¡ í•™ìŠµí•´ì•¼ í•œë‹¤. Source - Target ë‘ ë¬¸ì¥ì€ Pairë¡œ ì˜ ê°€ì§€ê³  ìˆë‹¤. Decoderë¥¼ í•™ìŠµí•  ë•ŒëŠ” Targetì—ì„œ ì²« ë²ˆì§¸ ê¸€ìë¥¼ ì•ˆë‹¤ê³  ìƒê°í•˜ê³  ë‘ ë²ˆì§¸ ê¸€ìë¥¼ í•™ìŠµí•œë‹¤. ì„¸ ë²ˆì§¸ ê¸€ìëŠ” ì•ì˜ ë‘ ê¸€ìë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤. ì´ë ‡ê²Œ í•œ ê¸€ìì”© ì˜ˆì¸¡í•˜ê³  Lossë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì„ Auto Regressiveë¼ê³  í•œë‹¤.

3. "ë‘ ë²ˆì§¸ í† í°ì´ ì˜ëª» ìƒì„±ë˜ì—ˆìœ¼ë©´ ì„¸ ë²ˆì§¸ í† í°ì„ ë§Œë“¤ê³  Lossë¥¼ ì¸¡ì •í•  ë•Œ ë¬¸ì œê°€ ìƒê¸°ëŠ”ê°€?"ë¼ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. í•™ìŠµ ë‹¨ê³„ì—ì„œ ì˜ëª» ìƒì„±ë˜ì–´ë„ ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì›ë³¸ ë¬¸ì¥ì„ ë‹¤ì‹œ ë„£ëŠ”ë‹¤. ìƒì„±í•œ í† í°ì„ ì§ì ‘ ë„£ëŠ” Inferenceì™€ ì°¨ì´ë¥¼ ë³´ì¸ë‹¤. ì´ë ‡ê²Œ ì˜ëª» ìƒì„±ë˜ì–´ë„ ë‹¤ìŒ í† í°ì„ ë§Œë“¤ ë•Œ ì˜¬ë°”ë¥¸ ë°ì´í„°ë¥¼ ë„£ëŠ” ë°©ì‹ì„ Teacher Forcingì´ë¼ê³  í•œë‹¤.

4. ë”°ë¼ì„œ, DecoderëŠ” End Token ì§ì „ê¹Œì§€ ìƒì„± ëª¨ë¸ì„ íƒœì›Œì„œ End Tokenì´ ì˜ ë§Œë“¤ì–´ì§€ëŠ”ì§€ í™•ì¸í•˜ë©´ í•œ Sequenceë¥¼ í•™ìŠµí•œ ê²ƒì´ë‹¤. RNNì—ì„œë„ í•˜ë‚˜ë¥¼ ë§Œë“¤ê³ , ê·¸ ë‹¤ìŒ í† í°ì„ ë§Œë“¤ê¸° ìœ„í•´ ì´ì „ì˜ Vectorë¥¼ ì „ë¶€ ì´ìš©í•˜ëŠ” ê²ƒì„ ìƒê°í•˜ë©´ ëœë‹¤.

Original Comments.

    transformer ì˜ decoder ëŠ” generate í• ë•Œ ë¬´ì¡°ê±´ '1ê°œì˜ token' ì„ ìƒì„±í•˜ê²Œ í•™ìŠµë˜ì–´ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ sequence ë¥¼ generate í• ë ¤ë©´ sequence ê¸¸ì´ ë§Œí¼ decoder ë¥¼ ë°˜ë³µí•´ì„œ ì‹¤í–‰í•´ì•¼í•©ë‹ˆë‹¤.
    ìœ„ ìƒí™©ì€ inference ìƒí™©ì´ë¼ê³  í•´ë³´ë©´, ê·¸ëŸ¼ í•™ìŠµì€ ì–´ë–»ê²Œ ì´ë¤„ì§ˆê¹Œ ìƒê°í•´ë³´ì‹œë©´ ë©ë‹ˆë‹¤. í•™ìŠµí• ë•Œë„ 1ê°œì”© ìƒì„±í•˜ê²Œ í•™ìŠµí•´ì•¼í•˜ëŠ”ë°, source - target ë‘ ë¬¸ì¥ì˜ pair ëŠ” ìŒìœ¼ë¡œ ì˜ ë“¤ê³  ìˆê³ , ê·¸ëŸ¼ decoder ë¥¼ í•™ìŠµí• ë•ŒëŠ” targetì—ì„œ ë§¨ ì• ê¸€ìë§Œ ì•ˆë‹¤ ì¹˜ê³  ê·¸ ë‹¤ìŒ ê¸€ìë¥¼ í•™ìŠµí•˜ê³ , ê·¸ ë‹¤ìŒì—ëŠ” 3ë²ˆì§¸ ê¸€ìë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì•ì— ë‘ ê¸€ìë§Œ ì‚¬ìš©í•˜ê² ì£ ?
    ì´ë ‡ê²Œ í•œ ê¸€ìì”© ì˜ˆì¸¡í•˜ê³  loss ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì„ auto regressive ë°©ì‹ ì´ë¼ê³  í•©ë‹ˆë‹¤.

    ê·¸ëŸ¼ ë‹¤ìŒ ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆëŠ”ë°,
    'ë§Œì•½ ì²« ë²ˆì§¸ í† í° ê¸°ë°˜ìœ¼ë¡œ ë‘ ë²ˆì§¸ í† í°ì„ ìƒì„±í–ˆëŠ”ë°, ì´ê²Œ ì˜ëª» ë§Œë“¤ì–´ì¡Œìœ¼ë©´ ì„¸ ë²ˆì§¸ í† í°ì„ ë§Œë“¤ê³  ë¡œìŠ¤ë¥¼ ì¸¡ì •í• ë•Œ ì˜ëª» ë§Œë“¤ì–´ì§„ ë‘ ë²ˆì§¸ í† í°ì„ ì‚¬ìš©í•˜ë©´ ë¬¸ì œê°€ ë˜ëŠ”ê²Œ ì•„ë‹ê¹Œ?' ë¼ëŠ” ì˜ë¬¸ì´ ìƒê¸¸ ìˆ˜ ìˆëŠ”ë°, í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” ì˜ëª» ìƒì„±í–ˆì–´ë„ ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì›ë³¸ ë¬¸ì¥ì„ ë‹¤ì‹œ ë„£ìŠµë‹ˆë‹¤. ì¸í¼ëŸ°ìŠ¤ í• ë•Œì™€ ë‹¤ë¥¸ ì ì´ì£ (ì¸í¼ëŸ°ìŠ¤ëŠ” ìƒì„±í•œ í† í°ì„ ì§ì ‘ ë„£ìŒ.)
    ì´ë ‡ê²Œ ì˜ëª» ë§Œë“¤ì—ˆì–´ë„ ë°”ë¡œ ë‹¤ìŒ í† í°ì„ ìƒì„±í• ë•Œ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥¸ ë°ì´í„°ë¥¼ ë„£ì–´ì£¼ëŠ” ë°©ì‹ì„ teacher forcing ì´ë¼ê³  í•©ë‹ˆë‹¤. (ê·¼ë° ìœ„ ë§í¬ ë²ˆì—­ì´ êµì‚¬ ê°•ì œ... ë¼ê³  ë˜ì–´ìˆë„¤ìš”.. ì—¬ëŸ¬ë¶„ ì˜ì–´ë¡œ ê°™ì´ ë³´ì„¸ìš”...)

    ë”°ë¼ì„œ, decoder ëŠ” ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ì„ ì•Œë¦¬ëŠ” í† í° ì§ì „ê¹Œì§€ ìƒì„± ëª¨ë¸ì„ íƒœì›Œì„œ ë§ˆì§€ë§‰ í† í°ì´ ì˜ ë§Œë“¤ì–´ì§€ëŠ”ì§€ í™•ì¸í•˜ë©´ í•œ ì‹œí€€ìŠ¤ë¥¼ ì „ë¶€ í•™ìŠµí•œê²Œ ë©ë‹ˆë‹¤. auto regressive í•˜ê²Œ ë§ì´ì£ .

    RNN ì—ì„œë„ í•˜ë‚˜ë¥¼ ë§Œë“¤ê³ , ê·¸ ë‹¤ìŒ í† í°ì„ ë§Œë“¤ê¸° ìœ„í•´ ì§ì „ vector ë“¤ì„ ì „ë¶€ ì´ìš©í•˜ëŠ”ê²ƒì„ ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤ :)

## ë§í¬

[BERT](https://www.notion.so/BERT-c61ed6193e85436c8916c641c9372188)

[Google Colaboratory](https://colab.research.google.com/drive/1cwhr9FD4Ogmr8zNieCRICGD5V9kuT_jv#scrollTo=11l-Oq-Mjfg7)

[soline013/transformer-tensorflow2.0](https://github.com/soline013/transformer-tensorflow2.0/blob/master/transformer_implement_tf2_0.ipynb)

[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

[ìœ„í‚¤ë…ìŠ¤](https://wikidocs.net/31379)

[Transformer model for language understanding | TensorFlow Core](https://www.tensorflow.org/tutorials/text/transformer)

[The Illustrated Transformer](https://nlpinkorean.github.io/illustrated-transformer/)

[TorchTextë¡œ ì–¸ì–´ ë²ˆì—­í•˜ê¸° - PyTorch Tutorials 1.6.0 documentation](https://tutorials.pytorch.kr/beginner/torchtext_translation_tutorial.html)

[11ì£¼ì°¨(2) - Attention is All You Need (Transformer)](https://www.quantumdl.com/entry/11%EC%A3%BC%EC%B0%A82-Attention-is-All-You-Need-Transformer)
