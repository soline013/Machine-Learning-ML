{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AGXi52kkuNzg",
        "RBbhoW7wuZSl",
        "Pk0wNAwMj5Ij",
        "zxR9_fEAu0Hz",
        "ou1yGz3Gq3u_",
        "oW9U3Ihg8Ctv",
        "_z5nmudhnL8h",
        "JdlIL9m0fVRA",
        "oLRbcjMQrNQo",
        "W3Rglb066XuI",
        "KUptTr0JdqAC",
        "Uff-dbE7rW-u",
        "J0toKFA9ng3z",
        "trrFQt_nJ_lZ",
        "WrSMbBLMwOfN",
        "HvfBUpZaVb8W",
        "E6A0CDbIVdOR",
        "-pGGSSlo4N0u",
        "itv3Kng4r2Sc",
        "0UnfMbiBtw_u",
        "OrHLM2lY1eFr",
        "hPFaXOtj8ABg",
        "1B3h7C3KsKr9",
        "NjIFTBEpI3BX",
        "3xDpd3YtGKpA",
        "xx438ZNhGNeb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja0e1SSoZeJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58f5d17f-9cea-4b27-9220-1b5c77111eec"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrP6lZaogyOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "theKfbglg8pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20f04a92-5c8f-46ee-937d-e7fe79006de3"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwYeFmjwO3Q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGXi52kkuNzg",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2k6B4UepD7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e56493ba-e1bf-48c9-bdd1-498eaa9c84c0"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRa3lRoLVm9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "25032308-80ae-4c28-9e9c-2be7191cd026"
      },
      "source": [
        "hello = tf.constant(\"Hello, Tensorflow!\") #.constant()\n",
        "sess = tf.Session() #.Session()\n",
        "print(sess.run(hello)) #.run() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Hello, Tensorflow!'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UOddTw1Tk1Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f87f5362-390d-4d9d-deb2-a826a3f71221"
      },
      "source": [
        "node1 = tf.constant(3.0, tf.float32)\n",
        "node2 = tf.constant(4.0)\n",
        "node3 = tf.add(node1, node2)  #.add()\n",
        "\n",
        "sess = tf.Session()\n",
        "print(\"sess.run(node1, node2): \", sess.run([node1, node2])) \n",
        "print(\"sess.run(node3): \", sess.run(node3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sess.run(node1, node2):  [3.0, 4.0]\n",
            "sess.run(node3):  7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDvANm1kVgSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "98b3c17b-fe38-492f-c12d-b2dd08cf4e36"
      },
      "source": [
        "a_node = tf.placeholder(tf.float32) #.placeholeder()와 feed_dict={a:a_data}\n",
        "b_node = tf.placeholder(tf.float32)\n",
        "add_node = a_node + b_node\n",
        "\n",
        "print(sess.run(add_node, feed_dict={a_node:2, b_node:13}))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBbhoW7wuZSl",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sWVWbSaucq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGd_ht7sv0qr",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3d4c7060-232c-4c77-f1be-839c44ffa86e"
      },
      "source": [
        "#H(x) = Wx + b\n",
        "x_train = [1, 2, 3]\n",
        "y_train = [1, 2, 3]\n",
        "w = tf.Variable(tf.random_normal([1]), name='weight') #.Variable(): 변수\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias') #.random_normal(Shapes)\n",
        "hypothesis = x_train * w + b\n",
        "\n",
        "#Cost(W,b)=∑...\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y_train)) #.reduce_mean(): 평균 #.square()\n",
        "\n",
        "#Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) #.GradientDescentOptimizer()\n",
        "train = optimizer.minimize(cost) #.minimize()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer()) #.global_variables_initializer()\n",
        "\n",
        "for step in range(2001):\n",
        "  sess.run(train)\n",
        "  if step % 20 == 0:\n",
        "    print(\"step:\", step, \"cost:\", sess.run(cost), \"w:\", sess.run(w), \"b:\", sess.run(b))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 cost: 8.001567 w: [0.05218883] b: [-0.82516235]\n",
            "step: 20 cost: 0.087637685 w: [1.0388123] b: [-0.37196013]\n",
            "step: 40 cost: 0.014499336 w: [1.1266925] b: [-0.3150184]\n",
            "step: 60 cost: 0.012578337 w: [1.12928] b: [-0.2964563]\n",
            "step: 80 cost: 0.011418496 w: [1.1240177] b: [-0.28216615]\n",
            "step: 100 cost: 0.010370421 w: [1.1182667] b: [-0.26887134]\n",
            "step: 120 cost: 0.009418575 w: [1.112716] b: [-0.25623214]\n",
            "step: 140 cost: 0.008554102 w: [1.1074195] b: [-0.24418989]\n",
            "step: 160 cost: 0.0077689807 w: [1.1023713] b: [-0.23271383]\n",
            "step: 180 cost: 0.007055905 w: [1.09756] b: [-0.2217771]\n",
            "step: 200 cost: 0.006408289 w: [1.0929751] b: [-0.21135437]\n",
            "step: 220 cost: 0.005820108 w: [1.0886056] b: [-0.20142147]\n",
            "step: 240 cost: 0.0052859164 w: [1.0844417] b: [-0.19195546]\n",
            "step: 260 cost: 0.0048007597 w: [1.0804731] b: [-0.18293434]\n",
            "step: 280 cost: 0.0043601193 w: [1.0766912] b: [-0.17433707]\n",
            "step: 300 cost: 0.003959936 w: [1.073087] b: [-0.1661439]\n",
            "step: 320 cost: 0.0035964798 w: [1.0696522] b: [-0.1583357]\n",
            "step: 340 cost: 0.003266383 w: [1.0663787] b: [-0.1508945]\n",
            "step: 360 cost: 0.0029665772 w: [1.0632592] b: [-0.14380303]\n",
            "step: 380 cost: 0.002694295 w: [1.0602863] b: [-0.1370449]\n",
            "step: 400 cost: 0.0024469974 w: [1.057453] b: [-0.13060425]\n",
            "step: 420 cost: 0.002222405 w: [1.054753] b: [-0.1244663]\n",
            "step: 440 cost: 0.0020184228 w: [1.0521798] b: [-0.11861686]\n",
            "step: 460 cost: 0.001833165 w: [1.0497274] b: [-0.1130423]\n",
            "step: 480 cost: 0.0016649073 w: [1.0473905] b: [-0.10772972]\n",
            "step: 500 cost: 0.0015120987 w: [1.0451634] b: [-0.10266685]\n",
            "step: 520 cost: 0.0013733123 w: [1.0430409] b: [-0.0978419]\n",
            "step: 540 cost: 0.0012472713 w: [1.0410185] b: [-0.09324382]\n",
            "step: 560 cost: 0.0011327891 w: [1.0390904] b: [-0.08886182]\n",
            "step: 580 cost: 0.0010288182 w: [1.0372533] b: [-0.08468563]\n",
            "step: 600 cost: 0.00093438994 w: [1.0355026] b: [-0.08070569]\n",
            "step: 620 cost: 0.00084862736 w: [1.033834] b: [-0.07691283]\n",
            "step: 640 cost: 0.0007707357 w: [1.0322441] b: [-0.0732982]\n",
            "step: 660 cost: 0.0006999954 w: [1.0307287] b: [-0.0698535]\n",
            "step: 680 cost: 0.00063574885 w: [1.0292846] b: [-0.06657066]\n",
            "step: 700 cost: 0.00057739636 w: [1.0279083] b: [-0.06344213]\n",
            "step: 720 cost: 0.00052440265 w: [1.0265967] b: [-0.06046062]\n",
            "step: 740 cost: 0.0004762716 w: [1.0253468] b: [-0.05761919]\n",
            "step: 760 cost: 0.00043255533 w: [1.0241556] b: [-0.05491129]\n",
            "step: 780 cost: 0.00039285704 w: [1.0230204] b: [-0.05233068]\n",
            "step: 800 cost: 0.00035679733 w: [1.0219384] b: [-0.04987136]\n",
            "step: 820 cost: 0.00032404845 w: [1.0209074] b: [-0.04752761]\n",
            "step: 840 cost: 0.00029430576 w: [1.0199246] b: [-0.04529391]\n",
            "step: 860 cost: 0.00026729095 w: [1.0189883] b: [-0.04316513]\n",
            "step: 880 cost: 0.00024275472 w: [1.0180959] b: [-0.04113644]\n",
            "step: 900 cost: 0.00022047617 w: [1.0172455] b: [-0.03920312]\n",
            "step: 920 cost: 0.00020023808 w: [1.016435] b: [-0.03736073]\n",
            "step: 940 cost: 0.00018186127 w: [1.0156627] b: [-0.03560492]\n",
            "step: 960 cost: 0.00016517028 w: [1.0149266] b: [-0.0339316]\n",
            "step: 980 cost: 0.00015000883 w: [1.014225] b: [-0.03233692]\n",
            "step: 1000 cost: 0.00013624186 w: [1.0135566] b: [-0.03081723]\n",
            "step: 1020 cost: 0.00012373694 w: [1.0129195] b: [-0.029369]\n",
            "step: 1040 cost: 0.00011237932 w: [1.0123123] b: [-0.02798876]\n",
            "step: 1060 cost: 0.000102065766 w: [1.0117338] b: [-0.02667342]\n",
            "step: 1080 cost: 9.269565e-05 w: [1.0111822] b: [-0.02541987]\n",
            "step: 1100 cost: 8.41893e-05 w: [1.0106568] b: [-0.02422526]\n",
            "step: 1120 cost: 7.646373e-05 w: [1.0101562] b: [-0.02308688]\n",
            "step: 1140 cost: 6.944507e-05 w: [1.0096786] b: [-0.02200195]\n",
            "step: 1160 cost: 6.306978e-05 w: [1.0092238] b: [-0.02096789]\n",
            "step: 1180 cost: 5.728142e-05 w: [1.0087903] b: [-0.01998246]\n",
            "step: 1200 cost: 5.2024337e-05 w: [1.0083772] b: [-0.01904335]\n",
            "step: 1220 cost: 4.7249352e-05 w: [1.0079834] b: [-0.01814838]\n",
            "step: 1240 cost: 4.2912387e-05 w: [1.0076083] b: [-0.01729547]\n",
            "step: 1260 cost: 3.897308e-05 w: [1.0072507] b: [-0.01648264]\n",
            "step: 1280 cost: 3.539734e-05 w: [1.0069101] b: [-0.01570806]\n",
            "step: 1300 cost: 3.2148757e-05 w: [1.0065854] b: [-0.01496992]\n",
            "step: 1320 cost: 2.9197465e-05 w: [1.0062759] b: [-0.01426642]\n",
            "step: 1340 cost: 2.6517737e-05 w: [1.005981] b: [-0.01359597]\n",
            "step: 1360 cost: 2.4084375e-05 w: [1.0056999] b: [-0.01295707]\n",
            "step: 1380 cost: 2.1873166e-05 w: [1.0054319] b: [-0.01234811]\n",
            "step: 1400 cost: 1.9866444e-05 w: [1.0051767] b: [-0.01176783]\n",
            "step: 1420 cost: 1.8042996e-05 w: [1.0049334] b: [-0.0112148]\n",
            "step: 1440 cost: 1.6386099e-05 w: [1.0047015] b: [-0.01068774]\n",
            "step: 1460 cost: 1.488239e-05 w: [1.0044805] b: [-0.0101854]\n",
            "step: 1480 cost: 1.3516248e-05 w: [1.00427] b: [-0.00970671]\n",
            "step: 1500 cost: 1.2276071e-05 w: [1.0040694] b: [-0.00925055]\n",
            "step: 1520 cost: 1.1149526e-05 w: [1.0038781] b: [-0.00881586]\n",
            "step: 1540 cost: 1.0125576e-05 w: [1.0036958] b: [-0.00840156]\n",
            "step: 1560 cost: 9.196692e-06 w: [1.0035223] b: [-0.00800679]\n",
            "step: 1580 cost: 8.352511e-06 w: [1.0033567] b: [-0.0076305]\n",
            "step: 1600 cost: 7.5860867e-06 w: [1.003199] b: [-0.00727195]\n",
            "step: 1620 cost: 6.88985e-06 w: [1.0030487] b: [-0.00693027]\n",
            "step: 1640 cost: 6.258027e-06 w: [1.0029055] b: [-0.00660465]\n",
            "step: 1660 cost: 5.683765e-06 w: [1.002769] b: [-0.00629434]\n",
            "step: 1680 cost: 5.162045e-06 w: [1.0026388] b: [-0.00599859]\n",
            "step: 1700 cost: 4.68816e-06 w: [1.0025148] b: [-0.00571671]\n",
            "step: 1720 cost: 4.2580164e-06 w: [1.0023966] b: [-0.00544807]\n",
            "step: 1740 cost: 3.867226e-06 w: [1.002284] b: [-0.00519206]\n",
            "step: 1760 cost: 3.5124287e-06 w: [1.0021768] b: [-0.00494809]\n",
            "step: 1780 cost: 3.189941e-06 w: [1.0020745] b: [-0.00471559]\n",
            "step: 1800 cost: 2.897218e-06 w: [1.001977] b: [-0.00449399]\n",
            "step: 1820 cost: 2.6315795e-06 w: [1.0018841] b: [-0.00428281]\n",
            "step: 1840 cost: 2.3899343e-06 w: [1.0017956] b: [-0.00408161]\n",
            "step: 1860 cost: 2.1707433e-06 w: [1.0017111] b: [-0.00388986]\n",
            "step: 1880 cost: 1.9716733e-06 w: [1.0016309] b: [-0.00370714]\n",
            "step: 1900 cost: 1.7907236e-06 w: [1.0015543] b: [-0.00353298]\n",
            "step: 1920 cost: 1.6264777e-06 w: [1.0014813] b: [-0.00336703]\n",
            "step: 1940 cost: 1.4772226e-06 w: [1.0014116] b: [-0.00320888]\n",
            "step: 1960 cost: 1.341712e-06 w: [1.0013454] b: [-0.00305814]\n",
            "step: 1980 cost: 1.2185493e-06 w: [1.0012822] b: [-0.00291448]\n",
            "step: 2000 cost: 1.106765e-06 w: [1.0012219] b: [-0.00277757]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "zwDZ9AC6c-Jj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6013769a-0a86-4c76-8160-36c65d7c871c"
      },
      "source": [
        "#H(x) = Wx + b\n",
        "x = tf.placeholder(tf.float32, shape=[None])\n",
        "y = tf.placeholder(tf.float32, shape=[None])\n",
        "w = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "hypothesis = x * w + b\n",
        "\n",
        "#Cost(W,b)=∑...\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
        "\n",
        "#Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "  cost_val, w_val, b_val, _ = sess.run([cost, w, b, train],\n",
        "      feed_dict={x: [2, 2.1, 2.2], y: [11, 12, 13]})\n",
        "  if step % 20 == 0:\n",
        "    print(\"step:\", step, \"cost:\", cost_val, \"w:\", w_val, \"b:\", b_val)\n",
        "\n",
        "#11.97 = 2.1 * 4.7 + 2.1\n",
        "#y = x * w + b"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 cost: 192.79057 w: [0.16918574] b: [-0.7116902]\n",
            "step: 20 cost: 2.133228 w: [4.4891696] b: [1.3365304]\n",
            "step: 40 cost: 0.18844186 w: [4.9276814] b: [1.5387093]\n",
            "step: 60 cost: 0.16844177 w: [4.9741907] b: [1.5544536]\n",
            "step: 80 cost: 0.16807397 w: [4.98111] b: [1.5513713]\n",
            "step: 100 cost: 0.1679064 w: [4.98403] b: [1.5463904]\n",
            "step: 120 cost: 0.16774124 w: [4.986544] b: [1.5412202]\n",
            "step: 140 cost: 0.16757601 w: [4.9890165] b: [1.5360327]\n",
            "step: 160 cost: 0.16741097 w: [4.9914856] b: [1.5308458]\n",
            "step: 180 cost: 0.16724616 w: [4.993951] b: [1.5256611]\n",
            "step: 200 cost: 0.16708177 w: [4.9964147] b: [1.5204796]\n",
            "step: 220 cost: 0.16691716 w: [4.9988766] b: [1.515301]\n",
            "step: 240 cost: 0.16675295 w: [5.001338] b: [1.5101255]\n",
            "step: 260 cost: 0.16658853 w: [5.0037985] b: [1.5049528]\n",
            "step: 280 cost: 0.16642486 w: [5.006259] b: [1.4997815]\n",
            "step: 300 cost: 0.166261 w: [5.0087185] b: [1.4946123]\n",
            "step: 320 cost: 0.1660974 w: [5.011176] b: [1.4894451]\n",
            "step: 340 cost: 0.16593389 w: [5.0136313] b: [1.4842808]\n",
            "step: 360 cost: 0.16577046 w: [5.016085] b: [1.4791198]\n",
            "step: 380 cost: 0.16560741 w: [5.018538] b: [1.4739616]\n",
            "step: 400 cost: 0.16544448 w: [5.0209894] b: [1.4688065]\n",
            "step: 420 cost: 0.16528167 w: [5.0234404] b: [1.463654]\n",
            "step: 440 cost: 0.16511899 w: [5.0258913] b: [1.4585035]\n",
            "step: 460 cost: 0.16495635 w: [5.0283413] b: [1.4533538]\n",
            "step: 480 cost: 0.16479376 w: [5.030789] b: [1.4482069]\n",
            "step: 500 cost: 0.1646318 w: [5.0332346] b: [1.4430631]\n",
            "step: 520 cost: 0.16446966 w: [5.035679] b: [1.4379222]\n",
            "step: 540 cost: 0.1643079 w: [5.038122] b: [1.4327844]\n",
            "step: 560 cost: 0.16414593 w: [5.040564] b: [1.4276496]\n",
            "step: 580 cost: 0.16398437 w: [5.0430055] b: [1.4225172]\n",
            "step: 600 cost: 0.16382296 w: [5.045447] b: [1.4173864]\n",
            "step: 620 cost: 0.16366191 w: [5.0478873] b: [1.4122573]\n",
            "step: 640 cost: 0.16350056 w: [5.0503244] b: [1.4071306]\n",
            "step: 660 cost: 0.16334021 w: [5.052761] b: [1.4020071]\n",
            "step: 680 cost: 0.163179 w: [5.055196] b: [1.3968867]\n",
            "step: 700 cost: 0.16301854 w: [5.057629] b: [1.391769]\n",
            "step: 720 cost: 0.16285785 w: [5.0600615] b: [1.3866544]\n",
            "step: 740 cost: 0.16269757 w: [5.0624933] b: [1.3815422]\n",
            "step: 760 cost: 0.1625374 w: [5.064925] b: [1.376432]\n",
            "step: 780 cost: 0.1623776 w: [5.0673556] b: [1.3713229]\n",
            "step: 800 cost: 0.16221781 w: [5.0697837] b: [1.3662168]\n",
            "step: 820 cost: 0.16205812 w: [5.07221] b: [1.3611135]\n",
            "step: 840 cost: 0.16189848 w: [5.074635] b: [1.3560131]\n",
            "step: 860 cost: 0.16173925 w: [5.077059] b: [1.3509159]\n",
            "step: 880 cost: 0.1615801 w: [5.0794816] b: [1.3458216]\n",
            "step: 900 cost: 0.1614211 w: [5.081904] b: [1.3407296]\n",
            "step: 920 cost: 0.1612622 w: [5.0843263] b: [1.3356394]\n",
            "step: 940 cost: 0.16110305 w: [5.086747] b: [1.33055]\n",
            "step: 960 cost: 0.16094483 w: [5.0891657] b: [1.3254635]\n",
            "step: 980 cost: 0.16078635 w: [5.0915833] b: [1.3203802]\n",
            "step: 1000 cost: 0.16062798 w: [5.093999] b: [1.3153]\n",
            "step: 1020 cost: 0.16047 w: [5.0964127] b: [1.3102229]\n",
            "step: 1040 cost: 0.16031212 w: [5.098826] b: [1.3051485]\n",
            "step: 1060 cost: 0.16015436 w: [5.1012387] b: [1.3000766]\n",
            "step: 1080 cost: 0.15999669 w: [5.1036515] b: [1.2950059]\n",
            "step: 1100 cost: 0.15983908 w: [5.106063] b: [1.2899371]\n",
            "step: 1120 cost: 0.15968181 w: [5.1084723] b: [1.2848709]\n",
            "step: 1140 cost: 0.15952462 w: [5.1108794] b: [1.2798077]\n",
            "step: 1160 cost: 0.15936749 w: [5.1132855] b: [1.2747473]\n",
            "step: 1180 cost: 0.15921073 w: [5.1156907] b: [1.2696898]\n",
            "step: 1200 cost: 0.15905409 w: [5.118094] b: [1.2646352]\n",
            "step: 1220 cost: 0.15889724 w: [5.120497] b: [1.2595831]\n",
            "step: 1240 cost: 0.15874115 w: [5.1229005] b: [1.2545332]\n",
            "step: 1260 cost: 0.1585848 w: [5.1253023] b: [1.2494841]\n",
            "step: 1280 cost: 0.15842877 w: [5.127702] b: [1.2444376]\n",
            "step: 1300 cost: 0.15827279 w: [5.1301003] b: [1.2393941]\n",
            "step: 1320 cost: 0.15811688 w: [5.1324973] b: [1.2343534]\n",
            "step: 1340 cost: 0.15796138 w: [5.1348925] b: [1.2293161]\n",
            "step: 1360 cost: 0.15780598 w: [5.1372867] b: [1.2242815]\n",
            "step: 1380 cost: 0.15765066 w: [5.1396804] b: [1.2192492]\n",
            "step: 1400 cost: 0.15749548 w: [5.142074] b: [1.2142186]\n",
            "step: 1420 cost: 0.15734036 w: [5.144467] b: [1.2091899]\n",
            "step: 1440 cost: 0.15718557 w: [5.1468573] b: [1.2041633]\n",
            "step: 1460 cost: 0.1570308 w: [5.1492457] b: [1.1991395]\n",
            "step: 1480 cost: 0.15687619 w: [5.151633] b: [1.1941191]\n",
            "step: 1500 cost: 0.1567219 w: [5.154019] b: [1.1891015]\n",
            "step: 1520 cost: 0.15656771 w: [5.1564035] b: [1.1840866]\n",
            "step: 1540 cost: 0.15641363 w: [5.1587877] b: [1.1790743]\n",
            "step: 1560 cost: 0.15625967 w: [5.161172] b: [1.1740639]\n",
            "step: 1580 cost: 0.1561058 w: [5.1635556] b: [1.1690547]\n",
            "step: 1600 cost: 0.1559519 w: [5.165936] b: [1.1640477]\n",
            "step: 1620 cost: 0.15579838 w: [5.1683154] b: [1.1590439]\n",
            "step: 1640 cost: 0.15564527 w: [5.1706934] b: [1.1540431]\n",
            "step: 1660 cost: 0.15549189 w: [5.1730695] b: [1.1490449]\n",
            "step: 1680 cost: 0.15533893 w: [5.175445] b: [1.1440495]\n",
            "step: 1700 cost: 0.15518636 w: [5.1778197] b: [1.1390569]\n",
            "step: 1720 cost: 0.1550333 w: [5.1801944] b: [1.1340667]\n",
            "step: 1740 cost: 0.15488067 w: [5.182569] b: [1.129077]\n",
            "step: 1760 cost: 0.1547283 w: [5.1849403] b: [1.1240897]\n",
            "step: 1780 cost: 0.15457599 w: [5.1873107] b: [1.1191053]\n",
            "step: 1800 cost: 0.15442377 w: [5.189679] b: [1.114124]\n",
            "step: 1820 cost: 0.15427193 w: [5.192046] b: [1.1091453]\n",
            "step: 1840 cost: 0.15411988 w: [5.194412] b: [1.1041697]\n",
            "step: 1860 cost: 0.1539682 w: [5.1967773] b: [1.099197]\n",
            "step: 1880 cost: 0.15381669 w: [5.1991425] b: [1.094226]\n",
            "step: 1900 cost: 0.15366524 w: [5.2015076] b: [1.089257]\n",
            "step: 1920 cost: 0.15351407 w: [5.20387] b: [1.0842893]\n",
            "step: 1940 cost: 0.15336299 w: [5.2062306] b: [1.0793244]\n",
            "step: 1960 cost: 0.153212 w: [5.20859] b: [1.0743623]\n",
            "step: 1980 cost: 0.15306108 w: [5.210948] b: [1.0694033]\n",
            "step: 2000 cost: 0.15291019 w: [5.2133045] b: [1.0644472]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk0wNAwMj5Ij",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_OUD--UkB8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import matplotlib.pyplot as plt\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm7M9B0u9Rvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "da97c5b9-b853-4700-de5b-e79671e3e4d3"
      },
      "source": [
        "x = [1, 2, 3]\n",
        "y = [1, 2, 3]\n",
        "W = tf.placeholder(tf.float32)\n",
        "hypothesis = x * W\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "W_val = []\n",
        "cost_val = []\n",
        "for i in range(-30, 50):\n",
        "  feed_W = i * 0.1\n",
        "  curr_cost, curr_W = sess.run([cost, W], feed_dict={W : feed_W})\n",
        "  W_val.append(curr_W) #.append()\n",
        "  cost_val.append(curr_cost)\n",
        "\n",
        "plt.plot(W_val, cost_val) #.plot()\n",
        "plt.show() #.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eta9cwISElXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ef994a30-c49b-42b3-a776-e4536550dadc"
      },
      "source": [
        "x_data = [1, 2, 3]\n",
        "y_data = [1, 2, 3]\n",
        "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "hypothesis = x * W\n",
        "\n",
        "cost = tf.reduce_sum(tf.square(hypothesis -y)) #.reduce_sum()\n",
        "\n",
        "learning_rate = 0.1\n",
        "gradient = tf.reduce_mean((W * x - y) * x)\n",
        "descent = W - learning_rate * gradient\n",
        "update = W.assign(descent) #.assign()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(21):\n",
        "  sess.run(update, feed_dict={x: x_data, y: y_data})\n",
        "  print(\"step:\", step, \"cost:\", sess.run(cost, feed_dict={x: x_data, y: y_data}), \"W:\", sess.run(W))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 cost: 0.3761779 W: [0.8360797]\n",
            "step: 1 cost: 0.10700171 W: [0.91257584]\n",
            "step: 2 cost: 0.030436082 W: [0.9533738]\n",
            "step: 3 cost: 0.008657354 W: [0.9751327]\n",
            "step: 4 cost: 0.0024625454 W: [0.98673743]\n",
            "step: 5 cost: 0.00070044794 W: [0.99292666]\n",
            "step: 6 cost: 0.00019923665 W: [0.99622756]\n",
            "step: 7 cost: 5.667075e-05 W: [0.99798805]\n",
            "step: 8 cost: 1.6120097e-05 W: [0.99892694]\n",
            "step: 9 cost: 4.5861725e-06 W: [0.9994277]\n",
            "step: 10 cost: 1.3044701e-06 W: [0.99969476]\n",
            "step: 11 cost: 3.709057e-07 W: [0.9998372]\n",
            "step: 12 cost: 1.056173e-07 W: [0.99991316]\n",
            "step: 13 cost: 3.0044834e-08 W: [0.9999537]\n",
            "step: 14 cost: 8.507257e-09 W: [0.9999753]\n",
            "step: 15 cost: 2.4339677e-09 W: [0.9999868]\n",
            "step: 16 cost: 6.8753536e-10 W: [0.99999297]\n",
            "step: 17 cost: 1.9607072e-10 W: [0.99999624]\n",
            "step: 18 cost: 5.8960836e-11 W: [0.999998]\n",
            "step: 19 cost: 1.6896706e-11 W: [0.9999989]\n",
            "step: 20 cost: 4.2241766e-12 W: [0.99999946]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWjbwVe_L4NI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d4127dd1-7355-43a5-a4a8-0d7aefa6678d"
      },
      "source": [
        "x = [1, 2, 3]\n",
        "y = [1, 2, 3]\n",
        "W = tf.Variable(5.0)\n",
        "hypothesis = x * W\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis -y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(100):\n",
        "  print(\"step:\", step, \"W:\", sess.run(W))\n",
        "  sess.run(train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 W: 5.0\n",
            "step: 1 W: 1.2666664\n",
            "step: 2 W: 1.0177778\n",
            "step: 3 W: 1.0011852\n",
            "step: 4 W: 1.000079\n",
            "step: 5 W: 1.0000052\n",
            "step: 6 W: 1.0000004\n",
            "step: 7 W: 1.0\n",
            "step: 8 W: 1.0\n",
            "step: 9 W: 1.0\n",
            "step: 10 W: 1.0\n",
            "step: 11 W: 1.0\n",
            "step: 12 W: 1.0\n",
            "step: 13 W: 1.0\n",
            "step: 14 W: 1.0\n",
            "step: 15 W: 1.0\n",
            "step: 16 W: 1.0\n",
            "step: 17 W: 1.0\n",
            "step: 18 W: 1.0\n",
            "step: 19 W: 1.0\n",
            "step: 20 W: 1.0\n",
            "step: 21 W: 1.0\n",
            "step: 22 W: 1.0\n",
            "step: 23 W: 1.0\n",
            "step: 24 W: 1.0\n",
            "step: 25 W: 1.0\n",
            "step: 26 W: 1.0\n",
            "step: 27 W: 1.0\n",
            "step: 28 W: 1.0\n",
            "step: 29 W: 1.0\n",
            "step: 30 W: 1.0\n",
            "step: 31 W: 1.0\n",
            "step: 32 W: 1.0\n",
            "step: 33 W: 1.0\n",
            "step: 34 W: 1.0\n",
            "step: 35 W: 1.0\n",
            "step: 36 W: 1.0\n",
            "step: 37 W: 1.0\n",
            "step: 38 W: 1.0\n",
            "step: 39 W: 1.0\n",
            "step: 40 W: 1.0\n",
            "step: 41 W: 1.0\n",
            "step: 42 W: 1.0\n",
            "step: 43 W: 1.0\n",
            "step: 44 W: 1.0\n",
            "step: 45 W: 1.0\n",
            "step: 46 W: 1.0\n",
            "step: 47 W: 1.0\n",
            "step: 48 W: 1.0\n",
            "step: 49 W: 1.0\n",
            "step: 50 W: 1.0\n",
            "step: 51 W: 1.0\n",
            "step: 52 W: 1.0\n",
            "step: 53 W: 1.0\n",
            "step: 54 W: 1.0\n",
            "step: 55 W: 1.0\n",
            "step: 56 W: 1.0\n",
            "step: 57 W: 1.0\n",
            "step: 58 W: 1.0\n",
            "step: 59 W: 1.0\n",
            "step: 60 W: 1.0\n",
            "step: 61 W: 1.0\n",
            "step: 62 W: 1.0\n",
            "step: 63 W: 1.0\n",
            "step: 64 W: 1.0\n",
            "step: 65 W: 1.0\n",
            "step: 66 W: 1.0\n",
            "step: 67 W: 1.0\n",
            "step: 68 W: 1.0\n",
            "step: 69 W: 1.0\n",
            "step: 70 W: 1.0\n",
            "step: 71 W: 1.0\n",
            "step: 72 W: 1.0\n",
            "step: 73 W: 1.0\n",
            "step: 74 W: 1.0\n",
            "step: 75 W: 1.0\n",
            "step: 76 W: 1.0\n",
            "step: 77 W: 1.0\n",
            "step: 78 W: 1.0\n",
            "step: 79 W: 1.0\n",
            "step: 80 W: 1.0\n",
            "step: 81 W: 1.0\n",
            "step: 82 W: 1.0\n",
            "step: 83 W: 1.0\n",
            "step: 84 W: 1.0\n",
            "step: 85 W: 1.0\n",
            "step: 86 W: 1.0\n",
            "step: 87 W: 1.0\n",
            "step: 88 W: 1.0\n",
            "step: 89 W: 1.0\n",
            "step: 90 W: 1.0\n",
            "step: 91 W: 1.0\n",
            "step: 92 W: 1.0\n",
            "step: 93 W: 1.0\n",
            "step: 94 W: 1.0\n",
            "step: 95 W: 1.0\n",
            "step: 96 W: 1.0\n",
            "step: 97 W: 1.0\n",
            "step: 98 W: 1.0\n",
            "step: 99 W: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nkdDJZ-fOwxf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "05eb35c8-07a7-476f-e222-4e495ffcaeb9"
      },
      "source": [
        "x = [1, 2, 3]\n",
        "y = [1, 2, 3]\n",
        "W = tf.Variable(5.0)\n",
        "hypothesis = x * W\n",
        "gradient = tf.reduce_mean((W * x - y) * x) * 2\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis -y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "gvs = optimizer.compute_gradients(cost, [W]) #.compute_gradients()\n",
        "apply_gradients = optimizer.apply_gradients(gvs) #.apply_gradients()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(100):\n",
        "  print(step, sess.run([gradient, W, gvs]))\n",
        "  sess.run(apply_gradients)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [37.333332, 5.0, [(37.333336, 5.0)]]\n",
            "1 [33.84889, 4.6266665, [(33.84889, 4.6266665)]]\n",
            "2 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]\n",
            "3 [27.825289, 3.981281, [(27.825289, 3.981281)]]\n",
            "4 [25.228264, 3.7030282, [(25.228264, 3.7030282)]]\n",
            "5 [22.873625, 3.4507456, [(22.873627, 3.4507456)]]\n",
            "6 [20.738754, 3.2220094, [(20.738754, 3.2220094)]]\n",
            "7 [18.803139, 3.014622, [(18.803139, 3.014622)]]\n",
            "8 [17.04818, 2.8265905, [(17.04818, 2.8265905)]]\n",
            "9 [15.457016, 2.6561089, [(15.457016, 2.6561089)]]\n",
            "10 [14.014362, 2.5015388, [(14.014362, 2.5015388)]]\n",
            "11 [12.706355, 2.3613951, [(12.706355, 2.3613951)]]\n",
            "12 [11.520428, 2.2343316, [(11.520429, 2.2343316)]]\n",
            "13 [10.445188, 2.1191273, [(10.4451885, 2.1191273)]]\n",
            "14 [9.470304, 2.0146754, [(9.4703045, 2.0146754)]]\n",
            "15 [8.586408, 1.9199723, [(8.586408, 1.9199723)]]\n",
            "16 [7.78501, 1.8341082, [(7.7850103, 1.8341082)]]\n",
            "17 [7.058409, 1.7562581, [(7.0584097, 1.7562581)]]\n",
            "18 [6.3996243, 1.6856741, [(6.399625, 1.6856741)]]\n",
            "19 [5.8023267, 1.6216779, [(5.802327, 1.6216779)]]\n",
            "20 [5.2607765, 1.5636547, [(5.2607765, 1.5636547)]]\n",
            "21 [4.769771, 1.5110469, [(4.769771, 1.5110469)]]\n",
            "22 [4.3245926, 1.4633492, [(4.3245926, 1.4633492)]]\n",
            "23 [3.9209645, 1.4201033, [(3.9209647, 1.4201033)]]\n",
            "24 [3.555008, 1.3808937, [(3.555008, 1.3808937)]]\n",
            "25 [3.2232068, 1.3453436, [(3.223207, 1.3453436)]]\n",
            "26 [2.9223745, 1.3131115, [(2.9223745, 1.3131115)]]\n",
            "27 [2.6496189, 1.2838877, [(2.6496186, 1.2838877)]]\n",
            "28 [2.4023216, 1.2573916, [(2.4023216, 1.2573916)]]\n",
            "29 [2.178105, 1.2333684, [(2.178105, 1.2333684)]]\n",
            "30 [1.9748148, 1.2115873, [(1.9748147, 1.2115873)]]\n",
            "31 [1.7904993, 1.1918392, [(1.7904994, 1.1918392)]]\n",
            "32 [1.623386, 1.1739342, [(1.6233861, 1.1739342)]]\n",
            "33 [1.4718704, 1.1577004, [(1.4718704, 1.1577004)]]\n",
            "34 [1.3344965, 1.1429818, [(1.3344965, 1.1429818)]]\n",
            "35 [1.2099432, 1.1296368, [(1.2099432, 1.1296368)]]\n",
            "36 [1.0970153, 1.1175374, [(1.0970154, 1.1175374)]]\n",
            "37 [0.99462754, 1.1065673, [(0.9946276, 1.1065673)]]\n",
            "38 [0.90179634, 1.096621, [(0.90179634, 1.096621)]]\n",
            "39 [0.81762886, 1.0876031, [(0.81762886, 1.0876031)]]\n",
            "40 [0.7413165, 1.0794268, [(0.7413165, 1.0794268)]]\n",
            "41 [0.67212707, 1.0720136, [(0.6721271, 1.0720136)]]\n",
            "42 [0.6093953, 1.0652924, [(0.6093954, 1.0652924)]]\n",
            "43 [0.5525182, 1.0591984, [(0.55251825, 1.0591984)]]\n",
            "44 [0.50094914, 1.0536731, [(0.50094914, 1.0536731)]]\n",
            "45 [0.45419374, 1.0486636, [(0.45419377, 1.0486636)]]\n",
            "46 [0.41180158, 1.0441216, [(0.41180158, 1.0441216)]]\n",
            "47 [0.37336722, 1.0400037, [(0.37336725, 1.0400037)]]\n",
            "48 [0.33851996, 1.03627, [(0.33852, 1.03627)]]\n",
            "49 [0.30692515, 1.0328848, [(0.30692515, 1.0328848)]]\n",
            "50 [0.27827826, 1.0298156, [(0.2782783, 1.0298156)]]\n",
            "51 [0.25230527, 1.0270327, [(0.25230527, 1.0270327)]]\n",
            "52 [0.2287569, 1.0245097, [(0.2287569, 1.0245097)]]\n",
            "53 [0.20740573, 1.022222, [(0.20740573, 1.022222)]]\n",
            "54 [0.18804836, 1.020148, [(0.18804836, 1.020148)]]\n",
            "55 [0.17049654, 1.0182675, [(0.17049655, 1.0182675)]]\n",
            "56 [0.15458433, 1.0165626, [(0.15458433, 1.0165626)]]\n",
            "57 [0.14015675, 1.0150168, [(0.14015675, 1.0150168)]]\n",
            "58 [0.12707591, 1.0136153, [(0.12707591, 1.0136153)]]\n",
            "59 [0.11521538, 1.0123445, [(0.11521538, 1.0123445)]]\n",
            "60 [0.10446167, 1.0111923, [(0.10446167, 1.0111923)]]\n",
            "61 [0.09471202, 1.0101477, [(0.09471202, 1.0101477)]]\n",
            "62 [0.08587202, 1.0092006, [(0.08587202, 1.0092006)]]\n",
            "63 [0.07785805, 1.0083419, [(0.07785805, 1.0083419)]]\n",
            "64 [0.07059129, 1.0075634, [(0.07059129, 1.0075634)]]\n",
            "65 [0.06400236, 1.0068574, [(0.06400236, 1.0068574)]]\n",
            "66 [0.05802846, 1.0062174, [(0.05802846, 1.0062174)]]\n",
            "67 [0.052612226, 1.005637, [(0.052612226, 1.005637)]]\n",
            "68 [0.047702473, 1.005111, [(0.047702473, 1.005111)]]\n",
            "69 [0.043249767, 1.0046339, [(0.043249767, 1.0046339)]]\n",
            "70 [0.03921318, 1.0042014, [(0.03921318, 1.0042014)]]\n",
            "71 [0.035553534, 1.0038093, [(0.035553537, 1.0038093)]]\n",
            "72 [0.032236177, 1.0034539, [(0.03223618, 1.0034539)]]\n",
            "73 [0.029227654, 1.0031315, [(0.029227655, 1.0031315)]]\n",
            "74 [0.02649951, 1.0028392, [(0.02649951, 1.0028392)]]\n",
            "75 [0.024025917, 1.0025742, [(0.024025917, 1.0025742)]]\n",
            "76 [0.021783749, 1.002334, [(0.02178375, 1.002334)]]\n",
            "77 [0.01975123, 1.0021162, [(0.019751232, 1.0021162)]]\n",
            "78 [0.017907381, 1.0019187, [(0.017907381, 1.0019187)]]\n",
            "79 [0.016236702, 1.0017396, [(0.016236704, 1.0017396)]]\n",
            "80 [0.014720838, 1.0015773, [(0.014720838, 1.0015773)]]\n",
            "81 [0.01334699, 1.00143, [(0.013346991, 1.00143)]]\n",
            "82 [0.012100856, 1.0012965, [(0.012100856, 1.0012965)]]\n",
            "83 [0.010971785, 1.0011755, [(0.010971785, 1.0011755)]]\n",
            "84 [0.0099481745, 1.0010659, [(0.0099481745, 1.0010659)]]\n",
            "85 [0.009018898, 1.0009663, [(0.009018898, 1.0009663)]]\n",
            "86 [0.008176883, 1.0008761, [(0.008176884, 1.0008761)]]\n",
            "87 [0.007413149, 1.0007943, [(0.007413149, 1.0007943)]]\n",
            "88 [0.006721576, 1.0007201, [(0.006721576, 1.0007201)]]\n",
            "89 [0.0060940585, 1.0006529, [(0.0060940585, 1.0006529)]]\n",
            "90 [0.005525271, 1.000592, [(0.0055252714, 1.000592)]]\n",
            "91 [0.0050098896, 1.0005368, [(0.0050098896, 1.0005368)]]\n",
            "92 [0.004542589, 1.0004867, [(0.004542589, 1.0004867)]]\n",
            "93 [0.0041189194, 1.0004413, [(0.0041189194, 1.0004413)]]\n",
            "94 [0.0037339528, 1.0004001, [(0.003733953, 1.0004001)]]\n",
            "95 [0.0033854644, 1.0003628, [(0.0033854644, 1.0003628)]]\n",
            "96 [0.0030694802, 1.0003289, [(0.0030694804, 1.0003289)]]\n",
            "97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]\n",
            "98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]\n",
            "99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxR9_fEAu0Hz",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 04."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1yGz3Gq3u_",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 04-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiqLjOb1u5WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJwJqrPMu9xZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e4356ab6-3309-4285-810c-79b4a7833062"
      },
      "source": [
        "x1_data = [73, 93, 89, 96, 73]\n",
        "x2_data = [80, 88, 91, 98, 66]\n",
        "x3_data = [75, 93, 90, 100, 70]\n",
        "y_data = [152, 185, 180, 196, 142]\n",
        "\n",
        "x1 = tf.placeholder(tf.float32)\n",
        "x2 = tf.placeholder(tf.float32)\n",
        "x3 = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "\n",
        "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
        "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
        "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = x1*w1 + x2*w2 + x3*w3 + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis -y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for step in range(2001):\n",
        "  cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
        "                                 feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, y: y_data,})\n",
        "  if step % 10 == 0:\n",
        "    print(\"Step: \", step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:  0 Cost:  3482.2644 \n",
            "Prediction:\n",
            " [202.87988 248.2411  242.28265 263.43707 190.95135]\n",
            "Step:  10 Cost:  0.6831382 \n",
            "Prediction:\n",
            " [150.6857  185.50235 180.4678  196.12195 143.09647]\n",
            "Step:  20 Cost:  0.6485432 \n",
            "Prediction:\n",
            " [150.53053 185.31064 180.28163 195.91887 142.9492 ]\n",
            "Step:  30 Cost:  0.6458231 \n",
            "Prediction:\n",
            " [150.53282 185.30818 180.28192 195.91885 142.94629]\n",
            "Step:  40 Cost:  0.6431097 \n",
            "Prediction:\n",
            " [150.53558 185.30629 180.28275 195.91948 142.94382]\n",
            "Step:  50 Cost:  0.6404303 \n",
            "Prediction:\n",
            " [150.53833 185.30441 180.2836  195.92007 142.94138]\n",
            "Step:  60 Cost:  0.6377506 \n",
            "Prediction:\n",
            " [150.54108 185.30254 180.28444 195.92067 142.93892]\n",
            "Step:  70 Cost:  0.6350961 \n",
            "Prediction:\n",
            " [150.5438  185.30066 180.28528 195.92126 142.93646]\n",
            "Step:  80 Cost:  0.63245255 \n",
            "Prediction:\n",
            " [150.54652 185.2988  180.28612 195.92188 142.93404]\n",
            "Step:  90 Cost:  0.629811 \n",
            "Prediction:\n",
            " [150.54926 185.29694 180.28696 195.92247 142.9316 ]\n",
            "Step:  100 Cost:  0.6272005 \n",
            "Prediction:\n",
            " [150.55196 185.29507 180.2878  195.92305 142.92917]\n",
            "Step:  110 Cost:  0.62459576 \n",
            "Prediction:\n",
            " [150.55466 185.29323 180.2886  195.92361 142.92674]\n",
            "Step:  120 Cost:  0.62201494 \n",
            "Prediction:\n",
            " [150.55736 185.2914  180.28946 195.92422 142.92435]\n",
            "Step:  130 Cost:  0.61943763 \n",
            "Prediction:\n",
            " [150.56004 185.28954 180.29028 195.92482 142.92194]\n",
            "Step:  140 Cost:  0.6168696 \n",
            "Prediction:\n",
            " [150.56273 185.2877  180.29109 195.9254  142.91953]\n",
            "Step:  150 Cost:  0.61432344 \n",
            "Prediction:\n",
            " [150.5654  185.28586 180.29192 195.92596 142.91713]\n",
            "Step:  160 Cost:  0.61179006 \n",
            "Prediction:\n",
            " [150.56807 185.28404 180.29272 195.92657 142.91475]\n",
            "Step:  170 Cost:  0.6092753 \n",
            "Prediction:\n",
            " [150.57072 185.28223 180.29355 195.92714 142.91237]\n",
            "Step:  180 Cost:  0.60676396 \n",
            "Prediction:\n",
            " [150.57338 185.2804  180.29436 195.9277  142.90999]\n",
            "Step:  190 Cost:  0.60427606 \n",
            "Prediction:\n",
            " [150.57602 185.2786  180.29517 195.9283  142.90762]\n",
            "Step:  200 Cost:  0.6017937 \n",
            "Prediction:\n",
            " [150.57864 185.27676 180.29596 195.92886 142.90524]\n",
            "Step:  210 Cost:  0.5993358 \n",
            "Prediction:\n",
            " [150.58127 185.27496 180.29678 195.92944 142.9029 ]\n",
            "Step:  220 Cost:  0.596876 \n",
            "Prediction:\n",
            " [150.58391 185.27318 180.29759 195.93001 142.90054]\n",
            "Step:  230 Cost:  0.59443724 \n",
            "Prediction:\n",
            " [150.58653 185.2714  180.2984  195.93057 142.89821]\n",
            "Step:  240 Cost:  0.59201014 \n",
            "Prediction:\n",
            " [150.58914 185.26959 180.2992  195.93114 142.89587]\n",
            "Step:  250 Cost:  0.58959115 \n",
            "Prediction:\n",
            " [150.59175 185.2678  180.29999 195.9317  142.89354]\n",
            "Step:  260 Cost:  0.5871954 \n",
            "Prediction:\n",
            " [150.59435 185.26604 180.3008  195.9323  142.89122]\n",
            "Step:  270 Cost:  0.5848144 \n",
            "Prediction:\n",
            " [150.59692 185.26427 180.30159 195.93283 142.8889 ]\n",
            "Step:  280 Cost:  0.58243924 \n",
            "Prediction:\n",
            " [150.5995  185.2625  180.30238 195.9334  142.88658]\n",
            "Step:  290 Cost:  0.5800824 \n",
            "Prediction:\n",
            " [150.60208 185.26074 180.30318 195.93398 142.8843 ]\n",
            "Step:  300 Cost:  0.5777201 \n",
            "Prediction:\n",
            " [150.60466 185.25899 180.30396 195.93452 142.88197]\n",
            "Step:  310 Cost:  0.57539046 \n",
            "Prediction:\n",
            " [150.60721 185.25722 180.30473 195.93507 142.87968]\n",
            "Step:  320 Cost:  0.57306314 \n",
            "Prediction:\n",
            " [150.60977 185.25548 180.30553 195.93564 142.8774 ]\n",
            "Step:  330 Cost:  0.570753 \n",
            "Prediction:\n",
            " [150.6123  185.25371 180.30629 195.93617 142.8751 ]\n",
            "Step:  340 Cost:  0.5684587 \n",
            "Prediction:\n",
            " [150.61485 185.25198 180.30708 195.93675 142.87285]\n",
            "Step:  350 Cost:  0.5661777 \n",
            "Prediction:\n",
            " [150.61739 185.25026 180.30786 195.9373  142.87059]\n",
            "Step:  360 Cost:  0.5638962 \n",
            "Prediction:\n",
            " [150.61992 185.24852 180.30864 195.93785 142.86832]\n",
            "Step:  370 Cost:  0.5616345 \n",
            "Prediction:\n",
            " [150.62244 185.2468  180.3094  195.93839 142.86606]\n",
            "Step:  380 Cost:  0.55938876 \n",
            "Prediction:\n",
            " [150.62495 185.24509 180.3102  195.93895 142.86382]\n",
            "Step:  390 Cost:  0.5571459 \n",
            "Prediction:\n",
            " [150.62746 185.24335 180.31096 195.9395  142.86156]\n",
            "Step:  400 Cost:  0.55492973 \n",
            "Prediction:\n",
            " [150.62994 185.24164 180.3117  195.94002 142.85933]\n",
            "Step:  410 Cost:  0.55272055 \n",
            "Prediction:\n",
            " [150.63245 185.23994 180.31248 195.94058 142.85712]\n",
            "Step:  420 Cost:  0.5505198 \n",
            "Prediction:\n",
            " [150.63492 185.23822 180.31325 195.9411  142.85487]\n",
            "Step:  430 Cost:  0.54832715 \n",
            "Prediction:\n",
            " [150.6374  185.23653 180.314   195.94167 142.85266]\n",
            "Step:  440 Cost:  0.5461543 \n",
            "Prediction:\n",
            " [150.63988 185.23485 180.31477 195.94221 142.85045]\n",
            "Step:  450 Cost:  0.54397947 \n",
            "Prediction:\n",
            " [150.64235 185.23314 180.3155  195.94273 142.84824]\n",
            "Step:  460 Cost:  0.54183334 \n",
            "Prediction:\n",
            " [150.64479 185.23146 180.31627 195.94327 142.84602]\n",
            "Step:  470 Cost:  0.5396863 \n",
            "Prediction:\n",
            " [150.64726 185.2298  180.31702 195.9438  142.84384]\n",
            "Step:  480 Cost:  0.5375628 \n",
            "Prediction:\n",
            " [150.64969 185.22809 180.31778 195.94432 142.84164]\n",
            "Step:  490 Cost:  0.5354389 \n",
            "Prediction:\n",
            " [150.65215 185.22644 180.31854 195.94485 142.83946]\n",
            "Step:  500 Cost:  0.53333676 \n",
            "Prediction:\n",
            " [150.65457 185.22478 180.31927 195.9454  142.8373 ]\n",
            "Step:  510 Cost:  0.5312432 \n",
            "Prediction:\n",
            " [150.65698 185.2231  180.32    195.94589 142.83511]\n",
            "Step:  520 Cost:  0.529154 \n",
            "Prediction:\n",
            " [150.65941 185.22145 180.32074 195.94643 142.83295]\n",
            "Step:  530 Cost:  0.5270782 \n",
            "Prediction:\n",
            " [150.66183 185.21979 180.3215  195.94696 142.8308 ]\n",
            "Step:  540 Cost:  0.525022 \n",
            "Prediction:\n",
            " [150.66423 185.21814 180.32224 195.94748 142.82864]\n",
            "Step:  550 Cost:  0.5229642 \n",
            "Prediction:\n",
            " [150.66664 185.2165  180.32297 195.94801 142.82649]\n",
            "Step:  560 Cost:  0.52092725 \n",
            "Prediction:\n",
            " [150.66902 185.21486 180.3237  195.94853 142.82434]\n",
            "Step:  570 Cost:  0.51890236 \n",
            "Prediction:\n",
            " [150.67142 185.21324 180.32445 195.94905 142.82222]\n",
            "Step:  580 Cost:  0.5168807 \n",
            "Prediction:\n",
            " [150.6738  185.21161 180.32516 195.94957 142.82008]\n",
            "Step:  590 Cost:  0.51487994 \n",
            "Prediction:\n",
            " [150.67616 185.20998 180.3259  195.95007 142.81796]\n",
            "Step:  600 Cost:  0.51288074 \n",
            "Prediction:\n",
            " [150.67854 185.20837 180.32664 195.95059 142.81584]\n",
            "Step:  610 Cost:  0.51088774 \n",
            "Prediction:\n",
            " [150.68091 185.20676 180.32735 195.95111 142.81372]\n",
            "Step:  620 Cost:  0.5089127 \n",
            "Prediction:\n",
            " [150.68326 185.20511 180.32806 195.95163 142.81161]\n",
            "Step:  630 Cost:  0.5069549 \n",
            "Prediction:\n",
            " [150.68561 185.20352 180.3288  195.95215 142.80952]\n",
            "Step:  640 Cost:  0.5049952 \n",
            "Prediction:\n",
            " [150.68796 185.2019  180.32953 195.95265 142.80742]\n",
            "Step:  650 Cost:  0.5030612 \n",
            "Prediction:\n",
            " [150.69028 185.2003  180.33025 195.95316 142.80533]\n",
            "Step:  660 Cost:  0.50112003 \n",
            "Prediction:\n",
            " [150.69263 185.19872 180.33095 195.95367 142.80325]\n",
            "Step:  670 Cost:  0.49920592 \n",
            "Prediction:\n",
            " [150.69492 185.19711 180.33165 195.95415 142.80115]\n",
            "Step:  680 Cost:  0.4972914 \n",
            "Prediction:\n",
            " [150.69725 185.19554 180.33237 195.95467 142.79909]\n",
            "Step:  690 Cost:  0.49539018 \n",
            "Prediction:\n",
            " [150.69957 185.19397 180.33308 195.95517 142.79703]\n",
            "Step:  700 Cost:  0.49349886 \n",
            "Prediction:\n",
            " [150.70187 185.19238 180.33379 195.95567 142.79497]\n",
            "Step:  710 Cost:  0.4916238 \n",
            "Prediction:\n",
            " [150.70416 185.19081 180.3345  195.95618 142.79291]\n",
            "Step:  720 Cost:  0.48974285 \n",
            "Prediction:\n",
            " [150.70645 185.18921 180.33519 195.95667 142.79083]\n",
            "Step:  730 Cost:  0.4878859 \n",
            "Prediction:\n",
            " [150.70876 185.18768 180.33589 195.95717 142.78882]\n",
            "Step:  740 Cost:  0.48603886 \n",
            "Prediction:\n",
            " [150.71101 185.1861  180.3366  195.95766 142.78676]\n",
            "Step:  750 Cost:  0.4841918 \n",
            "Prediction:\n",
            " [150.7133  185.18452 180.3373  195.95815 142.78473]\n",
            "Step:  760 Cost:  0.4823676 \n",
            "Prediction:\n",
            " [150.71558 185.183   180.33801 195.95865 142.78271]\n",
            "Step:  770 Cost:  0.4805499 \n",
            "Prediction:\n",
            " [150.71782 185.18143 180.33868 195.95914 142.78069]\n",
            "Step:  780 Cost:  0.4787317 \n",
            "Prediction:\n",
            " [150.7201  185.1799  180.33937 195.95963 142.77867]\n",
            "Step:  790 Cost:  0.47693467 \n",
            "Prediction:\n",
            " [150.72234 185.17836 180.34007 195.96013 142.77666]\n",
            "Step:  800 Cost:  0.47514582 \n",
            "Prediction:\n",
            " [150.72456 185.1768  180.34074 195.96059 142.77464]\n",
            "Step:  810 Cost:  0.47337013 \n",
            "Prediction:\n",
            " [150.7268  185.1753  180.34145 195.96109 142.77266]\n",
            "Step:  820 Cost:  0.47159362 \n",
            "Prediction:\n",
            " [150.72903 185.17375 180.34213 195.96158 142.77065]\n",
            "Step:  830 Cost:  0.46982542 \n",
            "Prediction:\n",
            " [150.73128 185.17224 180.34282 195.96208 142.76866]\n",
            "Step:  840 Cost:  0.4680771 \n",
            "Prediction:\n",
            " [150.73347 185.1707  180.34349 195.96254 142.76666]\n",
            "Step:  850 Cost:  0.46633 \n",
            "Prediction:\n",
            " [150.7357  185.1692  180.34416 195.96303 142.7647 ]\n",
            "Step:  860 Cost:  0.46459085 \n",
            "Prediction:\n",
            " [150.73793 185.1677  180.34486 195.96352 142.76273]\n",
            "Step:  870 Cost:  0.4628725 \n",
            "Prediction:\n",
            " [150.7401  185.16615 180.3455  195.96397 142.76074]\n",
            "Step:  880 Cost:  0.46116012 \n",
            "Prediction:\n",
            " [150.74231 185.16469 180.34622 195.96446 142.75879]\n",
            "Step:  890 Cost:  0.45945674 \n",
            "Prediction:\n",
            " [150.74448 185.16316 180.34686 195.96492 142.75682]\n",
            "Step:  900 Cost:  0.4577588 \n",
            "Prediction:\n",
            " [150.74667 185.16168 180.34753 195.9654  142.75488]\n",
            "Step:  910 Cost:  0.45606726 \n",
            "Prediction:\n",
            " [150.74886 185.16019 180.3482  195.96588 142.75293]\n",
            "Step:  920 Cost:  0.45438614 \n",
            "Prediction:\n",
            " [150.75104 185.1587  180.34888 195.96635 142.75099]\n",
            "Step:  930 Cost:  0.45271736 \n",
            "Prediction:\n",
            " [150.75319 185.15721 180.34953 195.96681 142.74904]\n",
            "Step:  940 Cost:  0.45106062 \n",
            "Prediction:\n",
            " [150.75534 185.15573 180.3502  195.96727 142.7471 ]\n",
            "Step:  950 Cost:  0.44940385 \n",
            "Prediction:\n",
            " [150.7575  185.15427 180.35086 195.96776 142.74518]\n",
            "Step:  960 Cost:  0.44776616 \n",
            "Prediction:\n",
            " [150.75964 185.15279 180.35152 195.96822 142.74326]\n",
            "Step:  970 Cost:  0.4461306 \n",
            "Prediction:\n",
            " [150.7618  185.15132 180.35217 195.96869 142.74135]\n",
            "Step:  980 Cost:  0.44450933 \n",
            "Prediction:\n",
            " [150.76392 185.14984 180.35283 195.96915 142.73943]\n",
            "Step:  990 Cost:  0.44288802 \n",
            "Prediction:\n",
            " [150.76608 185.1484  180.3535  195.96962 142.73755]\n",
            "Step:  1000 Cost:  0.4412818 \n",
            "Prediction:\n",
            " [150.76819 185.14694 180.35414 195.97008 142.73563]\n",
            "Step:  1010 Cost:  0.43968892 \n",
            "Prediction:\n",
            " [150.77031 185.1455  180.3548  195.97054 142.73375]\n",
            "Step:  1020 Cost:  0.4380885 \n",
            "Prediction:\n",
            " [150.77243 185.14403 180.35544 195.97098 142.73184]\n",
            "Step:  1030 Cost:  0.43651772 \n",
            "Prediction:\n",
            " [150.77452 185.1426  180.35608 195.97145 142.72997]\n",
            "Step:  1040 Cost:  0.4349416 \n",
            "Prediction:\n",
            " [150.77663 185.14116 180.35674 195.97191 142.72807]\n",
            "Step:  1050 Cost:  0.4333765 \n",
            "Prediction:\n",
            " [150.77873 185.13972 180.35738 195.97237 142.72621]\n",
            "Step:  1060 Cost:  0.43182382 \n",
            "Prediction:\n",
            " [150.78082 185.1383  180.35802 195.97282 142.72435]\n",
            "Step:  1070 Cost:  0.43028027 \n",
            "Prediction:\n",
            " [150.78288 185.13686 180.35866 195.97324 142.72246]\n",
            "Step:  1080 Cost:  0.42873663 \n",
            "Prediction:\n",
            " [150.78497 185.13545 180.3593  195.97371 142.7206 ]\n",
            "Step:  1090 Cost:  0.42720023 \n",
            "Prediction:\n",
            " [150.78706 185.13402 180.35994 195.97417 142.71875]\n",
            "Step:  1100 Cost:  0.42568126 \n",
            "Prediction:\n",
            " [150.78912 185.1326  180.36057 195.97462 142.7169 ]\n",
            "Step:  1110 Cost:  0.42417103 \n",
            "Prediction:\n",
            " [150.79118 185.13118 180.36122 195.97507 142.71506]\n",
            "Step:  1120 Cost:  0.42265907 \n",
            "Prediction:\n",
            " [150.79324 185.12978 180.36183 195.97551 142.71321]\n",
            "Step:  1130 Cost:  0.42115974 \n",
            "Prediction:\n",
            " [150.7953  185.12837 180.36247 195.97597 142.71138]\n",
            "Step:  1140 Cost:  0.41967732 \n",
            "Prediction:\n",
            " [150.79733 185.12697 180.3631  195.9764  142.70955]\n",
            "Step:  1150 Cost:  0.41819352 \n",
            "Prediction:\n",
            " [150.79938 185.12556 180.36371 195.97684 142.70773]\n",
            "Step:  1160 Cost:  0.41672102 \n",
            "Prediction:\n",
            " [150.80142 185.12418 180.36436 195.9773  142.70592]\n",
            "Step:  1170 Cost:  0.41525108 \n",
            "Prediction:\n",
            " [150.80345 185.12279 180.36496 195.97774 142.7041 ]\n",
            "Step:  1180 Cost:  0.4137928 \n",
            "Prediction:\n",
            " [150.80547 185.1214  180.36559 195.97815 142.70227]\n",
            "Step:  1190 Cost:  0.4123439 \n",
            "Prediction:\n",
            " [150.8075  185.12001 180.36621 195.97858 142.70049]\n",
            "Step:  1200 Cost:  0.41090256 \n",
            "Prediction:\n",
            " [150.8095  185.11862 180.36682 195.97902 142.69867]\n",
            "Step:  1210 Cost:  0.40946263 \n",
            "Prediction:\n",
            " [150.81152 185.11726 180.36745 195.97948 142.69688]\n",
            "Step:  1220 Cost:  0.40804857 \n",
            "Prediction:\n",
            " [150.8135  185.1159  180.36807 195.97992 142.6951 ]\n",
            "Step:  1230 Cost:  0.40661898 \n",
            "Prediction:\n",
            " [150.8155  185.11452 180.36865 195.98032 142.6933 ]\n",
            "Step:  1240 Cost:  0.40521246 \n",
            "Prediction:\n",
            " [150.8175  185.11317 180.3693  195.98076 142.69153]\n",
            "Step:  1250 Cost:  0.40381107 \n",
            "Prediction:\n",
            " [150.81949 185.1118  180.3699  195.9812  142.68976]\n",
            "Step:  1260 Cost:  0.40241432 \n",
            "Prediction:\n",
            " [150.82147 185.11044 180.37051 195.98163 142.68799]\n",
            "Step:  1270 Cost:  0.40101987 \n",
            "Prediction:\n",
            " [150.82344 185.10909 180.3711  195.98204 142.6862 ]\n",
            "Step:  1280 Cost:  0.3996418 \n",
            "Prediction:\n",
            " [150.82542 185.10774 180.37172 195.98248 142.68446]\n",
            "Step:  1290 Cost:  0.3982708 \n",
            "Prediction:\n",
            " [150.82738 185.10638 180.37233 195.9829  142.6827 ]\n",
            "Step:  1300 Cost:  0.39690638 \n",
            "Prediction:\n",
            " [150.82935 185.10507 180.37294 195.98334 142.68095]\n",
            "Step:  1310 Cost:  0.39554948 \n",
            "Prediction:\n",
            " [150.83128 185.10371 180.37352 195.98375 142.6792 ]\n",
            "Step:  1320 Cost:  0.39419675 \n",
            "Prediction:\n",
            " [150.83324 185.10239 180.37411 195.98418 142.67746]\n",
            "Step:  1330 Cost:  0.39284998 \n",
            "Prediction:\n",
            " [150.83517 185.10104 180.37471 195.9846  142.6757 ]\n",
            "Step:  1340 Cost:  0.3915135 \n",
            "Prediction:\n",
            " [150.83713 185.09973 180.3753  195.98502 142.674  ]\n",
            "Step:  1350 Cost:  0.39018062 \n",
            "Prediction:\n",
            " [150.83905 185.09837 180.3759  195.98543 142.67224]\n",
            "Step:  1360 Cost:  0.3888612 \n",
            "Prediction:\n",
            " [150.84099 185.09708 180.37651 195.98586 142.67053]\n",
            "Step:  1370 Cost:  0.38754037 \n",
            "Prediction:\n",
            " [150.84291 185.09575 180.37708 195.98627 142.66881]\n",
            "Step:  1380 Cost:  0.38623917 \n",
            "Prediction:\n",
            " [150.84483 185.09445 180.37769 195.9867  142.66711]\n",
            "Step:  1390 Cost:  0.3849396 \n",
            "Prediction:\n",
            " [150.84673 185.09312 180.37825 195.98709 142.66539]\n",
            "Step:  1400 Cost:  0.38363844 \n",
            "Prediction:\n",
            " [150.84866 185.09183 180.37886 195.98752 142.6637 ]\n",
            "Step:  1410 Cost:  0.38235527 \n",
            "Prediction:\n",
            " [150.85054 185.09052 180.37941 195.9879  142.66199]\n",
            "Step:  1420 Cost:  0.3810734 \n",
            "Prediction:\n",
            " [150.85245 185.08923 180.38    195.98833 142.6603 ]\n",
            "Step:  1430 Cost:  0.3798062 \n",
            "Prediction:\n",
            " [150.85432 185.08794 180.38058 195.98874 142.6586 ]\n",
            "Step:  1440 Cost:  0.37853238 \n",
            "Prediction:\n",
            " [150.85623 185.08664 180.38116 195.98917 142.65692]\n",
            "Step:  1450 Cost:  0.3772768 \n",
            "Prediction:\n",
            " [150.8581  185.08534 180.38173 195.98955 142.65523]\n",
            "Step:  1460 Cost:  0.3760242 \n",
            "Prediction:\n",
            " [150.85999 185.08408 180.38232 195.98997 142.65356]\n",
            "Step:  1470 Cost:  0.3747794 \n",
            "Prediction:\n",
            " [150.86185 185.08278 180.38289 195.99036 142.65189]\n",
            "Step:  1480 Cost:  0.37354746 \n",
            "Prediction:\n",
            " [150.86371 185.08151 180.38348 195.99078 142.65022]\n",
            "Step:  1490 Cost:  0.37230703 \n",
            "Prediction:\n",
            " [150.86557 185.08022 180.38403 195.99117 142.64854]\n",
            "Step:  1500 Cost:  0.3710839 \n",
            "Prediction:\n",
            " [150.86743 185.07896 180.38461 195.99158 142.6469 ]\n",
            "Step:  1510 Cost:  0.36986428 \n",
            "Prediction:\n",
            " [150.86928 185.0777  180.38518 195.99197 142.64523]\n",
            "Step:  1520 Cost:  0.36864716 \n",
            "Prediction:\n",
            " [150.87115 185.07645 180.38577 195.99239 142.6436 ]\n",
            "Step:  1530 Cost:  0.36744276 \n",
            "Prediction:\n",
            " [150.87297 185.07516 180.3863  195.99277 142.64194]\n",
            "Step:  1540 Cost:  0.36625046 \n",
            "Prediction:\n",
            " [150.8748  185.07391 180.38689 195.99318 142.6403 ]\n",
            "Step:  1550 Cost:  0.3650524 \n",
            "Prediction:\n",
            " [150.87663 185.07265 180.38744 195.99356 142.63866]\n",
            "Step:  1560 Cost:  0.36386427 \n",
            "Prediction:\n",
            " [150.87846 185.0714  180.388   195.99396 142.63702]\n",
            "Step:  1570 Cost:  0.36268854 \n",
            "Prediction:\n",
            " [150.88028 185.07018 180.38855 195.99435 142.6354 ]\n",
            "Step:  1580 Cost:  0.36151263 \n",
            "Prediction:\n",
            " [150.88211 185.06891 180.38913 195.99475 142.63379]\n",
            "Step:  1590 Cost:  0.3603434 \n",
            "Prediction:\n",
            " [150.88391 185.06767 180.38966 195.99513 142.63216]\n",
            "Step:  1600 Cost:  0.35917914 \n",
            "Prediction:\n",
            " [150.88573 185.06644 180.39023 195.99553 142.63054]\n",
            "Step:  1610 Cost:  0.35802865 \n",
            "Prediction:\n",
            " [150.88753 185.0652  180.3908  195.99593 142.62894]\n",
            "Step:  1620 Cost:  0.3568793 \n",
            "Prediction:\n",
            " [150.88933 185.06398 180.39134 195.99632 142.62733]\n",
            "Step:  1630 Cost:  0.3557329 \n",
            "Prediction:\n",
            " [150.89111 185.06273 180.39188 195.99669 142.62572]\n",
            "Step:  1640 Cost:  0.35460275 \n",
            "Prediction:\n",
            " [150.8929  185.06151 180.39244 195.99707 142.62413]\n",
            "Step:  1650 Cost:  0.35347065 \n",
            "Prediction:\n",
            " [150.8947  185.0603  180.39299 195.99748 142.62256]\n",
            "Step:  1660 Cost:  0.35234532 \n",
            "Prediction:\n",
            " [150.89645 185.05908 180.39351 195.99785 142.62094]\n",
            "Step:  1670 Cost:  0.35122845 \n",
            "Prediction:\n",
            " [150.89824 185.05788 180.39407 195.99823 142.61937]\n",
            "Step:  1680 Cost:  0.35011882 \n",
            "Prediction:\n",
            " [150.9     185.05664 180.3946  195.9986  142.61778]\n",
            "Step:  1690 Cost:  0.34901264 \n",
            "Prediction:\n",
            " [150.90176 185.05545 180.39516 195.999   142.61621]\n",
            "Step:  1700 Cost:  0.34791598 \n",
            "Prediction:\n",
            " [150.9035  185.05423 180.39569 195.99936 142.61462]\n",
            "Step:  1710 Cost:  0.3468166 \n",
            "Prediction:\n",
            " [150.90529 185.05304 180.39624 195.99976 142.61308]\n",
            "Step:  1720 Cost:  0.3457279 \n",
            "Prediction:\n",
            " [150.90703 185.05185 180.39676 196.00012 142.61151]\n",
            "Step:  1730 Cost:  0.3446437 \n",
            "Prediction:\n",
            " [150.90878 185.05064 180.39731 196.00052 142.60995]\n",
            "Step:  1740 Cost:  0.34357366 \n",
            "Prediction:\n",
            " [150.9105  185.04945 180.39784 196.0009  142.6084 ]\n",
            "Step:  1750 Cost:  0.34249985 \n",
            "Prediction:\n",
            " [150.91225 185.04826 180.39838 196.00125 142.60684]\n",
            "Step:  1760 Cost:  0.34143344 \n",
            "Prediction:\n",
            " [150.91397 185.04709 180.3989  196.00162 142.60529]\n",
            "Step:  1770 Cost:  0.34037971 \n",
            "Prediction:\n",
            " [150.9157  185.04591 180.39943 196.00201 142.60376]\n",
            "Step:  1780 Cost:  0.3393244 \n",
            "Prediction:\n",
            " [150.91743 185.04472 180.39998 196.0024  142.60223]\n",
            "Step:  1790 Cost:  0.33827662 \n",
            "Prediction:\n",
            " [150.91914 185.04353 180.4005  196.00275 142.6007 ]\n",
            "Step:  1800 Cost:  0.33723587 \n",
            "Prediction:\n",
            " [150.92085 185.04237 180.40102 196.00313 142.59917]\n",
            "Step:  1810 Cost:  0.33620054 \n",
            "Prediction:\n",
            " [150.92255 185.0412  180.40154 196.0035  142.59763]\n",
            "Step:  1820 Cost:  0.33516762 \n",
            "Prediction:\n",
            " [150.92427 185.04005 180.40208 196.00388 142.59612]\n",
            "Step:  1830 Cost:  0.3341429 \n",
            "Prediction:\n",
            " [150.92595 185.03886 180.40257 196.00421 142.59459]\n",
            "Step:  1840 Cost:  0.33312306 \n",
            "Prediction:\n",
            " [150.92766 185.0377  180.4031  196.0046  142.5931 ]\n",
            "Step:  1850 Cost:  0.33211318 \n",
            "Prediction:\n",
            " [150.92934 185.03656 180.40363 196.00496 142.59158]\n",
            "Step:  1860 Cost:  0.3310968 \n",
            "Prediction:\n",
            " [150.93103 185.0354  180.40413 196.00533 142.59007]\n",
            "Step:  1870 Cost:  0.3300984 \n",
            "Prediction:\n",
            " [150.93271 185.03426 180.40466 196.00569 142.58858]\n",
            "Step:  1880 Cost:  0.32910043 \n",
            "Prediction:\n",
            " [150.93437 185.0331  180.40517 196.00603 142.58707]\n",
            "Step:  1890 Cost:  0.32810858 \n",
            "Prediction:\n",
            " [150.93605 185.03195 180.40569 196.0064  142.58559]\n",
            "Step:  1900 Cost:  0.32712 \n",
            "Prediction:\n",
            " [150.93771 185.03082 180.40619 196.00676 142.58409]\n",
            "Step:  1910 Cost:  0.32614014 \n",
            "Prediction:\n",
            " [150.93938 185.0297  180.40671 196.00711 142.58261]\n",
            "Step:  1920 Cost:  0.32516426 \n",
            "Prediction:\n",
            " [150.94102 185.02855 180.4072  196.00748 142.58113]\n",
            "Step:  1930 Cost:  0.3241896 \n",
            "Prediction:\n",
            " [150.94269 185.02742 180.40771 196.00781 142.57965]\n",
            "Step:  1940 Cost:  0.32322755 \n",
            "Prediction:\n",
            " [150.94434 185.02628 180.40823 196.00818 142.57819]\n",
            "Step:  1950 Cost:  0.32226855 \n",
            "Prediction:\n",
            " [150.94597 185.02515 180.40874 196.00851 142.5767 ]\n",
            "Step:  1960 Cost:  0.3213133 \n",
            "Prediction:\n",
            " [150.94762 185.02405 180.40924 196.00891 142.57526]\n",
            "Step:  1970 Cost:  0.32036033 \n",
            "Prediction:\n",
            " [150.94925 185.0229  180.40973 196.00925 142.57379]\n",
            "Step:  1980 Cost:  0.31941897 \n",
            "Prediction:\n",
            " [150.95088 185.0218  180.41025 196.0096  142.57234]\n",
            "Step:  1990 Cost:  0.31847495 \n",
            "Prediction:\n",
            " [150.95251 185.02069 180.41074 196.00995 142.57089]\n",
            "Step:  2000 Cost:  0.31753626 \n",
            "Prediction:\n",
            " [150.95413 185.01956 180.41122 196.01028 142.56943]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gZY6WQPyqvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d1fa4a8e-9909-4797-e12d-9f09845b26f9"
      },
      "source": [
        "x_data = [[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]\n",
        "y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "w = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(x, w)+b #.matmul()\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for step in range(2001):\n",
        "  cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={x: x_data, y: y_data})\n",
        "  if step % 10 == 0:\n",
        "    print(\"Step: \", step, \"Cost: \", cost_val, \"\\nPrediction\\n\", hy_val)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:  0 Cost:  23582.1 \n",
            "Prediction\n",
            " [[13.864965]\n",
            " [21.8033  ]\n",
            " [18.733358]\n",
            " [20.29496 ]\n",
            " [18.238739]]\n",
            "Step:  10 Cost:  1.2510335 \n",
            "Prediction\n",
            " [[149.71147]\n",
            " [185.0737 ]\n",
            " [179.6101 ]\n",
            " [195.48521]\n",
            " [142.77159]]\n",
            "Step:  20 Cost:  1.0303948 \n",
            "Prediction\n",
            " [[150.12616]\n",
            " [185.5652 ]\n",
            " [180.09799]\n",
            " [196.01605]\n",
            " [143.14516]]\n",
            "Step:  30 Cost:  1.0256412 \n",
            "Prediction\n",
            " [[150.13109]\n",
            " [185.5642 ]\n",
            " [180.1006 ]\n",
            " [196.01842]\n",
            " [143.14307]]\n",
            "Step:  40 Cost:  1.020928 \n",
            "Prediction\n",
            " [[150.13474]\n",
            " [185.56169]\n",
            " [180.10172]\n",
            " [196.01918]\n",
            " [143.13983]]\n",
            "Step:  50 Cost:  1.0162392 \n",
            "Prediction\n",
            " [[150.13837]\n",
            " [185.55919]\n",
            " [180.10284]\n",
            " [196.01991]\n",
            " [143.1366 ]]\n",
            "Step:  60 Cost:  1.0115693 \n",
            "Prediction\n",
            " [[150.14201]\n",
            " [185.5567 ]\n",
            " [180.10397]\n",
            " [196.02068]\n",
            " [143.13339]]\n",
            "Step:  70 Cost:  1.006926 \n",
            "Prediction\n",
            " [[150.14565]\n",
            " [185.55423]\n",
            " [180.10509]\n",
            " [196.02142]\n",
            " [143.13019]]\n",
            "Step:  80 Cost:  1.0023066 \n",
            "Prediction\n",
            " [[150.14926]\n",
            " [185.55176]\n",
            " [180.1062 ]\n",
            " [196.02219]\n",
            " [143.12698]]\n",
            "Step:  90 Cost:  0.9977182 \n",
            "Prediction\n",
            " [[150.15286]\n",
            " [185.54929]\n",
            " [180.10733]\n",
            " [196.02292]\n",
            " [143.1238 ]]\n",
            "Step:  100 Cost:  0.9931469 \n",
            "Prediction\n",
            " [[150.15648]\n",
            " [185.54683]\n",
            " [180.10843]\n",
            " [196.02367]\n",
            " [143.12064]]\n",
            "Step:  110 Cost:  0.9886085 \n",
            "Prediction\n",
            " [[150.16005]\n",
            " [185.54437]\n",
            " [180.10954]\n",
            " [196.0244 ]\n",
            " [143.11745]]\n",
            "Step:  120 Cost:  0.98408747 \n",
            "Prediction\n",
            " [[150.16364]\n",
            " [185.54193]\n",
            " [180.11064]\n",
            " [196.02515]\n",
            " [143.11429]]\n",
            "Step:  130 Cost:  0.9795898 \n",
            "Prediction\n",
            " [[150.1672 ]\n",
            " [185.53949]\n",
            " [180.11174]\n",
            " [196.0259 ]\n",
            " [143.11113]]\n",
            "Step:  140 Cost:  0.97512186 \n",
            "Prediction\n",
            " [[150.17078]\n",
            " [185.53706]\n",
            " [180.11285]\n",
            " [196.02663]\n",
            " [143.108  ]]\n",
            "Step:  150 Cost:  0.9706763 \n",
            "Prediction\n",
            " [[150.17433]\n",
            " [185.53464]\n",
            " [180.11395]\n",
            " [196.02736]\n",
            " [143.10487]]\n",
            "Step:  160 Cost:  0.9662534 \n",
            "Prediction\n",
            " [[150.17787]\n",
            " [185.53221]\n",
            " [180.11504]\n",
            " [196.0281 ]\n",
            " [143.10175]]\n",
            "Step:  170 Cost:  0.9618576 \n",
            "Prediction\n",
            " [[150.18138]\n",
            " [185.52979]\n",
            " [180.11612]\n",
            " [196.02882]\n",
            " [143.0986 ]]\n",
            "Step:  180 Cost:  0.957486 \n",
            "Prediction\n",
            " [[150.1849 ]\n",
            " [185.52737]\n",
            " [180.1172 ]\n",
            " [196.02954]\n",
            " [143.0955 ]]\n",
            "Step:  190 Cost:  0.9531306 \n",
            "Prediction\n",
            " [[150.18842]\n",
            " [185.52496]\n",
            " [180.11829]\n",
            " [196.03027]\n",
            " [143.09239]]\n",
            "Step:  200 Cost:  0.9487961 \n",
            "Prediction\n",
            " [[150.19193]\n",
            " [185.52257]\n",
            " [180.11935]\n",
            " [196.03099]\n",
            " [143.0893 ]]\n",
            "Step:  210 Cost:  0.9445058 \n",
            "Prediction\n",
            " [[150.1954 ]\n",
            " [185.52019]\n",
            " [180.12044]\n",
            " [196.03171]\n",
            " [143.08621]]\n",
            "Step:  220 Cost:  0.94021624 \n",
            "Prediction\n",
            " [[150.1989 ]\n",
            " [185.5178 ]\n",
            " [180.12152]\n",
            " [196.03244]\n",
            " [143.08313]]\n",
            "Step:  230 Cost:  0.9359579 \n",
            "Prediction\n",
            " [[150.20238]\n",
            " [185.51544]\n",
            " [180.12259]\n",
            " [196.03314]\n",
            " [143.08006]]\n",
            "Step:  240 Cost:  0.9317258 \n",
            "Prediction\n",
            " [[150.20584]\n",
            " [185.51306]\n",
            " [180.12366]\n",
            " [196.03386]\n",
            " [143.07701]]\n",
            "Step:  250 Cost:  0.927512 \n",
            "Prediction\n",
            " [[150.2093 ]\n",
            " [185.51071]\n",
            " [180.12474]\n",
            " [196.03458]\n",
            " [143.07396]]\n",
            "Step:  260 Cost:  0.92333347 \n",
            "Prediction\n",
            " [[150.21274]\n",
            " [185.50835]\n",
            " [180.1258 ]\n",
            " [196.0353 ]\n",
            " [143.07092]]\n",
            "Step:  270 Cost:  0.91915905 \n",
            "Prediction\n",
            " [[150.21619]\n",
            " [185.506  ]\n",
            " [180.12686]\n",
            " [196.03601]\n",
            " [143.06789]]\n",
            "Step:  280 Cost:  0.9150089 \n",
            "Prediction\n",
            " [[150.21962]\n",
            " [185.50366]\n",
            " [180.12791]\n",
            " [196.03671]\n",
            " [143.06485]]\n",
            "Step:  290 Cost:  0.91090167 \n",
            "Prediction\n",
            " [[150.22302]\n",
            " [185.50131]\n",
            " [180.12897]\n",
            " [196.03741]\n",
            " [143.06184]]\n",
            "Step:  300 Cost:  0.9067909 \n",
            "Prediction\n",
            " [[150.22644]\n",
            " [185.49898]\n",
            " [180.13002]\n",
            " [196.0381 ]\n",
            " [143.05882]]\n",
            "Step:  310 Cost:  0.90270805 \n",
            "Prediction\n",
            " [[150.22984]\n",
            " [185.49664]\n",
            " [180.13106]\n",
            " [196.0388 ]\n",
            " [143.05582]]\n",
            "Step:  320 Cost:  0.8986681 \n",
            "Prediction\n",
            " [[150.23322]\n",
            " [185.49432]\n",
            " [180.13211]\n",
            " [196.0395 ]\n",
            " [143.05283]]\n",
            "Step:  330 Cost:  0.89462984 \n",
            "Prediction\n",
            " [[150.23662]\n",
            " [185.49203]\n",
            " [180.13316]\n",
            " [196.0402 ]\n",
            " [143.04985]]\n",
            "Step:  340 Cost:  0.89062005 \n",
            "Prediction\n",
            " [[150.24   ]\n",
            " [185.48975]\n",
            " [180.13422]\n",
            " [196.04092]\n",
            " [143.04689]]\n",
            "Step:  350 Cost:  0.8866291 \n",
            "Prediction\n",
            " [[150.24335]\n",
            " [185.48741]\n",
            " [180.13524]\n",
            " [196.0416 ]\n",
            " [143.0439 ]]\n",
            "Step:  360 Cost:  0.8826567 \n",
            "Prediction\n",
            " [[150.24672]\n",
            " [185.48512]\n",
            " [180.13628]\n",
            " [196.0423 ]\n",
            " [143.04095]]\n",
            "Step:  370 Cost:  0.87872046 \n",
            "Prediction\n",
            " [[150.25005]\n",
            " [185.48283]\n",
            " [180.13731]\n",
            " [196.04297]\n",
            " [143.038  ]]\n",
            "Step:  380 Cost:  0.8747935 \n",
            "Prediction\n",
            " [[150.25339]\n",
            " [185.48056]\n",
            " [180.13835]\n",
            " [196.04366]\n",
            " [143.03505]]\n",
            "Step:  390 Cost:  0.8708941 \n",
            "Prediction\n",
            " [[150.2567 ]\n",
            " [185.47827]\n",
            " [180.13936]\n",
            " [196.04434]\n",
            " [143.0321 ]]\n",
            "Step:  400 Cost:  0.86699665 \n",
            "Prediction\n",
            " [[150.26004]\n",
            " [185.47601]\n",
            " [180.14038]\n",
            " [196.04503]\n",
            " [143.02917]]\n",
            "Step:  410 Cost:  0.86314803 \n",
            "Prediction\n",
            " [[150.26334]\n",
            " [185.47375]\n",
            " [180.1414 ]\n",
            " [196.04572]\n",
            " [143.02626]]\n",
            "Step:  420 Cost:  0.8592989 \n",
            "Prediction\n",
            " [[150.26665]\n",
            " [185.4715 ]\n",
            " [180.14243]\n",
            " [196.04639]\n",
            " [143.02335]]\n",
            "Step:  430 Cost:  0.85547715 \n",
            "Prediction\n",
            " [[150.26994]\n",
            " [185.46924]\n",
            " [180.14345]\n",
            " [196.04707]\n",
            " [143.02045]]\n",
            "Step:  440 Cost:  0.8516781 \n",
            "Prediction\n",
            " [[150.27322]\n",
            " [185.467  ]\n",
            " [180.14445]\n",
            " [196.04774]\n",
            " [143.01755]]\n",
            "Step:  450 Cost:  0.84790564 \n",
            "Prediction\n",
            " [[150.27649]\n",
            " [185.46475]\n",
            " [180.14546]\n",
            " [196.0484 ]\n",
            " [143.01466]]\n",
            "Step:  460 Cost:  0.8441372 \n",
            "Prediction\n",
            " [[150.27977]\n",
            " [185.46252]\n",
            " [180.14648]\n",
            " [196.04909]\n",
            " [143.01178]]\n",
            "Step:  470 Cost:  0.84039724 \n",
            "Prediction\n",
            " [[150.28304]\n",
            " [185.46031]\n",
            " [180.14749]\n",
            " [196.04976]\n",
            " [143.00891]]\n",
            "Step:  480 Cost:  0.83667564 \n",
            "Prediction\n",
            " [[150.28627]\n",
            " [185.45807]\n",
            " [180.14848]\n",
            " [196.05043]\n",
            " [143.00603]]\n",
            "Step:  490 Cost:  0.8329811 \n",
            "Prediction\n",
            " [[150.28952]\n",
            " [185.45587]\n",
            " [180.14949]\n",
            " [196.05109]\n",
            " [143.00319]]\n",
            "Step:  500 Cost:  0.8293047 \n",
            "Prediction\n",
            " [[150.29274]\n",
            " [185.45364]\n",
            " [180.15048]\n",
            " [196.05174]\n",
            " [143.00034]]\n",
            "Step:  510 Cost:  0.8256401 \n",
            "Prediction\n",
            " [[150.29596]\n",
            " [185.45143]\n",
            " [180.15146]\n",
            " [196.0524 ]\n",
            " [142.99748]]\n",
            "Step:  520 Cost:  0.8220053 \n",
            "Prediction\n",
            " [[150.29918]\n",
            " [185.44925]\n",
            " [180.15247]\n",
            " [196.05307]\n",
            " [142.99466]]\n",
            "Step:  530 Cost:  0.8183897 \n",
            "Prediction\n",
            " [[150.30237]\n",
            " [185.44705]\n",
            " [180.15344]\n",
            " [196.05371]\n",
            " [142.99182]]\n",
            "Step:  540 Cost:  0.81478345 \n",
            "Prediction\n",
            " [[150.30557]\n",
            " [185.44487]\n",
            " [180.15443]\n",
            " [196.05437]\n",
            " [142.989  ]]\n",
            "Step:  550 Cost:  0.8111995 \n",
            "Prediction\n",
            " [[150.30878]\n",
            " [185.44269]\n",
            " [180.15543]\n",
            " [196.05504]\n",
            " [142.9862 ]]\n",
            "Step:  560 Cost:  0.807642 \n",
            "Prediction\n",
            " [[150.31195]\n",
            " [185.44052]\n",
            " [180.15642]\n",
            " [196.0557 ]\n",
            " [142.9834 ]]\n",
            "Step:  570 Cost:  0.80409896 \n",
            "Prediction\n",
            " [[150.31512]\n",
            " [185.43835]\n",
            " [180.1574 ]\n",
            " [196.05634]\n",
            " [142.9806 ]]\n",
            "Step:  580 Cost:  0.80058545 \n",
            "Prediction\n",
            " [[150.31827]\n",
            " [185.43619]\n",
            " [180.15836]\n",
            " [196.05699]\n",
            " [142.97781]]\n",
            "Step:  590 Cost:  0.7970717 \n",
            "Prediction\n",
            " [[150.32143]\n",
            " [185.43402]\n",
            " [180.15933]\n",
            " [196.05763]\n",
            " [142.97502]]\n",
            "Step:  600 Cost:  0.7935923 \n",
            "Prediction\n",
            " [[150.32457]\n",
            " [185.43187]\n",
            " [180.16031]\n",
            " [196.05827]\n",
            " [142.97226]]\n",
            "Step:  610 Cost:  0.79011756 \n",
            "Prediction\n",
            " [[150.32773]\n",
            " [185.42975]\n",
            " [180.16129]\n",
            " [196.05893]\n",
            " [142.9695 ]]\n",
            "Step:  620 Cost:  0.78667593 \n",
            "Prediction\n",
            " [[150.33084]\n",
            " [185.4276 ]\n",
            " [180.16225]\n",
            " [196.05956]\n",
            " [142.96674]]\n",
            "Step:  630 Cost:  0.7832452 \n",
            "Prediction\n",
            " [[150.33397]\n",
            " [185.42548]\n",
            " [180.16321]\n",
            " [196.06021]\n",
            " [142.96399]]\n",
            "Step:  640 Cost:  0.7798411 \n",
            "Prediction\n",
            " [[150.33705]\n",
            " [185.42332]\n",
            " [180.16415]\n",
            " [196.06082]\n",
            " [142.96123]]\n",
            "Step:  650 Cost:  0.7764403 \n",
            "Prediction\n",
            " [[150.34018]\n",
            " [185.4212 ]\n",
            " [180.16512]\n",
            " [196.06146]\n",
            " [142.95851]]\n",
            "Step:  660 Cost:  0.77306527 \n",
            "Prediction\n",
            " [[150.34329]\n",
            " [185.41911]\n",
            " [180.16609]\n",
            " [196.06212]\n",
            " [142.9558 ]]\n",
            "Step:  670 Cost:  0.76971555 \n",
            "Prediction\n",
            " [[150.34636]\n",
            " [185.41699]\n",
            " [180.16704]\n",
            " [196.06273]\n",
            " [142.95306]]\n",
            "Step:  680 Cost:  0.7663739 \n",
            "Prediction\n",
            " [[150.34944]\n",
            " [185.41489]\n",
            " [180.16798]\n",
            " [196.06337]\n",
            " [142.95035]]\n",
            "Step:  690 Cost:  0.76305383 \n",
            "Prediction\n",
            " [[150.35251]\n",
            " [185.41277]\n",
            " [180.16893]\n",
            " [196.06398]\n",
            " [142.94765]]\n",
            "Step:  700 Cost:  0.7597507 \n",
            "Prediction\n",
            " [[150.35558]\n",
            " [185.41069]\n",
            " [180.16988]\n",
            " [196.0646 ]\n",
            " [142.94495]]\n",
            "Step:  710 Cost:  0.7564697 \n",
            "Prediction\n",
            " [[150.35863]\n",
            " [185.4086 ]\n",
            " [180.17082]\n",
            " [196.06523]\n",
            " [142.94226]]\n",
            "Step:  720 Cost:  0.75319904 \n",
            "Prediction\n",
            " [[150.36168]\n",
            " [185.40652]\n",
            " [180.17175]\n",
            " [196.06586]\n",
            " [142.93958]]\n",
            "Step:  730 Cost:  0.7499541 \n",
            "Prediction\n",
            " [[150.36472]\n",
            " [185.40445]\n",
            " [180.1727 ]\n",
            " [196.06648]\n",
            " [142.9369 ]]\n",
            "Step:  740 Cost:  0.7467262 \n",
            "Prediction\n",
            " [[150.36774]\n",
            " [185.40237]\n",
            " [180.17363]\n",
            " [196.0671 ]\n",
            " [142.93423]]\n",
            "Step:  750 Cost:  0.7435132 \n",
            "Prediction\n",
            " [[150.37076]\n",
            " [185.4003 ]\n",
            " [180.17456]\n",
            " [196.06772]\n",
            " [142.93158]]\n",
            "Step:  760 Cost:  0.7403138 \n",
            "Prediction\n",
            " [[150.37378]\n",
            " [185.39825]\n",
            " [180.17549]\n",
            " [196.06834]\n",
            " [142.92892]]\n",
            "Step:  770 Cost:  0.73712975 \n",
            "Prediction\n",
            " [[150.37679]\n",
            " [185.3962 ]\n",
            " [180.17642]\n",
            " [196.06894]\n",
            " [142.92627]]\n",
            "Step:  780 Cost:  0.7339786 \n",
            "Prediction\n",
            " [[150.37978]\n",
            " [185.39415]\n",
            " [180.17735]\n",
            " [196.06956]\n",
            " [142.92365]]\n",
            "Step:  790 Cost:  0.730836 \n",
            "Prediction\n",
            " [[150.38275]\n",
            " [185.39209]\n",
            " [180.17827]\n",
            " [196.07016]\n",
            " [142.921  ]]\n",
            "Step:  800 Cost:  0.72770387 \n",
            "Prediction\n",
            " [[150.38574]\n",
            " [185.39006]\n",
            " [180.1792 ]\n",
            " [196.07077]\n",
            " [142.91838]]\n",
            "Step:  810 Cost:  0.7245914 \n",
            "Prediction\n",
            " [[150.38872]\n",
            " [185.38805]\n",
            " [180.18011]\n",
            " [196.07138]\n",
            " [142.91576]]\n",
            "Step:  820 Cost:  0.721496 \n",
            "Prediction\n",
            " [[150.3917 ]\n",
            " [185.38602]\n",
            " [180.18103]\n",
            " [196.07199]\n",
            " [142.91316]]\n",
            "Step:  830 Cost:  0.71842295 \n",
            "Prediction\n",
            " [[150.39462]\n",
            " [185.38397]\n",
            " [180.18193]\n",
            " [196.07257]\n",
            " [142.91054]]\n",
            "Step:  840 Cost:  0.7153588 \n",
            "Prediction\n",
            " [[150.39758]\n",
            " [185.38197]\n",
            " [180.18285]\n",
            " [196.07318]\n",
            " [142.90794]]\n",
            "Step:  850 Cost:  0.71231854 \n",
            "Prediction\n",
            " [[150.40053]\n",
            " [185.37997]\n",
            " [180.18376]\n",
            " [196.0738 ]\n",
            " [142.90536]]\n",
            "Step:  860 Cost:  0.70927787 \n",
            "Prediction\n",
            " [[150.40347]\n",
            " [185.37796]\n",
            " [180.18468]\n",
            " [196.0744 ]\n",
            " [142.90277]]\n",
            "Step:  870 Cost:  0.7062732 \n",
            "Prediction\n",
            " [[150.40639]\n",
            " [185.37595]\n",
            " [180.18556]\n",
            " [196.07498]\n",
            " [142.9002 ]]\n",
            "Step:  880 Cost:  0.7032769 \n",
            "Prediction\n",
            " [[150.4093 ]\n",
            " [185.37396]\n",
            " [180.18646]\n",
            " [196.07556]\n",
            " [142.89763]]\n",
            "Step:  890 Cost:  0.70030147 \n",
            "Prediction\n",
            " [[150.41222]\n",
            " [185.37198]\n",
            " [180.18738]\n",
            " [196.07617]\n",
            " [142.89508]]\n",
            "Step:  900 Cost:  0.6973327 \n",
            "Prediction\n",
            " [[150.41513]\n",
            " [185.37   ]\n",
            " [180.18828]\n",
            " [196.07677]\n",
            " [142.89253]]\n",
            "Step:  910 Cost:  0.69438416 \n",
            "Prediction\n",
            " [[150.41801]\n",
            " [185.36801]\n",
            " [180.18916]\n",
            " [196.07735]\n",
            " [142.88997]]\n",
            "Step:  920 Cost:  0.69146395 \n",
            "Prediction\n",
            " [[150.42088]\n",
            " [185.36603]\n",
            " [180.19005]\n",
            " [196.07793]\n",
            " [142.88744]]\n",
            "Step:  930 Cost:  0.688539 \n",
            "Prediction\n",
            " [[150.42378]\n",
            " [185.36407]\n",
            " [180.19095]\n",
            " [196.07854]\n",
            " [142.8849 ]]\n",
            "Step:  940 Cost:  0.6856369 \n",
            "Prediction\n",
            " [[150.42665]\n",
            " [185.3621 ]\n",
            " [180.19183]\n",
            " [196.07912]\n",
            " [142.88237]]\n",
            "Step:  950 Cost:  0.68275815 \n",
            "Prediction\n",
            " [[150.4295 ]\n",
            " [185.36015]\n",
            " [180.1927 ]\n",
            " [196.07968]\n",
            " [142.87985]]\n",
            "Step:  960 Cost:  0.6798865 \n",
            "Prediction\n",
            " [[150.43236]\n",
            " [185.35818]\n",
            " [180.19359]\n",
            " [196.08026]\n",
            " [142.87733]]\n",
            "Step:  970 Cost:  0.67703515 \n",
            "Prediction\n",
            " [[150.43521]\n",
            " [185.35626]\n",
            " [180.19447]\n",
            " [196.08086]\n",
            " [142.87483]]\n",
            "Step:  980 Cost:  0.6741937 \n",
            "Prediction\n",
            " [[150.43808]\n",
            " [185.35434]\n",
            " [180.19537]\n",
            " [196.08145]\n",
            " [142.87236]]\n",
            "Step:  990 Cost:  0.67138016 \n",
            "Prediction\n",
            " [[150.44087]\n",
            " [185.35237]\n",
            " [180.19623]\n",
            " [196.082  ]\n",
            " [142.86984]]\n",
            "Step:  1000 Cost:  0.6685643 \n",
            "Prediction\n",
            " [[150.44371]\n",
            " [185.35045]\n",
            " [180.1971 ]\n",
            " [196.08257]\n",
            " [142.86736]]\n",
            "Step:  1010 Cost:  0.6657714 \n",
            "Prediction\n",
            " [[150.44653]\n",
            " [185.34853]\n",
            " [180.19797]\n",
            " [196.08315]\n",
            " [142.86488]]\n",
            "Step:  1020 Cost:  0.663005 \n",
            "Prediction\n",
            " [[150.44933]\n",
            " [185.3466 ]\n",
            " [180.19884]\n",
            " [196.08372]\n",
            " [142.86241]]\n",
            "Step:  1030 Cost:  0.6602408 \n",
            "Prediction\n",
            " [[150.45213]\n",
            " [185.34468]\n",
            " [180.1997 ]\n",
            " [196.08427]\n",
            " [142.85995]]\n",
            "Step:  1040 Cost:  0.65748966 \n",
            "Prediction\n",
            " [[150.45493]\n",
            " [185.34277]\n",
            " [180.20056]\n",
            " [196.08484]\n",
            " [142.85748]]\n",
            "Step:  1050 Cost:  0.6547581 \n",
            "Prediction\n",
            " [[150.45772]\n",
            " [185.34087]\n",
            " [180.20143]\n",
            " [196.08542]\n",
            " [142.85504]]\n",
            "Step:  1060 Cost:  0.65204555 \n",
            "Prediction\n",
            " [[150.4605 ]\n",
            " [185.33897]\n",
            " [180.2023 ]\n",
            " [196.086  ]\n",
            " [142.8526 ]]\n",
            "Step:  1070 Cost:  0.6493392 \n",
            "Prediction\n",
            " [[150.46327]\n",
            " [185.33707]\n",
            " [180.20314]\n",
            " [196.08653]\n",
            " [142.85017]]\n",
            "Step:  1080 Cost:  0.6466521 \n",
            "Prediction\n",
            " [[150.46603]\n",
            " [185.33519]\n",
            " [180.20401]\n",
            " [196.0871 ]\n",
            " [142.84773]]\n",
            "Step:  1090 Cost:  0.6439728 \n",
            "Prediction\n",
            " [[150.4688 ]\n",
            " [185.3333 ]\n",
            " [180.20485]\n",
            " [196.08766]\n",
            " [142.8453 ]]\n",
            "Step:  1100 Cost:  0.641313 \n",
            "Prediction\n",
            " [[150.47154]\n",
            " [185.33142]\n",
            " [180.2057 ]\n",
            " [196.08821]\n",
            " [142.84288]]\n",
            "Step:  1110 Cost:  0.63866985 \n",
            "Prediction\n",
            " [[150.47429]\n",
            " [185.32954]\n",
            " [180.20654]\n",
            " [196.08878]\n",
            " [142.84048]]\n",
            "Step:  1120 Cost:  0.6360407 \n",
            "Prediction\n",
            " [[150.47702]\n",
            " [185.32768]\n",
            " [180.2074 ]\n",
            " [196.08932]\n",
            " [142.83807]]\n",
            "Step:  1130 Cost:  0.6334225 \n",
            "Prediction\n",
            " [[150.47975]\n",
            " [185.32582]\n",
            " [180.20824]\n",
            " [196.08987]\n",
            " [142.83568]]\n",
            "Step:  1140 Cost:  0.63081855 \n",
            "Prediction\n",
            " [[150.48247]\n",
            " [185.32394]\n",
            " [180.20908]\n",
            " [196.09042]\n",
            " [142.83328]]\n",
            "Step:  1150 Cost:  0.62823075 \n",
            "Prediction\n",
            " [[150.48518]\n",
            " [185.3221 ]\n",
            " [180.20992]\n",
            " [196.09097]\n",
            " [142.8309 ]]\n",
            "Step:  1160 Cost:  0.62565887 \n",
            "Prediction\n",
            " [[150.4879 ]\n",
            " [185.32027]\n",
            " [180.21077]\n",
            " [196.09154]\n",
            " [142.82854]]\n",
            "Step:  1170 Cost:  0.6230997 \n",
            "Prediction\n",
            " [[150.49059]\n",
            " [185.3184 ]\n",
            " [180.2116 ]\n",
            " [196.09207]\n",
            " [142.82616]]\n",
            "Step:  1180 Cost:  0.62055683 \n",
            "Prediction\n",
            " [[150.49327]\n",
            " [185.31657]\n",
            " [180.21242]\n",
            " [196.09262]\n",
            " [142.82379]]\n",
            "Step:  1190 Cost:  0.61801606 \n",
            "Prediction\n",
            " [[150.49597]\n",
            " [185.31473]\n",
            " [180.21326]\n",
            " [196.09317]\n",
            " [142.82144]]\n",
            "Step:  1200 Cost:  0.61549747 \n",
            "Prediction\n",
            " [[150.49864]\n",
            " [185.31291]\n",
            " [180.21408]\n",
            " [196.0937 ]\n",
            " [142.81908]]\n",
            "Step:  1210 Cost:  0.61299473 \n",
            "Prediction\n",
            " [[150.5013 ]\n",
            " [185.31107]\n",
            " [180.21489]\n",
            " [196.09424]\n",
            " [142.81673]]\n",
            "Step:  1220 Cost:  0.6105034 \n",
            "Prediction\n",
            " [[150.50397]\n",
            " [185.30927]\n",
            " [180.21573]\n",
            " [196.09479]\n",
            " [142.81439]]\n",
            "Step:  1230 Cost:  0.6080241 \n",
            "Prediction\n",
            " [[150.50662]\n",
            " [185.30745]\n",
            " [180.21655]\n",
            " [196.09532]\n",
            " [142.81206]]\n",
            "Step:  1240 Cost:  0.60555524 \n",
            "Prediction\n",
            " [[150.50928]\n",
            " [185.30563]\n",
            " [180.21736]\n",
            " [196.09586]\n",
            " [142.80974]]\n",
            "Step:  1250 Cost:  0.6031052 \n",
            "Prediction\n",
            " [[150.51192]\n",
            " [185.30383]\n",
            " [180.21819]\n",
            " [196.09639]\n",
            " [142.80742]]\n",
            "Step:  1260 Cost:  0.60066056 \n",
            "Prediction\n",
            " [[150.51456]\n",
            " [185.30203]\n",
            " [180.219  ]\n",
            " [196.09692]\n",
            " [142.8051 ]]\n",
            "Step:  1270 Cost:  0.5982358 \n",
            "Prediction\n",
            " [[150.51718]\n",
            " [185.30023]\n",
            " [180.2198 ]\n",
            " [196.09744]\n",
            " [142.8028 ]]\n",
            "Step:  1280 Cost:  0.59582007 \n",
            "Prediction\n",
            " [[150.5198 ]\n",
            " [185.29845]\n",
            " [180.22061]\n",
            " [196.09798]\n",
            " [142.80049]]\n",
            "Step:  1290 Cost:  0.59342605 \n",
            "Prediction\n",
            " [[150.52242]\n",
            " [185.29666]\n",
            " [180.22144]\n",
            " [196.09851]\n",
            " [142.7982 ]]\n",
            "Step:  1300 Cost:  0.59103906 \n",
            "Prediction\n",
            " [[150.52502]\n",
            " [185.29489]\n",
            " [180.22224]\n",
            " [196.09904]\n",
            " [142.79591]]\n",
            "Step:  1310 Cost:  0.588663 \n",
            "Prediction\n",
            " [[150.52762]\n",
            " [185.2931 ]\n",
            " [180.22304]\n",
            " [196.09955]\n",
            " [142.79362]]\n",
            "Step:  1320 Cost:  0.58630264 \n",
            "Prediction\n",
            " [[150.53021]\n",
            " [185.29134]\n",
            " [180.22385]\n",
            " [196.10008]\n",
            " [142.79135]]\n",
            "Step:  1330 Cost:  0.5839542 \n",
            "Prediction\n",
            " [[150.53279]\n",
            " [185.28957]\n",
            " [180.22462]\n",
            " [196.1006 ]\n",
            " [142.78908]]\n",
            "Step:  1340 Cost:  0.58161247 \n",
            "Prediction\n",
            " [[150.53539]\n",
            " [185.28781]\n",
            " [180.22543]\n",
            " [196.10112]\n",
            " [142.78682]]\n",
            "Step:  1350 Cost:  0.57929176 \n",
            "Prediction\n",
            " [[150.53795]\n",
            " [185.28604]\n",
            " [180.22623]\n",
            " [196.10164]\n",
            " [142.78456]]\n",
            "Step:  1360 Cost:  0.5769857 \n",
            "Prediction\n",
            " [[150.54051]\n",
            " [185.2843 ]\n",
            " [180.22702]\n",
            " [196.10216]\n",
            " [142.78232]]\n",
            "Step:  1370 Cost:  0.5746793 \n",
            "Prediction\n",
            " [[150.54308]\n",
            " [185.28255]\n",
            " [180.22781]\n",
            " [196.10268]\n",
            " [142.78006]]\n",
            "Step:  1380 Cost:  0.57239926 \n",
            "Prediction\n",
            " [[150.54562]\n",
            " [185.2808 ]\n",
            " [180.2286 ]\n",
            " [196.1032 ]\n",
            " [142.77783]]\n",
            "Step:  1390 Cost:  0.57012683 \n",
            "Prediction\n",
            " [[150.54816]\n",
            " [185.27907]\n",
            " [180.22939]\n",
            " [196.10368]\n",
            " [142.77559]]\n",
            "Step:  1400 Cost:  0.56786644 \n",
            "Prediction\n",
            " [[150.55069]\n",
            " [185.27733]\n",
            " [180.23016]\n",
            " [196.1042 ]\n",
            " [142.77336]]\n",
            "Step:  1410 Cost:  0.5656106 \n",
            "Prediction\n",
            " [[150.55324]\n",
            " [185.2756 ]\n",
            " [180.23096]\n",
            " [196.1047 ]\n",
            " [142.77115]]\n",
            "Step:  1420 Cost:  0.5633723 \n",
            "Prediction\n",
            " [[150.55576]\n",
            " [185.27388]\n",
            " [180.23174]\n",
            " [196.10521]\n",
            " [142.76892]]\n",
            "Step:  1430 Cost:  0.56115323 \n",
            "Prediction\n",
            " [[150.55827]\n",
            " [185.27217]\n",
            " [180.23253]\n",
            " [196.10573]\n",
            " [142.76672]]\n",
            "Step:  1440 Cost:  0.5589437 \n",
            "Prediction\n",
            " [[150.56078]\n",
            " [185.27045]\n",
            " [180.23329]\n",
            " [196.10623]\n",
            " [142.76453]]\n",
            "Step:  1450 Cost:  0.55674326 \n",
            "Prediction\n",
            " [[150.56328]\n",
            " [185.26874]\n",
            " [180.23407]\n",
            " [196.10674]\n",
            " [142.76233]]\n",
            "Step:  1460 Cost:  0.55455196 \n",
            "Prediction\n",
            " [[150.56578]\n",
            " [185.26703]\n",
            " [180.23483]\n",
            " [196.10724]\n",
            " [142.76015]]\n",
            "Step:  1470 Cost:  0.5523737 \n",
            "Prediction\n",
            " [[150.56827]\n",
            " [185.26534]\n",
            " [180.23561]\n",
            " [196.10774]\n",
            " [142.75795]]\n",
            "Step:  1480 Cost:  0.55020225 \n",
            "Prediction\n",
            " [[150.57076]\n",
            " [185.26363]\n",
            " [180.23637]\n",
            " [196.10823]\n",
            " [142.75577]]\n",
            "Step:  1490 Cost:  0.54805166 \n",
            "Prediction\n",
            " [[150.57323]\n",
            " [185.26193]\n",
            " [180.23714]\n",
            " [196.10872]\n",
            " [142.7536 ]]\n",
            "Step:  1500 Cost:  0.5459071 \n",
            "Prediction\n",
            " [[150.57571]\n",
            " [185.26025]\n",
            " [180.23792]\n",
            " [196.10924]\n",
            " [142.75145]]\n",
            "Step:  1510 Cost:  0.5437745 \n",
            "Prediction\n",
            " [[150.57817]\n",
            " [185.25856]\n",
            " [180.23866]\n",
            " [196.10971]\n",
            " [142.74928]]\n",
            "Step:  1520 Cost:  0.54166543 \n",
            "Prediction\n",
            " [[150.58061]\n",
            " [185.25688]\n",
            " [180.23943]\n",
            " [196.11021]\n",
            " [142.74713]]\n",
            "Step:  1530 Cost:  0.53955424 \n",
            "Prediction\n",
            " [[150.58307]\n",
            " [185.25522]\n",
            " [180.24019]\n",
            " [196.1107 ]\n",
            " [142.74498]]\n",
            "Step:  1540 Cost:  0.537455 \n",
            "Prediction\n",
            " [[150.58551]\n",
            " [185.25354]\n",
            " [180.24094]\n",
            " [196.1112 ]\n",
            " [142.74283]]\n",
            "Step:  1550 Cost:  0.53537285 \n",
            "Prediction\n",
            " [[150.58795]\n",
            " [185.25188]\n",
            " [180.2417 ]\n",
            " [196.1117 ]\n",
            " [142.7407 ]]\n",
            "Step:  1560 Cost:  0.5332896 \n",
            "Prediction\n",
            " [[150.5904 ]\n",
            " [185.25021]\n",
            " [180.24245]\n",
            " [196.11217]\n",
            " [142.73857]]\n",
            "Step:  1570 Cost:  0.5312314 \n",
            "Prediction\n",
            " [[150.5928 ]\n",
            " [185.24857]\n",
            " [180.2432 ]\n",
            " [196.11266]\n",
            " [142.73643]]\n",
            "Step:  1580 Cost:  0.5291821 \n",
            "Prediction\n",
            " [[150.59523]\n",
            " [185.24693]\n",
            " [180.24396]\n",
            " [196.11314]\n",
            " [142.73433]]\n",
            "Step:  1590 Cost:  0.5271428 \n",
            "Prediction\n",
            " [[150.59763]\n",
            " [185.24525]\n",
            " [180.24469]\n",
            " [196.11362]\n",
            " [142.73221]]\n",
            "Step:  1600 Cost:  0.5251031 \n",
            "Prediction\n",
            " [[150.60005]\n",
            " [185.24362]\n",
            " [180.24544]\n",
            " [196.1141 ]\n",
            " [142.7301 ]]\n",
            "Step:  1610 Cost:  0.52309453 \n",
            "Prediction\n",
            " [[150.60243]\n",
            " [185.24199]\n",
            " [180.24619]\n",
            " [196.1146 ]\n",
            " [142.728  ]]\n",
            "Step:  1620 Cost:  0.52108395 \n",
            "Prediction\n",
            " [[150.60484]\n",
            " [185.24037]\n",
            " [180.24693]\n",
            " [196.11507]\n",
            " [142.72592]]\n",
            "Step:  1630 Cost:  0.5190876 \n",
            "Prediction\n",
            " [[150.60722]\n",
            " [185.23872]\n",
            " [180.24767]\n",
            " [196.11555]\n",
            " [142.72383]]\n",
            "Step:  1640 Cost:  0.5170967 \n",
            "Prediction\n",
            " [[150.6096 ]\n",
            " [185.23709]\n",
            " [180.2484 ]\n",
            " [196.11601]\n",
            " [142.72174]]\n",
            "Step:  1650 Cost:  0.5151288 \n",
            "Prediction\n",
            " [[150.61195]\n",
            " [185.23547]\n",
            " [180.24911]\n",
            " [196.11649]\n",
            " [142.71965]]\n",
            "Step:  1660 Cost:  0.5131632 \n",
            "Prediction\n",
            " [[150.61433]\n",
            " [185.23387]\n",
            " [180.24986]\n",
            " [196.11697]\n",
            " [142.71759]]\n",
            "Step:  1670 Cost:  0.51120806 \n",
            "Prediction\n",
            " [[150.61668]\n",
            " [185.23224]\n",
            " [180.25058]\n",
            " [196.11743]\n",
            " [142.71552]]\n",
            "Step:  1680 Cost:  0.50925857 \n",
            "Prediction\n",
            " [[150.61905]\n",
            " [185.23064]\n",
            " [180.25131]\n",
            " [196.11789]\n",
            " [142.71346]]\n",
            "Step:  1690 Cost:  0.5073285 \n",
            "Prediction\n",
            " [[150.6214 ]\n",
            " [185.22903]\n",
            " [180.25204]\n",
            " [196.11838]\n",
            " [142.71141]]\n",
            "Step:  1700 Cost:  0.50540465 \n",
            "Prediction\n",
            " [[150.62373]\n",
            " [185.22743]\n",
            " [180.25276]\n",
            " [196.11884]\n",
            " [142.70935]]\n",
            "Step:  1710 Cost:  0.5034941 \n",
            "Prediction\n",
            " [[150.62607]\n",
            " [185.22585]\n",
            " [180.2535 ]\n",
            " [196.11931]\n",
            " [142.7073 ]]\n",
            "Step:  1720 Cost:  0.50159264 \n",
            "Prediction\n",
            " [[150.62839]\n",
            " [185.22424]\n",
            " [180.25421]\n",
            " [196.11975]\n",
            " [142.70526]]\n",
            "Step:  1730 Cost:  0.49969715 \n",
            "Prediction\n",
            " [[150.63072]\n",
            " [185.22267]\n",
            " [180.25493]\n",
            " [196.12024]\n",
            " [142.70323]]\n",
            "Step:  1740 Cost:  0.49781305 \n",
            "Prediction\n",
            " [[150.63304]\n",
            " [185.22108]\n",
            " [180.25565]\n",
            " [196.12071]\n",
            " [142.7012 ]]\n",
            "Step:  1750 Cost:  0.49594226 \n",
            "Prediction\n",
            " [[150.63535]\n",
            " [185.21951]\n",
            " [180.25636]\n",
            " [196.12115]\n",
            " [142.69917]]\n",
            "Step:  1760 Cost:  0.49408144 \n",
            "Prediction\n",
            " [[150.63765]\n",
            " [185.21794]\n",
            " [180.25708]\n",
            " [196.12161]\n",
            " [142.69716]]\n",
            "Step:  1770 Cost:  0.49223042 \n",
            "Prediction\n",
            " [[150.63994]\n",
            " [185.21635]\n",
            " [180.25778]\n",
            " [196.12206]\n",
            " [142.69514]]\n",
            "Step:  1780 Cost:  0.49038726 \n",
            "Prediction\n",
            " [[150.64224]\n",
            " [185.21481]\n",
            " [180.2585 ]\n",
            " [196.12253]\n",
            " [142.69315]]\n",
            "Step:  1790 Cost:  0.4885548 \n",
            "Prediction\n",
            " [[150.64453]\n",
            " [185.21324]\n",
            " [180.25922]\n",
            " [196.123  ]\n",
            " [142.69115]]\n",
            "Step:  1800 Cost:  0.4867391 \n",
            "Prediction\n",
            " [[150.64679]\n",
            " [185.21167]\n",
            " [180.2599 ]\n",
            " [196.12344]\n",
            " [142.68915]]\n",
            "Step:  1810 Cost:  0.48492417 \n",
            "Prediction\n",
            " [[150.64906]\n",
            " [185.21013]\n",
            " [180.2606 ]\n",
            " [196.12389]\n",
            " [142.68715]]\n",
            "Step:  1820 Cost:  0.4831179 \n",
            "Prediction\n",
            " [[150.65134]\n",
            " [185.20857]\n",
            " [180.2613 ]\n",
            " [196.12434]\n",
            " [142.68517]]\n",
            "Step:  1830 Cost:  0.48132315 \n",
            "Prediction\n",
            " [[150.65361]\n",
            " [185.20705]\n",
            " [180.26201]\n",
            " [196.1248 ]\n",
            " [142.6832 ]]\n",
            "Step:  1840 Cost:  0.47953945 \n",
            "Prediction\n",
            " [[150.65587]\n",
            " [185.2055 ]\n",
            " [180.26271]\n",
            " [196.12524]\n",
            " [142.68123]]\n",
            "Step:  1850 Cost:  0.47776875 \n",
            "Prediction\n",
            " [[150.65811]\n",
            " [185.20396]\n",
            " [180.26341]\n",
            " [196.12569]\n",
            " [142.67926]]\n",
            "Step:  1860 Cost:  0.4760073 \n",
            "Prediction\n",
            " [[150.66035]\n",
            " [185.20244]\n",
            " [180.2641 ]\n",
            " [196.12614]\n",
            " [142.6773 ]]\n",
            "Step:  1870 Cost:  0.4742573 \n",
            "Prediction\n",
            " [[150.66257]\n",
            " [185.20088]\n",
            " [180.26477]\n",
            " [196.12657]\n",
            " [142.67534]]\n",
            "Step:  1880 Cost:  0.47251028 \n",
            "Prediction\n",
            " [[150.66481]\n",
            " [185.19939]\n",
            " [180.26549]\n",
            " [196.12701]\n",
            " [142.67339]]\n",
            "Step:  1890 Cost:  0.47077042 \n",
            "Prediction\n",
            " [[150.66704]\n",
            " [185.19786]\n",
            " [180.26617]\n",
            " [196.12746]\n",
            " [142.67143]]\n",
            "Step:  1900 Cost:  0.4690447 \n",
            "Prediction\n",
            " [[150.66927]\n",
            " [185.19635]\n",
            " [180.26686]\n",
            " [196.1279 ]\n",
            " [142.66951]]\n",
            "Step:  1910 Cost:  0.46732765 \n",
            "Prediction\n",
            " [[150.67148]\n",
            " [185.19484]\n",
            " [180.26755]\n",
            " [196.12834]\n",
            " [142.66757]]\n",
            "Step:  1920 Cost:  0.46562085 \n",
            "Prediction\n",
            " [[150.67368]\n",
            " [185.19333]\n",
            " [180.26822]\n",
            " [196.12877]\n",
            " [142.66563]]\n",
            "Step:  1930 Cost:  0.46392718 \n",
            "Prediction\n",
            " [[150.67587]\n",
            " [185.19183]\n",
            " [180.2689 ]\n",
            " [196.12923]\n",
            " [142.66371]]\n",
            "Step:  1940 Cost:  0.46223322 \n",
            "Prediction\n",
            " [[150.67809]\n",
            " [185.19034]\n",
            " [180.26959]\n",
            " [196.12967]\n",
            " [142.6618 ]]\n",
            "Step:  1950 Cost:  0.4605543 \n",
            "Prediction\n",
            " [[150.68027]\n",
            " [185.18884]\n",
            " [180.27028]\n",
            " [196.13008]\n",
            " [142.65988]]\n",
            "Step:  1960 Cost:  0.45888433 \n",
            "Prediction\n",
            " [[150.68243]\n",
            " [185.18733]\n",
            " [180.27094]\n",
            " [196.13051]\n",
            " [142.65796]]\n",
            "Step:  1970 Cost:  0.45722356 \n",
            "Prediction\n",
            " [[150.68462]\n",
            " [185.18585]\n",
            " [180.2716 ]\n",
            " [196.13095]\n",
            " [142.65607]]\n",
            "Step:  1980 Cost:  0.45556968 \n",
            "Prediction\n",
            " [[150.68678]\n",
            " [185.18437]\n",
            " [180.27228]\n",
            " [196.13136]\n",
            " [142.65416]]\n",
            "Step:  1990 Cost:  0.45392185 \n",
            "Prediction\n",
            " [[150.68896]\n",
            " [185.18289]\n",
            " [180.27295]\n",
            " [196.1318 ]\n",
            " [142.65228]]\n",
            "Step:  2000 Cost:  0.45228338 \n",
            "Prediction\n",
            " [[150.69113]\n",
            " [185.18141]\n",
            " [180.27364]\n",
            " [196.13223]\n",
            " [142.65039]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9U3Ihg8Ctv",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 04-2. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n21wZLDQnJNS",
        "colab_type": "text"
      },
      "source": [
        "파일 불러오기 #자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKzd6xIt8Gy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIlRUcMS8Kt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('sample_data/mnist_test.csv', delimiter=',', dtype=np.float32) #.loadtext()\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "print(x_data.shape, x_data, len(x_data))\n",
        "print(y_data.shape, y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z5nmudhnL8h",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 05."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxCupQGYnUMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZjERhv9nWgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]] #0,1 Encoding\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "w = tf.Variable(tf.random_normal([2, 1]), name='weight') #.random_normal([n: 들어오는 개수, m:나가는 개수])\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias') #.random_normal([m: 나가는 개수])\n",
        "hypothesis = tf.sigmoid((tf.matmul(x, w) + b) * 0.1) #.sigmoid()\n",
        "\n",
        "cost = tf.reduce_mean(y * tf.log(hypothesis) + (1-y) * tf.log(1-hypothesis)) #.log()\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #.cast()\n",
        "accurary = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32)) #.equal()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for step in range(10001):\n",
        "    cost_val, _ = sess.run([cost, train], feed_dict={x: x_data, y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(step, cost_val)\n",
        "\n",
        "  h, c, a = sess.run([hypothesis, predicted, accurary],\n",
        "                   feed_dict={x: x_data, y: y_data})\n",
        "  print(\"\\nHypothesis: \", h, \"\\nCorrect(Y): \", c, \"\\nAccuracy: \", a)\n",
        "\n",
        "#그냥 실행을 돌릴 경우 nan이 출력되어 0.1을 hypothesis에 곱해주었다.\n",
        "#이후 실행은 되지만, 제대로 된 결과가 나오지 않음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdlIL9m0fVRA",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 06."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLRbcjMQrNQo",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 06-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCXYocUEf0ch",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0jkaqk3NgNaO",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5,], [1,2,5,6,], [1,6,6,6], [1,7,7,7]]\n",
        "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
        "#One Hot Encoding [0,0,1]=2, [0,1,0]=1, [1,0,0]=0\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "nb_classes = 3\n",
        "w = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias') \n",
        "hypothesis = tf.nn.softmax(tf.matmul(x, w) + b) #.softmax()\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypothesis), axis=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "    sess.run(optimizer, feed_dict={x: x_data, y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(\"Step:\", step, \"Cost:\", sess.run(cost, feed_dict={x: x_data, y: y_data}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn8Ur2dVlQRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = sess.run(hypothesis, feed_dict={x: [[1,11,7,9]]}) \n",
        "print(a, sess.run(tf.arg_max(a, 1))) #.arg_max()\n",
        "\n",
        "print('----------')\n",
        "\n",
        "b = sess.run(hypothesis, feed_dict={x: [[1,3,4,3]]}) \n",
        "print(b, sess.run(tf.arg_max(b, 1)))\n",
        "\n",
        "print('----------')\n",
        "\n",
        "c = sess.run(hypothesis, feed_dict={x: [[1,1,0,1]]}) \n",
        "print(c, sess.run(tf.arg_max(c, 1)))\n",
        "\n",
        "print('----------')\n",
        "\n",
        "all = sess.run(hypothesis, feed_dict={x: [[1,11,7,9], [1,3,4,3], [1,1,0,1]]}) \n",
        "print(all, sess.run(tf.arg_max(all, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Rglb066XuI",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 06-2.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjUAUjdS6em3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LmBDRNGNAou8",
        "colab": {}
      },
      "source": [
        "#원래 강의와 다르게 직접 데이터를 입력함.\n",
        "\n",
        "x_data = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5,], [1,2,5,6,], [1,6,6,6], [1,7,7,7]]\n",
        "y_data = [[2], [2], [2], [1], [1], [1], [0], [0]]\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "y = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "y_one_hot = tf.one_hot(y, nb_classes) #.one_hot()\n",
        "y_one_hot = tf.reshape(y_one_hot, [-1, nb_classes]) #.reshape()\n",
        "#rank N이 rank N+1이 되기 때문에 .one_hot() 이후 .reshape()을 사용한다.\n",
        "nb_classes = 3\n",
        "w = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "logits = tf.matmul(x, w) + b\n",
        "hypothesis = tf.nn.softmax(logits)\n",
        "\n",
        "\n",
        "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot)\n",
        "cost = tf.reduce_mean(cost_i) #.softmax_cross_entropy_with_logits()\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "    sess.run(optimizer, feed_dict={x: x_data, y: y_data})\n",
        "    if step % 200 == 0:\n",
        "      print(\"Step:\", step, \"Cost:\", sess.run(cost, feed_dict={x: x_data, y: y_data}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUptTr0JdqAC",
        "colab_type": "text"
      },
      "source": [
        "# ML_Lab 07."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uff-dbE7rW-u",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 07-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rMpCB5ddtPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZiB2tW7dvEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2,1], [1,3,2], [1,3,4], [1,5,5,], [1,7,5], [1,2,5], [1,6,6,], [1,7,7]]\n",
        "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
        "x_test = [[2,1,1,], [3,1,2,], [3,3,4]]\n",
        "y_test = [[0,0,1], [0,0,1], [0,0,1]]\n",
        "\n",
        "x = tf.placeholder(\"float\", shape=[None, 3])\n",
        "y = tf.placeholder(\"float\", shape=[None, 3])\n",
        "w = tf.Variable(tf.random_normal([3, 3]))\n",
        "b = tf.Variable(tf.random_normal([3])) \n",
        "hypothesis = tf.nn.softmax(tf.matmul(x, w) + b)\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hypothesis), axis=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "prediction = tf.arg_max(hypothesis, 1)\n",
        "is_correct = tf.equal(prediction, tf.arg_max(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for step in range(201):\n",
        "    cost_val, W_val, _ = sess.run([cost, w, optimizer], feed_dict={x: x_data, y: y_data})\n",
        "    print(\"Step:\", step, \"Cost:\", cost_val, \"\\n\", W_val)\n",
        "  print(\"Prediction:\", sess.run(prediction, feed_dict={x: x_test}))\n",
        "  print(\"Accuracy:\", sess.run(accuracy, feed_dict={x: x_test, y: y_test}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWfQxY850Alu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#From GitHub\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "\n",
        "def min_max_scaler(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    # noise term prevents the zero division\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "\n",
        "xy = np.array(\n",
        "    [\n",
        "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "        [819, 823, 1198100, 816, 820.450012],\n",
        "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# very important. It does not work without it.\n",
        "xy = min_max_scaler(xy)\n",
        "print(xy)\n",
        "'''\n",
        "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
        " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
        " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
        " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
        " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
        " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
        " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
        " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
        "'''\n",
        "\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# Hypothesis\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "# Simplified cost/loss function\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "# Minimize\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
        "\n",
        "# Launch the graph in a session.\n",
        "with tf.Session() as sess:\n",
        "    # Initializes global variables in the graph.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(101):\n",
        "        _, cost_val, hy_val = sess.run(\n",
        "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
        "        )\n",
        "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0toKFA9ng3z",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 07-2. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn02xLqAwybs",
        "colab_type": "text"
      },
      "source": [
        "MNIST Dataset .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6J4-tnZnjkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "mnist = pd.read_csv(io.StringIO('mnist_test.csv'))\n",
        "print(mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR6RLgULNZLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "D = pd.read_csv(io.StringIO('Data01.csv'))\n",
        "print(D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrFQt_nJ_lZ",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 08."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc6IC3AhY402",
        "colab_type": "text"
      },
      "source": [
        "Simple ID Array and Slicing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5dzuJgmKDDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import pprint\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4) #.PrettyPrinter()\n",
        "#sess = tf.InteractiveSession() 이용하지 않음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMldRFjlKdyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([0, 1, 2, 3, 4, 5, 6]) #.array()\n",
        "pp.pprint(t)\n",
        "print(t.ndim) #rank\n",
        "print(t.shape) #shape\n",
        "print(t[0], t[1], t[-1])\n",
        "print(t[2:5], t[4:-1])\n",
        "print(t[:2], t[3:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVdmIymgM9tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
        "pp.pprint(t) #.pprint()\n",
        "print(t.ndim) # rank\n",
        "print(t.shape) # shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qGNLGHPY9xN",
        "colab_type": "text"
      },
      "source": [
        "Shape, Rank, Axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gatEqNd_Ocbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.constant([1,2,3,4])\n",
        "tf.shape(t).eval() #.shape(), .eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7NVmIkCOoL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.constant([[1,2],\n",
        "                 [3,4]])\n",
        "tf.shape(t).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTotWXDAOqki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.constant([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
        "tf.shape(t).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oheE0K52Qky5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[ #Axis=0\n",
        "    [ #Axis=1\n",
        "        [ #Axis=2\n",
        "            [1,2,3,4], \n",
        "            [5,6,7,8],\n",
        "            [9,10,11,12] #Axis=3 or -1\n",
        "        ],\n",
        "        [\n",
        "            [13,14,15,16],\n",
        "            [17,18,19,20], \n",
        "            [21,22,23,24]\n",
        "        ]\n",
        "    ]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZk1K2dFZF5M",
        "colab_type": "text"
      },
      "source": [
        "Matmul & Multiply."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLCIzA53TD9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix1 = tf.constant([[1.,2], [3.,4.]])\n",
        "matrix2 = tf.constant([[1.],[2.]])\n",
        "print(\"Metrix 1 shape\", matrix1.shape)\n",
        "print(\"Metrix 2 shape\", matrix2.shape)\n",
        "tf.matmul(matrix1, matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQyCC2rTGgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(matrix1*matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEnfR9dNZJTK",
        "colab_type": "text"
      },
      "source": [
        "Broadcasting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__RIkserU38r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix1 = tf.constant([[3., 3.]])\n",
        "matrix2 = tf.constant([[2.],[2.]])\n",
        "(matrix1+matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSEYX2MeU5pO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix1 = tf.constant([[3., 3.]])\n",
        "matrix2 = tf.constant([[2., 2.]])\n",
        "(matrix1+matrix2).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JZCr1GVVngn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean([1, 2], axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_h9OU-dZMeU",
        "colab_type": "text"
      },
      "source": [
        "Reduce_mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34fMivbTVqnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [[1., 2.],\n",
        "     [3., 4.]]\n",
        "\n",
        "\n",
        "tf.reduce_mean(x).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukW4rW95VvYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(x, axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_NislJRVxDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(x, axis=1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm4eJPhWVzUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(x, axis=-1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2we9VWwRZPgV",
        "colab_type": "text"
      },
      "source": [
        "Reduce_sum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YJbNdYZXU5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_sum(x).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_2ZyE5TXWuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_sum(x, axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F-uXC6SXX0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_sum(x, axis=-1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LczmPXlXY10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reduce_mean(tf.reduce_sum(x, axis=-1)).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2PkeAKCZirw",
        "colab_type": "text"
      },
      "source": [
        "Argmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4lfM4ygYAcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [[0, 1, 2],\n",
        "     [2, 1, 0]]\n",
        "tf.argmax(x, axis=0).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYPoWCocYC1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.argmax(x, axis=1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBNCFeu-YESJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.argmax(x, axis=-1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14C2tEw2Zk-t",
        "colab_type": "text"
      },
      "source": [
        "Reshape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQFHYoxoYpTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([[[0, 1, 2], \n",
        "               [3, 4, 5]],\n",
        "              \n",
        "              [[6, 7, 8], \n",
        "               [9, 10, 11]]])\n",
        "t.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGVswnxoYtDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reshape(t, shape=[-1, 3]).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6EefYn7Yugm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reshape(t, shape=[-1, 1, 3]).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-tP22LYwzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.squeeze([[0], [1], [2]]).eval() #.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUuFsIMoYxhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.expand_dims([0, 1, 2], 1).eval() #.expand_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oxXg1QqaSkz",
        "colab_type": "text"
      },
      "source": [
        "One_hot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4P5m4hkaYGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.one_hot([[0], [1], [2], [0]], depth=3).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVpvMGmWaZvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.one_hot([[0], [1], [2], [0]], depth=3)\n",
        "tf.reshape(t, shape=[-1, 3]).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ilpwKfcX1J",
        "colab_type": "text"
      },
      "source": [
        "Casting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vov-q2wHcfZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.cast([1.8, 2.2, 3.3, 4.9], tf.int32).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh9UVVBFco43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.cast([True, False, 1 == 1, 0 == 1], tf.int32).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VAYZ-fRdFsK",
        "colab_type": "text"
      },
      "source": [
        "Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzdcadsPdHuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [1, 4]\n",
        "y = [2, 5]\n",
        "z = [3, 6]\n",
        "\n",
        "tf.stack([x, y, z]).eval() #.stack()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQcrGYWndPNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.stack([x, y, z], axis=1).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K2XTmx8h0R6",
        "colab_type": "text"
      },
      "source": [
        "Ones_like & Zeros_like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Kg0d15iLOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [[0, 1, 2],\n",
        "     [2, 1, 0]]\n",
        "\n",
        "tf.ones_like(x).eval() #.ones_like()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydUOqucKiPYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.zeros_like(x).eval() #.zeros_like()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gipesm-Ai1Gv",
        "colab_type": "text"
      },
      "source": [
        "Zip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2-9ovYIi2lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x, y in zip([1, 2, 3], [4, 5, 6]):\n",
        "    print(x, y)\n",
        "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
        "    print(x, y, z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrSMbBLMwOfN",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 09."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26HDEFj_rj4i",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 09-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya3r0I8ewSYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvb66mv-wWms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
        "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
        "\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        _, cost_val, w_val = sess.run(\n",
        "                  [train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(\"Step:\", step, \"Cost:\", cost_val, \"\\n\", w_val)\n",
        "\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4e8r1sFynP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
        "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
        "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for step in range(10001):\n",
        "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(\"Step:\", step, \"Cost:\", cost_val)\n",
        "\n",
        "    h, p, a = sess.run(\n",
        "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNTIEvjT0gNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, [None, 2])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
        "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
        "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
        "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
        "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
        "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
        "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
        "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for step in range(10001):\n",
        "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(\"Step:\", step, \"Cost:\", cost_val)\n",
        "\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaGUlzMP02eS",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 09-2. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uisTHgfH_f_P",
        "colab_type": "text"
      },
      "source": [
        "Tensorboard .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6RPRenV07I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXdnObvc1Epb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32)\n",
        "y_data = np.array([[0.], [1.], [1.], [0.]], dtype=np.float32)\n",
        "X = tf.placeholder(tf.float32, shape=[None, 2], name=\"x\")\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"Layer1\"):\n",
        "    W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight_1\")\n",
        "    b1 = tf.Variable(tf.random_normal([2]), name=\"bias_1\")\n",
        "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "\n",
        "    tf.summary.histogram(\"W1\", W1)\n",
        "    tf.summary.histogram(\"b1\", b1)\n",
        "    tf.summary.histogram(\"Layer1\", layer1)\n",
        "\n",
        "\n",
        "with tf.name_scope(\"Layer2\"):\n",
        "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight_2\")\n",
        "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias_2\")\n",
        "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "    tf.summary.histogram(\"W2\", W2)\n",
        "    tf.summary.histogram(\"b2\", b2)\n",
        "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
        "\n",
        "# cost/loss function\n",
        "with tf.name_scope(\"Cost\"):\n",
        "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "    tf.summary.scalar(\"Cost\", cost)\n",
        "\n",
        "with tf.name_scope(\"Train\"):\n",
        "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    # tensorboard --logdir=./logs/xor_logs\n",
        "    merged_summary = tf.summary.merge_all()\n",
        "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
        "    writer.add_graph(sess.graph)  # Show the graph\n",
        "\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        _, summary, cost_val = sess.run(\n",
        "            [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}\n",
        "        )\n",
        "        writer.add_summary(summary, global_step=step)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(step, cost_val)\n",
        "\n",
        "    # Accuracy report\n",
        "    h, p, a = sess.run(\n",
        "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvfBUpZaVb8W",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 10. +"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMZ-FNVJVhDm",
        "colab_type": "text"
      },
      "source": [
        "다양한 방법으로 MNIST 돌리기 .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY6RGXP6jvra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W = tf.Variable(tf.random_normal([784, 10]))\n",
        "b = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "num_epochs = 50\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
        "    )\n",
        ")\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# train my model\n",
        "with tf.Session() as sess:\n",
        "    # initialize\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for iteration in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
        "\n",
        "    print(\"Learning Finished!\")\n",
        "\n",
        "    # Test model and check accuracy\n",
        "    print(\n",
        "        \"Accuracy:\",\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
        "    )\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(\n",
        "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6A0CDbIVdOR",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 11."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj3Srlf8rt9-",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 11-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZLYyQyuVrVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN6PAeVWWD55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "image = np.array([[[[1],[2],[3]],\n",
        "                   [[4],[5],[6]], \n",
        "                   [[7],[8],[9]]]], dtype=np.float32)\n",
        "print(image.shape)\n",
        "plt.imshow(image.reshape(3,3), cmap='Greys') #.imshow()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWSD_7y1X12b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"image.shape\", image.shape)\n",
        "weight = tf.constant([[[[1.]],[[1.]]],\n",
        "                      [[[1.]],[[1.]]]])\n",
        "print(\"weight.shape\", weight.shape)\n",
        "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID') #.nn.conv2d()\n",
        "conv2d_img = conv2d.eval()\n",
        "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3) #.swapaxes()\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "    print(one_img.reshape(2,2))\n",
        "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray') #.subplot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgU2In9jXPvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"image.shape\", image.shape)\n",
        "\n",
        "weight = tf.constant([[[[1.]],[[1.]]],\n",
        "                      [[[1.]],[[1.]]]])\n",
        "print(\"weight.shape\", weight.shape)\n",
        "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
        "conv2d_img = conv2d.eval()\n",
        "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "    print(one_img.reshape(3,3))\n",
        "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbIVmaztZI9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"image.shape\", image.shape)\n",
        "weight = tf.constant([[[[1.,10.,-1.]],[[1.,10.,-1.]]],[[[1.,10.,-1.]],[[1.,10.,-1.]]]])\n",
        "print(\"weight.shape\", weight.shape)\n",
        "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
        "conv2d_img = conv2d.eval()\n",
        "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "    print(one_img.reshape(3,3))\n",
        "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0BIfUWRZk2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = np.array([[[[4],[3]],\n",
        "                    [[2],[1]]]], dtype=np.float32)\n",
        "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1], #.nn.max_pool()\n",
        "                    strides=[1, 1, 1, 1], padding='VALID')\n",
        "print(pool.shape)\n",
        "print(pool.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Ai8uCTZ4Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = np.array([[[[4],[3]],\n",
        "                    [[2],[1]]]], dtype=np.float32)\n",
        "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 1, 1, 1], padding='SAME')\n",
        "print(pool.shape)\n",
        "print(pool.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KT36qFedpN",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 11-2. 3. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mp_Fts_hjnop"
      },
      "source": [
        "MNIST_CNN .#자세한 내용은 요약에서 #1.15.2를 불러오면 실행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo3ElfdqhNGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# L1 ImgIn shape=(?, 28, 28, 1)\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "#    Conv     -> (?, 28, 28, 32)\n",
        "#    Pool     -> (?, 14, 14, 32)\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "'''\n",
        "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
        "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
        "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
        "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L2 ImgIn shape=(?, 14, 14, 32)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "#    Conv      ->(?, 14, 14, 64)\n",
        "#    Pool      ->(?, 7, 7, 64)\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "'''\n",
        "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
        "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
        "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
        "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L3 ImgIn shape=(?, 7, 7, 64)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "#    Conv      ->(?, 7, 7, 128)\n",
        "#    Pool      ->(?, 4, 4, 128)\n",
        "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                    1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
        "'''\n",
        "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
        "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
        "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
        "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
        "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L4 FC 4x4x128 inputs -> 625 outputs\n",
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "'''\n",
        "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
        "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
        "'''\n",
        "\n",
        "# L5 Final FC 625 inputs -> 10 outputs\n",
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "'''\n",
        "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
        "'''\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "print('Learning started. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "\n",
        "# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pGGSSlo4N0u",
        "colab_type": "text"
      },
      "source": [
        "#ML_Lab 12."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itv3Kng4r2Sc",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-1.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1gEF3L-4See",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.contrib import rnn\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBRFXctnn0vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXDamUM24f7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2)\n",
        "hidden_size = 2\n",
        "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size) #.contrib.rnn.BasicRNNCell()\n",
        "\n",
        "x_data = np.array([[h]], dtype=np.float32)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32) #.nn.dynamic_rnn()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "pp.pprint(outputs.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw057jOpn5pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
        "hidden_size = 2\n",
        "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
        "x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
        "print(x_data.shape)\n",
        "pp.pprint(x_data)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "pp.pprint(outputs.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjUBe6eUnx_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
        "x_data = np.array([[h, e, l, l, o],\n",
        "                   [e, o, l, l, l],\n",
        "                   [l, l, e, e, l]], dtype=np.float32)\n",
        "pp.pprint(x_data)\n",
        "    \n",
        "hidden_size = 2\n",
        "cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True) #.BasicLSTMCell()\n",
        "outputs, _states = tf.nn.dynamic_rnn(\n",
        "    cell, x_data, dtype=tf.float32)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "pp.pprint(outputs.eval())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UnfMbiBtw_u",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-2.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0qGL507tzy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksXP39uTw5il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "x_data = [[0, 1, 0, 2, 3, 3]]\n",
        "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
        "              [0, 1, 0, 0, 0],   # i 1\n",
        "              [1, 0, 0, 0, 0],   # h 0\n",
        "              [0, 0, 1, 0, 0],   # e 2\n",
        "              [0, 0, 0, 1, 0],   # l 3\n",
        "              [0, 0, 0, 1, 0]]]  # l 3\n",
        "\n",
        "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n",
        "\n",
        "num_classes = 5\n",
        "input_dim = 5\n",
        "hidden_size = 5\n",
        "batch_size = 1\n",
        "sequence_length = 6\n",
        "learning_rate = 0.1\n",
        "\n",
        "X = tf.placeholder(\n",
        "    tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
        "\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
        "initial_state = cell.zero_state(batch_size, tf.float32) #.zero_state()\n",
        "outputs, _states = tf.nn.dynamic_rnn(\n",
        "    cell, X, initial_state=initial_state, dtype=tf.float32)\n",
        "\n",
        "# FC layer\n",
        "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
        "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
        "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
        "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
        "outputs = tf.contrib.layers.fully_connected( #.contrib.layers.fully_connected()\n",
        "    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
        "\n",
        "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
        "\n",
        "weights = tf.ones([batch_size, sequence_length]) #.ones()\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss( #.contrib.seq2seq.sequence_loss()\n",
        "    logits=outputs, targets=Y, weights=weights)\n",
        "loss = tf.reduce_mean(sequence_loss)\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "#.AdamOptimizer()\n",
        "\n",
        "prediction = tf.argmax(outputs, axis=2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(50):\n",
        "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
        "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
        "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
        "\n",
        "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
        "        print(\"\\tPrediction str: \", ''.join(result_str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHLM2lY1eFr",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-3.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJwyhMk01kDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcGTZu6W1l8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = \" if you want you\"\n",
        "idx2char = list(set(sample))  # index -> char\n",
        "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
        "\n",
        "# hyper parameters\n",
        "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
        "rnn_hidden_size = len(char2idx)  # RNN output size\n",
        "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
        "batch_size = 1  # one sample data, one batch\n",
        "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
        "learning_rate = 0.2\n",
        "\n",
        "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
        "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
        "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
        "\n",
        "# flatten the data (ignore batches for now). No effect if the batch size is 1\n",
        "X_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
        "X_for_softmax = tf.reshape(X_one_hot, [-1, rnn_hidden_size])\n",
        "\n",
        "# softmax layer (rnn_hidden_size -> num_classes)\n",
        "softmax_w = tf.get_variable(\"softmax_w\", [rnn_hidden_size, num_classes])\n",
        "softmax_b = tf.get_variable(\"softmax_b\", [num_classes]) #.get_variable()\n",
        "outputs = tf.matmul(X_for_softmax, softmax_w) + softmax_b\n",
        "\n",
        "# expend the data (revive the batches)\n",
        "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
        "weights = tf.ones([batch_size, sequence_length])\n",
        "\n",
        "# Compute sequence cost/loss\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
        "    logits=outputs, targets=Y, weights=weights)\n",
        "loss = tf.reduce_mean(sequence_loss)  # mean all sequence loss\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "prediction = tf.argmax(outputs, axis=2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(3000):\n",
        "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
        "        result = sess.run(prediction, feed_dict={X: x_data})\n",
        "\n",
        "        # print char using dic\n",
        "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
        "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPFaXOtj8ABg",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12.4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl6RSrpx-2j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "tf.set_random_seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5EITgxh-4iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {w: i for i, w in enumerate(char_set)}\n",
        "\n",
        "data_dim = len(char_set)\n",
        "hidden_size = len(char_set)\n",
        "num_classes = len(char_set)\n",
        "sequence_length = 10  # Any arbitrary number\n",
        "learning_rate = 0.1\n",
        "\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x = [char_dic[c] for c in x_str]  # x str to index\n",
        "    y = [char_dic[c] for c in y_str]  # y str to index\n",
        "\n",
        "    dataX.append(x)\n",
        "    dataY.append(y)\n",
        "\n",
        "batch_size = len(dataX)\n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "\n",
        "# One-hot encoding\n",
        "X_one_hot = tf.one_hot(X, num_classes)\n",
        "print(X_one_hot)  # check out the shape\n",
        "\n",
        "# Make a lstm cell with hidden_size (each unit output vector size)\n",
        "def lstm_cell():\n",
        "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
        "    return cell\n",
        "\n",
        "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n",
        "#.MultiRNNCell()\n",
        "\n",
        "# outputs: unfolding size x hidden size, state = hidden size\n",
        "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n",
        "\n",
        "# FC layer\n",
        "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
        "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
        "\n",
        "# reshape out for sequence_loss\n",
        "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
        "\n",
        "# All weights are 1 (equal weights)\n",
        "weights = tf.ones([batch_size, sequence_length])\n",
        "\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
        "    logits=outputs, targets=Y, weights=weights)\n",
        "mean_loss = tf.reduce_mean(sequence_loss)\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(500):\n",
        "    _, l, results = sess.run(\n",
        "        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n",
        "    for j, result in enumerate(results):\n",
        "        index = np.argmax(result, axis=1)\n",
        "        print(i, j, ''.join([char_set[t] for t in index]), l)\n",
        "\n",
        "# Let's print the last char of each result to check it works\n",
        "results = sess.run(outputs, feed_dict={X: dataX})\n",
        "for j, result in enumerate(results):\n",
        "    index = np.argmax(result, axis=1)\n",
        "    if j is 0:  # print all for the first result to make a sentence\n",
        "        print(''.join([char_set[t] for t in index]), end='')\n",
        "    else:\n",
        "        print(char_set[index[-1]], end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B3h7C3KsKr9",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12-5. +**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0OeGtD2Iugz",
        "colab_type": "text"
      },
      "source": [
        "Dynamic RNN .#자세한 내용은 요약에서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjIFTBEpI3BX",
        "colab_type": "text"
      },
      "source": [
        "##**ML_Lab 12.6.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRQaEIrqJQOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import os\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "#if \"DISPLAY\" not in os.environ:\n",
        "    # remove Travis CI Error\n",
        "    #matplotlib.use('Agg') 사용하지 않음.\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxCUw0cJSbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MinMaxScaler(data):\n",
        "    ''' Min Max Normalization\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : numpy.ndarray\n",
        "        input data to be normalized\n",
        "        shape: [Batch size, dimension]\n",
        "    Returns\n",
        "    ----------\n",
        "    data : numpy.ndarry\n",
        "        normalized data\n",
        "        shape: [Batch size, dimension]\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
        "    '''\n",
        "    numerator = data - np.min(data, 0) #.min()\n",
        "    denominator = np.max(data, 0) - np.min(data, 0) #.max()\n",
        "    # noise term prevents the zero division\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "\n",
        "# train Parameters\n",
        "seq_length = 7\n",
        "data_dim = 5\n",
        "hidden_dim = 10\n",
        "output_dim = 1\n",
        "learning_rate = 0.01\n",
        "iterations = 500\n",
        "\n",
        "# Open, High, Low, Volume, Close\n",
        "xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
        "xy = xy[::-1]  # reverse order (chronically ordered)\n",
        "\n",
        "# train/test split\n",
        "train_size = int(len(xy) * 0.7)\n",
        "train_set = xy[0:train_size]\n",
        "test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
        "\n",
        "# Scale each\n",
        "train_set = MinMaxScaler(train_set) #MinMaxScaler()\n",
        "test_set = MinMaxScaler(test_set)\n",
        "\n",
        "# build datasets\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, len(time_series) - seq_length):\n",
        "        _x = time_series[i:i + seq_length, :]\n",
        "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
        "        print(_x, \"->\", _y)\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "trainX, trainY = build_dataset(train_set, seq_length)\n",
        "testX, testY = build_dataset(test_set, seq_length)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "# build a LSTM network\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(\n",
        "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "Y_pred = tf.contrib.layers.fully_connected(\n",
        "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
        "\n",
        "# cost/loss\n",
        "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "# RMSE\n",
        "targets = tf.placeholder(tf.float32, [None, 1])\n",
        "predictions = tf.placeholder(tf.float32, [None, 1])\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training step\n",
        "    for i in range(iterations):\n",
        "        _, step_loss = sess.run([train, loss], feed_dict={\n",
        "                                X: trainX, Y: trainY})\n",
        "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
        "\n",
        "    # Test step\n",
        "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
        "    rmse_val = sess.run(rmse, feed_dict={\n",
        "                    targets: testY, predictions: test_predict})\n",
        "    print(\"RMSE: {}\".format(rmse_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R1kY5LRLvew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot predictions\n",
        "    plt.plot(testY)\n",
        "    plt.plot(test_predict)\n",
        "    plt.xlabel(\"Time Period\") #.xlabel()\n",
        "    plt.ylabel(\"Stock Price\") #.ylabel()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xDpd3YtGKpA",
        "colab_type": "text"
      },
      "source": [
        "#Memo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9wAvZCKGQIi",
        "colab_type": "text"
      },
      "source": [
        ".constant()\n",
        "\n",
        ".Session()\n",
        "\n",
        ".run()\n",
        "\n",
        ".add()\n",
        "\n",
        ".placeholeder()와 feed_dict={a:a_data}\n",
        "\n",
        ".Variable(): 변수\n",
        "\n",
        ".random_normal(Shapes)\n",
        "\n",
        ".reduce_mean(): 평균\n",
        "\n",
        ".square()\n",
        "\n",
        ".GradientDescentOptimizer()\n",
        "\n",
        ".minimize()\n",
        "\n",
        ".global_variables_initializer()\n",
        "\n",
        ".append()\n",
        "\n",
        ".plot()\n",
        "\n",
        ".show()\n",
        "\n",
        ".reduce_sum()\n",
        "\n",
        ".assign()\n",
        "\n",
        ".compute_gradients()\n",
        "\n",
        ".apply_gradients()\n",
        "\n",
        ".matmul()\n",
        "\n",
        ".loadtext()\n",
        "\n",
        ".set_random_seed()\n",
        "\n",
        ".string_input_producer()\n",
        "\n",
        ".TextLineReader()\n",
        "\n",
        ".read()\n",
        "\n",
        ".decode_csv()\n",
        "\n",
        ".batch()\n",
        "\n",
        ".Coordinator()\n",
        "\n",
        ".start_queue_runners()\n",
        "\n",
        ".request_stop()\n",
        "\n",
        ".join()\n",
        "\n",
        ".sigmoid()\n",
        "\n",
        ".log()\n",
        "\n",
        ".cast()\n",
        "\n",
        ".equal()\n",
        "\n",
        ".softmax()\n",
        "\n",
        ".arg_max()\n",
        "\n",
        ".one_hot()\n",
        "\n",
        ".reshape()\n",
        "\n",
        ".softmax_cross_entropy_with_logits()\n",
        "\n",
        ".format()\n",
        "\n",
        ".flatten()\n",
        "\n",
        ".PrettyPrinter()\n",
        "\n",
        ".InteractiveSession()\n",
        "\n",
        ".array()\n",
        "\n",
        ".pprint()\n",
        "\n",
        ".shape()\n",
        "\n",
        ".eval()\n",
        "\n",
        ".squeeze()\n",
        "\n",
        ".expand_dims()\n",
        "\n",
        ".stack()\n",
        "\n",
        ".ones_like()\n",
        "\n",
        ".zeros_like()\n",
        "\n",
        "zip()\n",
        "\n",
        ".nn.relu()\n",
        "\n",
        ".random.randn()\n",
        "\n",
        ".nn.dropout()\n",
        "\n",
        ".imshow()\n",
        "\n",
        ".nn.conv2d()\n",
        "\n",
        ".swapaxes()\n",
        "\n",
        ".subplot()\n",
        "\n",
        ".nn.max_pool()\n",
        "\n",
        ".contrib.rnn.BasicRNNCell()\n",
        "\n",
        ".nn.dynamic_rnn()\n",
        "\n",
        ".BasicLSTMCell()\n",
        "\n",
        ".zero_state()\n",
        "\n",
        ".contrib.layers.fully_connected()\n",
        "\n",
        ".ones()\n",
        "\n",
        ".contrib.seq2seq.sequence_loss()\n",
        "\n",
        ".AdamOptimizer()\n",
        "\n",
        ".get_variable()\n",
        "\n",
        ".MultiRNNCell()\n",
        "\n",
        ".min()\n",
        "\n",
        ".max()\n",
        "\n",
        "MinMaxScaler\n",
        "\n",
        ".xlabel()\n",
        "\n",
        ".ylabel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx438ZNhGNeb",
        "colab_type": "text"
      },
      "source": [
        "#이면지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtxsPLmzddsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "tf.set_random_seed(777)\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3iLyobhdjqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = pd.read_csv('sample_data/mnist_test.csv')\n",
        "print(mnist)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}